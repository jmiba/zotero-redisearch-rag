export const TOOL_ASSETS: Record<string, string> = {
  "docling_extract.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.2\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport statistics\nimport shutil\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple\n\n\nLOGGER = logging.getLogger(\"docling_extract\")\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\nProgressCallback = Callable[[int, str, str], None]\n\n\ndef make_progress_emitter(enabled: bool) -> ProgressCallback:\n    if not enabled:\n        def _noop(percent: int, stage: str, message: str) -> None:\n            return None\n        return _noop\n\n    def _emit(percent: int, stage: str, message: str) -> None:\n        payload = {\n            \"type\": \"progress\",\n            \"percent\": max(0, min(100, int(percent))),\n            \"stage\": stage,\n            \"message\": message,\n        }\n        print(json.dumps(payload), flush=True)\n\n    return _emit\n\n\n@dataclass\nclass DoclingProcessingConfig:\n    ocr_mode: str = \"auto\"\n    prefer_ocr_engine: str = \"paddle\"\n    fallback_ocr_engine: str = \"tesseract\"\n    language_hint: Optional[str] = None\n    default_lang_german: str = \"deu+eng\"\n    default_lang_english: str = \"eng\"\n    min_text_chars_per_page: int = 40\n    min_text_pages_ratio: float = 0.3\n    quality_alpha_ratio_threshold: float = 0.6\n    quality_suspicious_token_threshold: float = 0.25\n    quality_min_avg_chars_per_page: int = 80\n    quality_confidence_threshold: float = 0.5\n    quality_use_wordfreq: bool = True\n    quality_wordfreq_min_zipf: float = 3.0\n    column_detect_enable: bool = True\n    column_detect_dpi: int = 150\n    column_detect_max_pages: int = 3\n    column_detect_crop_top_ratio: float = 0.08\n    column_detect_crop_bottom_ratio: float = 0.08\n    column_detect_threshold_std_mult: float = 1.0\n    column_detect_threshold_min: int = 120\n    column_detect_threshold_max: int = 210\n    column_detect_text_percentile: float = 0.7\n    column_detect_min_text_density: float = 0.02\n    column_detect_gap_threshold_ratio: float = 0.2\n    column_detect_min_gap_density: float = 0.01\n    column_detect_min_gap_ratio: float = 0.03\n    column_detect_min_pages_ratio: float = 0.4\n    column_detect_smooth_window: int = 5\n    page_range_sample_tokens: int = 200\n    page_range_min_overlap: float = 0.02\n    page_range_min_hits: int = 5\n    page_range_top_k: int = 5\n    page_range_peak_ratio: float = 0.5\n    page_range_cluster_gap: int = 1\n    page_range_max_span_ratio: float = 0.7\n    max_chunk_chars: int = 3000\n    chunk_overlap_chars: int = 250\n    cleanup_remove_image_tags: bool = True\n    per_page_ocr_on_low_quality: bool = True\n    force_ocr_on_low_quality_text: bool = False\n    enable_post_correction: bool = True\n    enable_dictionary_correction: bool = False\n    dictionary_path: Optional[str] = None\n    dictionary_words: Optional[Sequence[str]] = None\n    default_dictionary_name: str = \"ocr_wordlist.txt\"\n    enable_llm_correction: bool = False\n    llm_correct: Optional[Callable[[str], str]] = None\n    llm_cleanup_base_url: Optional[str] = None\n    llm_cleanup_api_key: Optional[str] = None\n    llm_cleanup_model: Optional[str] = None\n    llm_cleanup_temperature: float = 0.0\n    llm_cleanup_timeout_sec: int = 60\n    llm_correction_min_quality: float = 0.35\n    llm_correction_max_chars: int = 2000\n    postprocess_markdown: bool = False\n    analysis_max_pages: int = 5\n    analysis_sample_strategy: str = \"middle\"\n    ocr_dpi: int = 300\n\n\n@dataclass\nclass OcrRouteDecision:\n    ocr_used: bool\n    ocr_engine: str\n    languages: str\n    route_reason: str\n    use_external_ocr: bool\n    per_page_ocr: bool\n    per_page_reason: str\n\n\n@dataclass\nclass TextQuality:\n    avg_chars_per_page: float\n    alpha_ratio: float\n    suspicious_token_ratio: float\n    confidence_proxy: float\n    dictionary_hit_ratio: Optional[float] = None\n\n@dataclass\nclass ColumnLayoutDetection:\n    detected: bool\n    page_ratio: float\n    reason: str\n\n@dataclass\nclass DoclingConversionResult:\n    markdown: str\n    pages: List[Dict[str, Any]]\n    metadata: Dict[str, Any]\n\n\ndef normalize_text(text: str) -> str:\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n\ndef remove_image_placeholders(text: str) -> str:\n    return re.sub(r\"<!--\\s*image\\s*-->\", \"\", text, flags=re.IGNORECASE)\n\n\ndef clean_chunk_text(text: str, config: Optional[DoclingProcessingConfig]) -> str:\n    if not text:\n        return \"\"\n    cleaned = text\n    if config and config.cleanup_remove_image_tags:\n        cleaned = remove_image_placeholders(cleaned)\n    return cleaned\n\n\ndef normalize_whitespace(text: str) -> str:\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()\n\n\ndef dehyphenate_text(text: str) -> str:\n    return re.sub(r\"(?<=\\w)-\\s*\\n\\s*(?=\\w)\", \"\", text)\n\n\ndef replace_ligatures(text: str) -> str:\n    return (\n        text.replace(\"\\ufb01\", \"fi\")\n        .replace(\"\\ufb02\", \"fl\")\n        .replace(\"\\ufb03\", \"ffi\")\n        .replace(\"\\ufb04\", \"ffl\")\n    )\n\n\ndef split_paragraphs(text: str) -> List[str]:\n    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n    return [para.strip() for para in paragraphs if para.strip()]\n\n\ndef split_long_text(text: str, max_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n    if len(sentences) <= 1:\n        return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n    for sentence in sentences:\n        sent = sentence.strip()\n        if not sent:\n            continue\n        if current_len + len(sent) + 1 > max_chars and current:\n            chunks.append(\" \".join(current).strip())\n            current = [sent]\n            current_len = len(sent)\n        else:\n            current.append(sent)\n            current_len += len(sent) + 1\n    if current:\n        chunks.append(\" \".join(current).strip())\n    return chunks\n\n\ndef split_text_by_size(text: str, max_chars: int, overlap_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    paragraphs = split_paragraphs(text) or [text]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n\n    def flush() -> None:\n        nonlocal current, current_len\n        if not current:\n            return\n        chunk = \"\\n\\n\".join(current).strip()\n        chunks.append(chunk)\n        current = []\n        current_len = 0\n\n    for para in paragraphs:\n        for piece in split_long_text(para, max_chars):\n            piece_len = len(piece)\n            if current_len + piece_len + 2 > max_chars and current:\n                flush()\n            current.append(piece)\n            current_len += piece_len + 2\n\n    flush()\n\n    if overlap_chars <= 0 or len(chunks) <= 1:\n        return chunks\n\n    overlapped: List[str] = []\n    previous = \"\"\n    for chunk in chunks:\n        if previous:\n            overlap = previous[-overlap_chars:]\n            combined = f\"{overlap}\\n{chunk}\".strip()\n        else:\n            combined = chunk\n        overlapped.append(combined)\n        previous = chunk\n    return overlapped\n\n\ndef select_wordfreq_languages(languages: str) -> List[str]:\n    lang = (languages or \"\").lower()\n    selected: List[str] = []\n    if any(token in lang for token in (\"deu\", \"ger\", \"de\", \"german\", \"deutsch\")):\n        selected.append(\"de\")\n    if any(token in lang for token in (\"eng\", \"en\", \"english\")):\n        selected.append(\"en\")\n    if not selected:\n        selected.append(\"en\")\n    return selected\n\n\ndef compute_dictionary_hit_ratio(\n    tokens: Sequence[str],\n    languages: str,\n    min_zipf: float,\n) -> Optional[float]:\n    try:\n        from wordfreq import zipf_frequency\n    except Exception:\n        return None\n\n    if not tokens:\n        return None\n    lang_codes = select_wordfreq_languages(languages)\n    hits = 0\n    total = 0\n    for token in tokens:\n        lower = token.lower()\n        if len(lower) < 2:\n            continue\n        total += 1\n        if any(zipf_frequency(lower, lang) >= min_zipf for lang in lang_codes):\n            hits += 1\n    if not total:\n        return None\n    return hits / total\n\n\ndef estimate_text_quality(\n    pages: Sequence[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    languages: Optional[str] = None,\n) -> TextQuality:\n    if not pages:\n        return TextQuality(0.0, 0.0, 1.0, 0.0, None)\n\n    texts = [str(page.get(\"text\", \"\")) for page in pages]\n    total_chars = sum(len(text) for text in texts)\n    alpha_chars = sum(sum(char.isalpha() for char in text) for text in texts)\n    alpha_ratio = alpha_chars / max(1, total_chars)\n\n    tokens = re.findall(r\"[A-Za-z0-9]+\", \" \".join(texts))\n    suspicious_tokens = [\n        token for token in tokens\n        if (sum(char.isdigit() for char in token) / max(1, len(token))) > 0.5\n        or re.search(r\"(.)\\1\\1\", token)\n    ]\n    suspicious_ratio = len(suspicious_tokens) / max(1, len(tokens))\n\n    avg_chars = total_chars / max(1, len(pages))\n    dictionary_hit_ratio = None\n    if config and config.quality_use_wordfreq and languages:\n        dictionary_hit_ratio = compute_dictionary_hit_ratio(\n            tokens,\n            languages,\n            config.quality_wordfreq_min_zipf,\n        )\n    confidence = alpha_ratio * (1.0 - suspicious_ratio)\n    if dictionary_hit_ratio is not None:\n        confidence *= 0.4 + (0.6 * dictionary_hit_ratio)\n    confidence = max(0.0, min(1.0, confidence))\n    return TextQuality(avg_chars, alpha_ratio, suspicious_ratio, confidence, dictionary_hit_ratio)\n\n\ndef detect_text_layer_from_pages(pages: Sequence[Dict[str, Any]], config: DoclingProcessingConfig) -> bool:\n    if not pages:\n        return False\n    pages_with_text = 0\n    for page in pages:\n        cleaned = normalize_text(str(page.get(\"text\", \"\")))\n        if len(cleaned) >= config.min_text_chars_per_page:\n            pages_with_text += 1\n    ratio = pages_with_text / max(1, len(pages))\n    return ratio >= config.min_text_pages_ratio\n\n\ndef is_low_quality(quality: TextQuality, config: DoclingProcessingConfig) -> bool:\n    if quality.confidence_proxy < config.quality_confidence_threshold:\n        return True\n    return (\n        quality.avg_chars_per_page < config.quality_min_avg_chars_per_page\n        or quality.alpha_ratio < config.quality_alpha_ratio_threshold\n        or quality.suspicious_token_ratio > config.quality_suspicious_token_threshold\n    )\n\n\ndef should_rasterize_text_layer(has_text_layer: bool, low_quality: bool, config: DoclingProcessingConfig) -> bool:\n    if config.ocr_mode == \"force\":\n        return True\n    return bool(has_text_layer and low_quality and config.force_ocr_on_low_quality_text)\n\n\ndef decide_per_page_ocr(\n    has_text_layer: bool,\n    quality: TextQuality,\n    config: DoclingProcessingConfig,\n) -> Tuple[bool, str]:\n    if not config.per_page_ocr_on_low_quality:\n        return False, \"Per-page OCR disabled by config\"\n    if not has_text_layer and is_low_quality(quality, config):\n        return True, \"Low-quality scan detected\"\n    if quality.suspicious_token_ratio > config.quality_suspicious_token_threshold:\n        return True, \"High suspicious token ratio\"\n    if quality.avg_chars_per_page < config.quality_min_avg_chars_per_page:\n        return True, \"Low text density\"\n    return False, \"Quality metrics acceptable\"\n\n\ndef select_language_set(\n    language_hint: Optional[str],\n    filename: str,\n    config: DoclingProcessingConfig,\n) -> str:\n    hint = (language_hint or \"\").lower().strip()\n    name = os.path.basename(filename).lower()\n\n    if hint:\n        if any(token in hint for token in (\"de\", \"deu\", \"ger\", \"german\", \"deutsch\")):\n            return config.default_lang_german\n        if any(token in hint for token in (\"en\", \"eng\", \"english\")):\n            return config.default_lang_english\n        return hint\n\n    if re.search(r\"(\\bde\\b|_de\\b|-de\\b|deu|german|deutsch)\", name):\n        return config.default_lang_german\n    return config.default_lang_english\n\n\ndef normalize_languages_for_engine(languages: str, engine: str) -> str:\n    lang = languages.lower()\n    if engine == \"paddle\":\n        if any(token in lang for token in (\"deu\", \"ger\", \"de\", \"german\", \"deutsch\")):\n            return \"german\"\n        return \"en\"\n    return languages\n\n\ndef decide_ocr_route(\n    has_text_layer: bool,\n    quality: TextQuality,\n    available_engines: Sequence[str],\n    config: DoclingProcessingConfig,\n    languages: str,\n) -> OcrRouteDecision:\n    low_quality = is_low_quality(quality, config)\n    if config.ocr_mode == \"off\":\n        return OcrRouteDecision(\n            False,\n            \"none\",\n            languages,\n            \"OCR disabled by config\",\n            False,\n            False,\n            \"Per-page OCR disabled by config\",\n        )\n\n    if config.ocr_mode == \"force\":\n        ocr_used = True\n        route_reason = \"OCR forced by config\"\n    elif has_text_layer and not (config.force_ocr_on_low_quality_text and low_quality):\n        return OcrRouteDecision(\n            False,\n            \"none\",\n            languages,\n            \"Text layer detected\",\n            False,\n            False,\n            \"Per-page OCR not applicable (text layer)\",\n        )\n    else:\n        ocr_used = True\n        if has_text_layer:\n            route_reason = \"Text layer detected but low quality\"\n        else:\n            route_reason = \"No usable text layer detected\"\n\n    engine = \"docling\"\n    use_external = False\n    if ocr_used:\n        if config.prefer_ocr_engine in available_engines:\n            engine = config.prefer_ocr_engine\n            use_external = True\n        elif config.fallback_ocr_engine in available_engines:\n            engine = config.fallback_ocr_engine\n            use_external = True\n\n    per_page = False\n    per_page_reason = \"Per-page OCR not applicable\"\n    if use_external:\n        per_page, per_page_reason = decide_per_page_ocr(has_text_layer, quality, config)\n    if low_quality and not has_text_layer:\n        route_reason = f\"{route_reason}; low-quality scan suspected\"\n\n    return OcrRouteDecision(ocr_used, engine, languages, route_reason, use_external, per_page, per_page_reason)\n\n\ndef detect_available_ocr_engines() -> List[str]:\n    available: List[str] = []\n    try:\n        import paddleocr  # noqa: F401\n        from pdf2image import convert_from_path  # noqa: F401\n        available.append(\"paddle\")\n    except Exception:\n        pass\n    try:\n        import pytesseract  # noqa: F401\n        from pdf2image import convert_from_path  # noqa: F401\n        available.append(\"tesseract\")\n    except Exception:\n        pass\n    return available\n\n\ndef load_default_wordlist(config: DoclingProcessingConfig) -> Sequence[str]:\n    path = config.dictionary_path\n    if not path:\n        path = os.path.join(os.path.dirname(__file__), config.default_dictionary_name)\n    if not path or not os.path.isfile(path):\n        return []\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as handle:\n            return [line.strip() for line in handle if line.strip() and not line.startswith(\"#\")]\n    except Exception as exc:\n        LOGGER.warning(\"Failed to load dictionary word list: %s\", exc)\n        return []\n\n\ndef prepare_dictionary_words(config: DoclingProcessingConfig) -> Sequence[str]:\n    if not config.enable_dictionary_correction:\n        return []\n    if config.dictionary_words:\n        return [word.strip() for word in config.dictionary_words if word and word.strip()]\n    words = load_default_wordlist(config)\n    if not words:\n        LOGGER.warning(\"Dictionary correction enabled but no wordlist was loaded.\")\n    return words\n\n\ndef apply_dictionary_correction(text: str, wordlist: Sequence[str]) -> str:\n    if not wordlist:\n        return text\n    dictionary = {word.lower() for word in wordlist}\n    token_re = re.compile(r\"[A-Za-z0-9]+\")\n\n    def match_case(candidate: str, original: str) -> str:\n        if original.isupper():\n            return candidate.upper()\n        if original[:1].isupper():\n            return candidate.capitalize()\n        return candidate\n\n    def generate_candidates(token: str) -> Iterable[str]:\n        candidates: List[str] = []\n        if any(char.isdigit() for char in token) and any(char.isalpha() for char in token):\n            candidates.append(token.replace(\"0\", \"o\"))\n            candidates.append(token.replace(\"1\", \"l\"))\n            candidates.append(token.replace(\"5\", \"s\"))\n        if \"rn\" in token:\n            candidates.append(token.replace(\"rn\", \"m\"))\n        return candidates\n\n    def replace_token(match: re.Match) -> str:\n        token = match.group(0)\n        lower = token.lower()\n        if lower in dictionary:\n            return token\n        for candidate in generate_candidates(token):\n            if candidate.lower() in dictionary:\n                return match_case(candidate, token)\n        return token\n\n    return token_re.sub(replace_token, text)\n\n\ndef apply_umlaut_corrections(text: str, languages: str, wordlist: Sequence[str]) -> str:\n    lang = languages.lower()\n    if not any(token in lang for token in (\"de\", \"deu\", \"german\", \"deutsch\")):\n        return text\n\n    dictionary = {word.lower() for word in wordlist}\n    replacements = {\n        \"ueber\": \"\\u00fcber\",\n        \"fuer\": \"f\\u00fcr\",\n        \"koennen\": \"k\\u00f6nnen\",\n        \"muessen\": \"m\\u00fcssen\",\n        \"haeufig\": \"h\\u00e4ufig\",\n    }\n\n    def replace_match(match: re.Match) -> str:\n        token = match.group(0)\n        lower = token.lower()\n        if lower in replacements:\n            replacement = replacements[lower]\n            if token.isupper():\n                return replacement.upper()\n            if token[:1].isupper():\n                return replacement.capitalize()\n            return replacement\n        if dictionary:\n            for ascii_seq, umlaut in ((\"ae\", \"\\u00e4\"), (\"oe\", \"\\u00f6\"), (\"ue\", \"\\u00fc\")):\n                if ascii_seq in lower:\n                    candidate = lower.replace(ascii_seq, umlaut)\n                    if candidate in dictionary:\n                        return candidate\n        return token\n\n    return re.sub(r\"[A-Za-z]{4,}\", replace_match, text)\n\n\ndef should_apply_llm_correction(text: str, config: DoclingProcessingConfig) -> bool:\n    if not config.enable_llm_correction:\n        return False\n    if not config.llm_correct:\n        return False\n    if config.llm_correction_max_chars and len(text) > config.llm_correction_max_chars:\n        return False\n    languages = select_language_set(config.language_hint, \"\", config)\n    quality = estimate_text_quality([{\"text\": text}], config, languages)\n    return quality.confidence_proxy < config.llm_correction_min_quality\n\n\ndef build_llm_cleanup_callback(config: DoclingProcessingConfig) -> Optional[Callable[[str], str]]:\n    if not config.enable_llm_correction:\n        return None\n    if not config.llm_cleanup_base_url or not config.llm_cleanup_model:\n        LOGGER.warning(\"LLM cleanup enabled but base URL or model is missing.\")\n        return None\n\n    base_url = config.llm_cleanup_base_url.rstrip(\"/\")\n    endpoint = f\"{base_url}/chat/completions\"\n    api_key = (config.llm_cleanup_api_key or \"\").strip()\n\n    def _call(text: str) -> str:\n        try:\n            import requests\n        except Exception as exc:\n            LOGGER.warning(\"requests not available for LLM cleanup: %s\", exc)\n            return text\n\n        headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n        payload = {\n            \"model\": config.llm_cleanup_model,\n            \"temperature\": config.llm_cleanup_temperature,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are an OCR cleanup assistant. Fix OCR errors without changing meaning. \"\n                        \"Do not add content. Return corrected text only.\"\n                    ),\n                },\n                {\"role\": \"user\", \"content\": text},\n            ],\n        }\n        try:\n            response = requests.post(endpoint, headers=headers, json=payload, timeout=config.llm_cleanup_timeout_sec)\n            response.raise_for_status()\n            data = response.json()\n            content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\")\n            if content:\n                return str(content).strip()\n        except Exception as exc:\n            LOGGER.warning(\"LLM cleanup failed: %s\", exc)\n        return text\n\n    return _call\n\n\ndef postprocess_text(\n    text: str,\n    config: DoclingProcessingConfig,\n    languages: str,\n    wordlist: Sequence[str],\n) -> str:\n    if not text:\n        return text\n    cleaned = dehyphenate_text(text)\n    cleaned = replace_ligatures(cleaned)\n    cleaned = normalize_whitespace(cleaned)\n    if config.enable_dictionary_correction:\n        cleaned = apply_dictionary_correction(cleaned, wordlist)\n    cleaned = apply_umlaut_corrections(cleaned, languages, wordlist)\n    if should_apply_llm_correction(cleaned, config) and config.llm_correct:\n        cleaned = config.llm_correct(cleaned)\n    return cleaned\n\ndef export_markdown(doc: Any) -> str:\n    for method_name in (\"export_to_markdown\", \"to_markdown\", \"export_to_md\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    for method_name in (\"export_to_text\", \"to_text\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    return str(doc)\n\n\ndef export_text(doc: Any) -> str:\n    for method_name in (\"export_to_text\", \"to_text\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    return str(doc)\n\n\ndef extract_pages(doc: Any) -> List[Dict[str, Any]]:\n    pages: List[Dict[str, Any]] = []\n    pages_attr = getattr(doc, \"pages\", None)\n    if pages_attr is not None and not isinstance(pages_attr, (str, bytes, dict)):\n        try:\n            pages_list = list(pages_attr)\n        except TypeError:\n            pages_list = []\n        if pages_list:\n            for idx, page in enumerate(pages_list, start=1):\n                page_num = getattr(page, \"page_number\", None) or getattr(page, \"number\", None) or idx\n                text = None\n                for attr in (\"text\", \"content\", \"markdown\", \"md\"):\n                    if hasattr(page, attr):\n                        value = getattr(page, attr)\n                        text = value() if callable(value) else value\n                        break\n                if text is None and hasattr(page, \"export_to_text\"):\n                    text = page.export_to_text()\n                if text is None:\n                    text = str(page)\n                pages.append({\"page_num\": int(page_num), \"text\": str(text)})\n            return pages\n\n    full_text = export_text(doc)\n    if full_text:\n        pages.append({\"page_num\": 1, \"text\": full_text})\n    return pages\n\n\ndef select_analysis_page_indices(\n    total_pages: int,\n    max_pages: Optional[int],\n    sample_strategy: str,\n) -> List[int]:\n    if total_pages <= 0:\n        return []\n    if not max_pages or max_pages <= 0 or total_pages <= max_pages:\n        return list(range(1, total_pages + 1))\n\n    strategy = (sample_strategy or \"first\").lower()\n    if strategy == \"middle\":\n        start = max(1, (total_pages - max_pages) // 2 + 1)\n        end = min(total_pages, start + max_pages - 1)\n        return list(range(start, end + 1))\n    return list(range(1, max_pages + 1))\n\n\ndef extract_pages_from_pdf(\n    pdf_path: str,\n    max_pages: Optional[int] = None,\n    sample_strategy: str = \"first\",\n) -> List[Dict[str, Any]]:\n    try:\n        from pypdf import PdfReader\n    except Exception as exc:\n        eprint(f\"pypdf is not available for fallback page extraction: {exc}\")\n        return []\n\n    pages: List[Dict[str, Any]] = []\n    try:\n        reader = PdfReader(pdf_path)\n        page_indices = select_analysis_page_indices(len(reader.pages), max_pages, sample_strategy)\n        for idx in page_indices:\n            page = reader.pages[idx - 1]\n            try:\n                text = page.extract_text() or \"\"\n            except Exception:\n                text = \"\"\n            pages.append({\"page_num\": idx, \"text\": text})\n    except Exception as exc:\n        eprint(f\"Failed to extract pages with pypdf: {exc}\")\n        return []\n\n    return pages\n\n\ndef split_markdown_sections(markdown: str) -> List[Dict[str, Any]]:\n    sections: List[Dict[str, Any]] = []\n    current_title = \"\"\n    current_lines: List[str] = []\n\n    def flush() -> None:\n        nonlocal current_title, current_lines\n        if current_title or current_lines:\n            sections.append({\n                \"title\": current_title.strip(),\n                \"text\": \"\\n\".join(current_lines).strip(),\n            })\n        current_title = \"\"\n        current_lines = []\n\n    for line in markdown.splitlines():\n        if line.startswith(\"#\"):\n            flush()\n            current_title = line.lstrip(\"#\").strip()\n        else:\n            current_lines.append(line)\n\n    flush()\n    return sections\n\n\n_PAGE_RANGE_STOPWORDS = {\n    \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"into\", \"over\",\n    \"under\", \"after\", \"before\", \"were\", \"was\", \"are\", \"is\", \"its\", \"their\",\n    \"then\", \"than\", \"than\", \"which\", \"when\", \"where\", \"have\", \"has\", \"had\",\n    \"into\", \"onto\", \"upon\", \"your\", \"yours\", \"they\", \"them\", \"these\", \"those\",\n    \"will\", \"would\", \"could\", \"should\", \"about\", \"there\", \"here\", \"while\",\n    \"what\", \"why\", \"how\", \"not\", \"but\", \"you\", \"your\", \"our\", \"ours\", \"his\",\n    \"her\", \"she\", \"him\", \"she\", \"him\", \"its\", \"also\", \"such\", \"been\", \"being\",\n    \"out\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n    \"nine\", \"ten\", \"more\", \"most\", \"some\", \"many\", \"few\", \"each\", \"per\",\n}\n\n\ndef tokenize_for_page_range(text: str) -> List[str]:\n    tokens = re.findall(r\"[A-Za-z0-9]{3,}\", text.lower())\n    return [token for token in tokens if token not in _PAGE_RANGE_STOPWORDS]\n\n\ndef sample_tokens(tokens: Sequence[str], max_tokens: int) -> List[str]:\n    if max_tokens <= 0 or len(tokens) <= max_tokens:\n        return list(tokens)\n    step = max(1, len(tokens) // max_tokens)\n    return list(tokens[::step])\n\n\ndef compute_page_overlap(\n    section_text: str,\n    pages: List[Dict[str, Any]],\n    config: DoclingProcessingConfig,\n) -> List[Tuple[float, int, int]]:\n    section_tokens = tokenize_for_page_range(section_text)\n    if not section_tokens:\n        return []\n    sample = sample_tokens(section_tokens, config.page_range_sample_tokens)\n    sample_set = set(sample)\n    total = len(sample_set)\n    results: List[Tuple[float, int, int]] = []\n    for page in pages:\n        page_text = str(page.get(\"text\", \"\"))\n        page_tokens = set(tokenize_for_page_range(page_text))\n        hits = len(sample_set & page_tokens)\n        ratio = hits / max(1, total)\n        results.append((ratio, hits, int(page.get(\"page_num\", 0))))\n    return results\n\n\ndef select_overlap_cluster(\n    overlap_scores: Sequence[Tuple[float, int, int]],\n    config: DoclingProcessingConfig,\n) -> List[int]:\n    if not overlap_scores:\n        return []\n    max_ratio = max(score[0] for score in overlap_scores)\n    max_hits = max(score[1] for score in overlap_scores)\n    ratio_cutoff = max(config.page_range_min_overlap, max_ratio * config.page_range_peak_ratio)\n    hits_cutoff = max(config.page_range_min_hits, int(max_hits * config.page_range_peak_ratio))\n    candidates = [\n        (ratio, hits, page_num)\n        for ratio, hits, page_num in overlap_scores\n        if ratio >= ratio_cutoff or hits >= hits_cutoff\n    ]\n    if not candidates:\n        candidates = sorted(overlap_scores, reverse=True)[: config.page_range_top_k]\n\n    candidates.sort(key=lambda item: item[2])\n    clusters: List[List[Tuple[float, int, int]]] = []\n    current: List[Tuple[float, int, int]] = []\n    for entry in candidates:\n        if not current:\n            current.append(entry)\n            continue\n        if entry[2] - current[-1][2] <= config.page_range_cluster_gap:\n            current.append(entry)\n        else:\n            clusters.append(current)\n            current = [entry]\n    if current:\n        clusters.append(current)\n\n    def cluster_score(cluster: Sequence[Tuple[float, int, int]]) -> Tuple[float, float]:\n        ratios = [item[0] for item in cluster]\n        return (sum(ratios), max(ratios))\n\n    best_cluster = max(clusters, key=cluster_score)\n    page_nums = [item[2] for item in best_cluster]\n    if len(page_nums) > 1:\n        span_ratio = (max(page_nums) - min(page_nums) + 1) / max(1, len(overlap_scores))\n        if span_ratio > config.page_range_max_span_ratio:\n            trimmed = sorted(best_cluster, reverse=True)[: config.page_range_top_k]\n            page_nums = [item[2] for item in trimmed]\n    return page_nums\n\n\ndef find_page_range(\n    section_text: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n) -> Tuple[int, int]:\n    if not pages:\n        return 0, 0\n\n    cleaned = normalize_text(section_text)\n    if not cleaned:\n        return 0, 0\n\n    snippet_start = cleaned[:200]\n    snippet_end = cleaned[-200:]\n\n    page_start = 0\n    page_end = 0\n\n    for page in pages:\n        page_clean = normalize_text(page.get(\"text\", \"\"))\n        if snippet_start and snippet_start in page_clean:\n            page_start = page.get(\"page_num\", 0)\n            break\n\n    for page in reversed(pages):\n        page_clean = normalize_text(page.get(\"text\", \"\"))\n        if snippet_end and snippet_end in page_clean:\n            page_end = page.get(\"page_num\", 0)\n            break\n\n    if page_start == 0 or page_end == 0:\n        config = config or DoclingProcessingConfig()\n        overlap_scores = compute_page_overlap(cleaned, pages, config)\n        page_nums = select_overlap_cluster(overlap_scores, config)\n        if page_nums:\n            if page_start == 0:\n                page_start = min(page_nums)\n            if page_end == 0:\n                page_end = max(page_nums)\n\n    if page_start == 0:\n        page_start = pages[0].get(\"page_num\", 0)\n    if page_end == 0:\n        page_end = pages[-1].get(\"page_num\", 0)\n\n    return int(page_start), int(page_end)\n\n\ndef slugify(text: str) -> str:\n    slug = re.sub(r\"[^a-z0-9]+\", \"-\", text.lower()).strip(\"-\")\n    return slug\n\n\ndef configure_layout_options(pipeline_options: Any) -> None:\n    if hasattr(pipeline_options, \"layout_mode\"):\n        pipeline_options.layout_mode = \"accurate\"\n    if hasattr(pipeline_options, \"detect_layout\"):\n        pipeline_options.detect_layout = True\n    if hasattr(pipeline_options, \"extract_tables\"):\n        pipeline_options.extract_tables = True\n    if hasattr(pipeline_options, \"table_structure\"):\n        pipeline_options.table_structure = True\n    layout_options = getattr(pipeline_options, \"layout_options\", None)\n    if layout_options is not None:\n        for name, value in (\n            (\"detect_columns\", True),\n            (\"detect_tables\", True),\n            (\"enable_table_structure\", True),\n            (\"max_columns\", 3),\n        ):\n            if hasattr(layout_options, name):\n                setattr(layout_options, name, value)\n\n\ndef build_converter(config: DoclingProcessingConfig, decision: OcrRouteDecision):\n    from docling.document_converter import DocumentConverter\n\n    try:\n        from docling.datamodel.base_models import InputFormat\n        from docling.datamodel.pipeline_options import PdfPipelineOptions, OCRMode\n        from docling.document_converter import PdfFormatOption\n    except Exception:\n        return DocumentConverter()\n\n    pipeline_options = PdfPipelineOptions()\n    if not decision.ocr_used:\n        if hasattr(pipeline_options, \"do_ocr\"):\n            pipeline_options.do_ocr = False\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.DISABLED\n    elif config.ocr_mode == \"force\":\n        if hasattr(pipeline_options, \"do_ocr\"):\n            pipeline_options.do_ocr = True\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.FORCE\n    else:\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.AUTO\n\n    if decision.ocr_used:\n        if hasattr(pipeline_options, \"ocr_engine\"):\n            pipeline_options.ocr_engine = decision.ocr_engine\n        if hasattr(pipeline_options, \"ocr_languages\"):\n            pipeline_options.ocr_languages = decision.languages\n        if hasattr(pipeline_options, \"ocr_lang\"):\n            pipeline_options.ocr_lang = decision.languages\n\n    configure_layout_options(pipeline_options)\n\n    format_options = {InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n    return DocumentConverter(format_options=format_options)\n\n\ndef find_poppler_path() -> Optional[str]:\n    env_path = os.environ.get(\"POPPLER_PATH\")\n    if env_path and os.path.isfile(os.path.join(env_path, \"pdftoppm\")):\n        return env_path\n    pdftoppm = shutil.which(\"pdftoppm\")\n    if pdftoppm:\n        return os.path.dirname(pdftoppm)\n    for candidate in (\"/opt/homebrew/bin\", \"/usr/local/bin\", \"/usr/bin\"):\n        if os.path.isfile(os.path.join(candidate, \"pdftoppm\")):\n            return candidate\n    return None\n\n\ndef render_pdf_pages(pdf_path: str, dpi: int) -> List[Any]:\n    from pdf2image import convert_from_path\n\n    poppler_path = find_poppler_path()\n    if poppler_path:\n        if shutil.which(\"pdftoppm\") is None:\n            LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n        return convert_from_path(pdf_path, dpi=dpi, poppler_path=poppler_path)\n    return convert_from_path(pdf_path, dpi=dpi)\n\n\ndef render_pdf_pages_sample(pdf_path: str, dpi: int, max_pages: int) -> List[Any]:\n    from pdf2image import convert_from_path\n\n    if max_pages <= 0:\n        return []\n    poppler_path = find_poppler_path()\n    kwargs = {\"dpi\": dpi, \"first_page\": 1, \"last_page\": max_pages}\n    if poppler_path:\n        if shutil.which(\"pdftoppm\") is None:\n            LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n        kwargs[\"poppler_path\"] = poppler_path\n    return convert_from_path(pdf_path, **kwargs)\n\n\ndef compute_column_density(\n    image: Any,\n    config: DoclingProcessingConfig,\n    target_width: int = 300,\n) -> List[float]:\n    gray = image.convert(\"L\")\n    width, height = gray.size\n    if width > target_width:\n        scale = target_width / max(1, width)\n        gray = gray.resize((target_width, max(1, int(height * scale))))\n    width, height = gray.size\n    crop_top = int(height * config.column_detect_crop_top_ratio)\n    crop_bottom = int(height * config.column_detect_crop_bottom_ratio)\n    if crop_top + crop_bottom < height - 1:\n        gray = gray.crop((0, crop_top, width, height - crop_bottom))\n\n    try:\n        import numpy as np\n    except Exception:\n        pixels = list(gray.getdata())\n        w, h = gray.size\n        if w == 0 or h == 0:\n            return []\n        sorted_pixels = sorted(pixels)\n        median = sorted_pixels[len(sorted_pixels) // 2]\n        mean = sum(pixels) / max(1, len(pixels))\n        variance = sum((value - mean) ** 2 for value in pixels) / max(1, len(pixels))\n        std = variance ** 0.5\n        threshold = median - (std * config.column_detect_threshold_std_mult)\n        threshold = min(threshold, config.column_detect_threshold_max)\n        threshold = max(threshold, config.column_detect_threshold_min)\n        densities = [0] * w\n        for y in range(h):\n            row = pixels[y * w:(y + 1) * w]\n            for x, value in enumerate(row):\n                if value < threshold:\n                    densities[x] += 1\n        return [count / h for count in densities]\n\n    arr = np.asarray(gray)\n    if arr.size == 0:\n        return []\n    median = float(np.median(arr))\n    std = float(arr.std())\n    threshold = median - (std * config.column_detect_threshold_std_mult)\n    threshold = min(threshold, config.column_detect_threshold_max)\n    threshold = max(threshold, config.column_detect_threshold_min)\n    mask = arr < threshold\n    return mask.mean(axis=0).tolist()\n\n\ndef smooth_density(density: Sequence[float], window: int) -> List[float]:\n    if window <= 1 or not density:\n        return list(density)\n    size = max(1, int(window))\n    half = size // 2\n    smoothed: List[float] = []\n    for idx in range(len(density)):\n        start = max(0, idx - half)\n        end = min(len(density), idx + half + 1)\n        smoothed.append(sum(density[start:end]) / max(1, end - start))\n    return smoothed\n\n\ndef density_percentile(density: Sequence[float], percentile: float) -> float:\n    if not density:\n        return 0.0\n    clamped = max(0.0, min(1.0, percentile))\n    sorted_vals = sorted(density)\n    idx = int(round(clamped * (len(sorted_vals) - 1)))\n    return sorted_vals[idx]\n\n\ndef count_column_gaps(\n    density: Sequence[float],\n    config: DoclingProcessingConfig,\n) -> int:\n    if not density:\n        return 0\n    total = len(density)\n    margin = max(1, int(total * 0.05))\n    start = margin\n    end = max(start + 1, total - margin)\n    core = density[start:end]\n    if not core:\n        return 0\n    text_level = density_percentile(core, config.column_detect_text_percentile)\n    if text_level < config.column_detect_min_text_density:\n        return 0\n    threshold = max(config.column_detect_min_gap_density, text_level * config.column_detect_gap_threshold_ratio)\n    min_gap = max(1, int(len(core) * config.column_detect_min_gap_ratio))\n\n    gaps = 0\n    idx = 0\n    while idx < len(core):\n        if core[idx] < threshold:\n            gap_start = idx\n            while idx < len(core) and core[idx] < threshold:\n                idx += 1\n            if idx - gap_start >= min_gap:\n                gaps += 1\n        else:\n            idx += 1\n    return gaps\n\n\ndef detect_multicolumn_layout(\n    images: Sequence[Any],\n    config: DoclingProcessingConfig,\n) -> ColumnLayoutDetection:\n    if not images:\n        return ColumnLayoutDetection(False, 0.0, \"No pages available\")\n    sample = list(images[: config.column_detect_max_pages])\n    if not sample:\n        return ColumnLayoutDetection(False, 0.0, \"No sample pages\")\n\n    hits = 0\n    for image in sample:\n        density = compute_column_density(image, config)\n        density = smooth_density(density, config.column_detect_smooth_window)\n        gaps = count_column_gaps(density, config)\n        if gaps >= 1:\n            hits += 1\n    ratio = hits / max(1, len(sample))\n    detected = ratio >= config.column_detect_min_pages_ratio\n    reason = f\"{hits}/{len(sample)} pages show column gutters\"\n    return ColumnLayoutDetection(detected, ratio, reason)\n\n\ndef rasterize_pdf_to_temp(pdf_path: str, dpi: int) -> str:\n    from tempfile import NamedTemporaryFile\n\n    images = render_pdf_pages(pdf_path, dpi)\n    if not images:\n        raise RuntimeError(\"Failed to render PDF pages for rasterization.\")\n\n    temp_file = NamedTemporaryFile(delete=False, suffix=\".pdf\")\n    temp_file.close()\n    first = images[0]\n    rest = images[1:]\n    first.save(temp_file.name, format=\"PDF\", save_all=True, append_images=rest)\n    return temp_file.name\n\n\ndef ocr_pages_with_paddle(\n    images: Sequence[Any],\n    languages: str,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    from paddleocr import PaddleOCR\n\n    try:\n        import numpy as np\n    except Exception as exc:\n        raise RuntimeError(f\"numpy is required for PaddleOCR: {exc}\") from exc\n\n    ocr = PaddleOCR(use_angle_cls=True, lang=languages)\n    pages: List[Dict[str, Any]] = []\n    confidences: List[float] = []\n\n    total = max(1, len(images))\n    for idx, image in enumerate(images, start=1):\n        result = ocr.ocr(np.array(image), cls=True)\n        lines: List[str] = []\n        if result:\n            for entry in result[0] if isinstance(result, list) else result:\n                if not entry:\n                    continue\n                if isinstance(entry, (list, tuple)) and len(entry) >= 2:\n                    text_part = entry[1]\n                    if isinstance(text_part, (list, tuple)) and text_part:\n                        lines.append(str(text_part[0]))\n                        if len(text_part) > 1 and isinstance(text_part[1], (float, int)):\n                            confidences.append(float(text_part[1]))\n                    else:\n                        lines.append(str(text_part))\n        pages.append({\"page_num\": idx, \"text\": \"\\n\".join(lines)})\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"OCR page {idx}/{total}\")\n\n    avg_conf = sum(confidences) / len(confidences) if confidences else None\n    return pages, {\"ocr_confidence_avg\": avg_conf}\n\n\ndef ocr_pages_with_tesseract(\n    images: Sequence[Any],\n    languages: str,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    import pytesseract\n\n    pages: List[Dict[str, Any]] = []\n    total = max(1, len(images))\n    for idx, image in enumerate(images, start=1):\n        text = pytesseract.image_to_string(image, lang=languages)\n        pages.append({\"page_num\": idx, \"text\": text})\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"OCR page {idx}/{total}\")\n    return pages, {}\n\n\ndef run_external_ocr_pages(\n    pdf_path: str,\n    engine: str,\n    languages: str,\n    config: DoclingProcessingConfig,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    images = render_pdf_pages(pdf_path, config.ocr_dpi)\n    if engine == \"paddle\":\n        return ocr_pages_with_paddle(\n            images,\n            normalize_languages_for_engine(languages, engine),\n            progress_cb,\n            progress_base,\n            progress_span,\n        )\n    if engine == \"tesseract\":\n        return ocr_pages_with_tesseract(\n            images,\n            normalize_languages_for_engine(languages, engine),\n            progress_cb,\n            progress_base,\n            progress_span,\n        )\n    return [], {}\n\n\ndef build_quality_report(pdf_path: str, config: DoclingProcessingConfig) -> Dict[str, Any]:\n    analysis_pages = extract_pages_from_pdf(\n        pdf_path,\n        max_pages=config.analysis_max_pages,\n        sample_strategy=config.analysis_sample_strategy,\n    )\n    has_text_layer = detect_text_layer_from_pages(analysis_pages, config)\n    languages = select_language_set(config.language_hint, pdf_path, config)\n    quality = estimate_text_quality(analysis_pages, config, languages)\n    low_quality = is_low_quality(quality, config)\n    return {\n        \"text_layer_detected\": has_text_layer,\n        \"text_layer_low_quality\": has_text_layer and low_quality,\n        \"avg_chars_per_page\": quality.avg_chars_per_page,\n        \"alpha_ratio\": quality.alpha_ratio,\n        \"suspicious_token_ratio\": quality.suspicious_token_ratio,\n        \"confidence_proxy\": quality.confidence_proxy,\n        \"dictionary_hit_ratio\": quality.dictionary_hit_ratio,\n    }\n\n\ndef convert_pdf_with_docling(\n    pdf_path: str,\n    config: DoclingProcessingConfig,\n    progress_cb: Optional[ProgressCallback] = None,\n) -> DoclingConversionResult:\n    emit = progress_cb or (lambda _p, _s, _m: None)\n    emit(5, \"analysis\", \"Analyzing text layer\")\n    analysis_pages = extract_pages_from_pdf(\n        pdf_path,\n        max_pages=config.analysis_max_pages,\n        sample_strategy=config.analysis_sample_strategy,\n    )\n    has_text_layer = detect_text_layer_from_pages(analysis_pages, config)\n    languages = select_language_set(config.language_hint, pdf_path, config)\n    quality = estimate_text_quality(analysis_pages, config, languages)\n    low_quality = is_low_quality(quality, config)\n    available_engines = detect_available_ocr_engines()\n    decision = decide_ocr_route(has_text_layer, quality, available_engines, config, languages)\n    emit(15, \"route\", \"Selecting OCR route\")\n    rasterized_source = False\n    rasterized_pdf_path = \"\"\n    rasterize_error: Optional[str] = None\n    column_layout: Optional[ColumnLayoutDetection] = None\n    if should_rasterize_text_layer(has_text_layer, low_quality, config):\n        try:\n            rasterized_pdf_path = rasterize_pdf_to_temp(pdf_path, config.ocr_dpi)\n            rasterized_source = True\n            emit(25, \"rasterize\", \"Rasterized PDF for OCR\")\n            LOGGER.info(\"Rasterized low-quality text layer for Docling OCR.\")\n        except Exception as exc:\n            rasterize_error = str(exc)\n            LOGGER.warning(\"Failed to rasterize PDF for OCR: %s\", exc)\n    if rasterized_source:\n        decision.per_page_ocr = False\n        decision.per_page_reason = \"Rasterized PDF for Docling OCR\"\n\n    if config.column_detect_enable and decision.ocr_used and (rasterized_source or not has_text_layer):\n        try:\n            sample_images = render_pdf_pages_sample(\n                pdf_path,\n                config.column_detect_dpi,\n                config.column_detect_max_pages,\n            )\n            column_layout = detect_multicolumn_layout(sample_images, config)\n            LOGGER.info(\n                \"Column layout detection: %s (%s)\",\n                column_layout.detected,\n                column_layout.reason,\n            )\n            emit(30, \"layout\", \"Checked column layout\")\n            if column_layout.detected and decision.use_external_ocr and decision.per_page_ocr:\n                decision.per_page_ocr = False\n                decision.per_page_reason = \"Columns detected; keep Docling layout\"\n        except Exception as exc:\n            LOGGER.warning(\"Column layout detection failed: %s\", exc)\n\n    dict_ratio = \"n/a\" if quality.dictionary_hit_ratio is None else f\"{quality.dictionary_hit_ratio:.2f}\"\n    LOGGER.info(\n        \"Text-layer check: %s (avg_chars=%.1f, alpha_ratio=%.2f, suspicious=%.2f, dict=%s)\",\n        has_text_layer,\n        quality.avg_chars_per_page,\n        quality.alpha_ratio,\n        quality.suspicious_token_ratio,\n        dict_ratio,\n    )\n    if available_engines:\n        LOGGER.info(\"Available OCR engines: %s\", \", \".join(available_engines))\n    else:\n        LOGGER.info(\"Available OCR engines: none (external OCR disabled)\")\n\n    LOGGER.info(\n        \"Docling OCR route: %s (engine=%s, languages=%s)\",\n        decision.route_reason,\n        decision.ocr_engine,\n        decision.languages,\n    )\n    LOGGER.info(\"Per-page OCR: %s (%s)\", decision.per_page_ocr, decision.per_page_reason)\n    if decision.ocr_used and not decision.use_external_ocr:\n        LOGGER.info(\"External OCR unavailable; relying on Docling OCR.\")\n\n    converter = build_converter(config, decision)\n    docling_input = rasterized_pdf_path or pdf_path\n    emit(40, \"docling\", \"Docling conversion running\")\n    result = converter.convert(docling_input)\n    doc = result.document if hasattr(result, \"document\") else result\n    markdown = export_markdown(doc)\n    pages = extract_pages(doc)\n    if len(pages) <= 1:\n        fallback_pages = extract_pages_from_pdf(pdf_path)\n        if len(fallback_pages) > len(pages):\n            pages = fallback_pages\n    emit(70, \"docling\", \"Docling conversion complete\")\n\n    ocr_stats: Dict[str, Any] = {}\n    if decision.ocr_used and decision.use_external_ocr and decision.per_page_ocr and not rasterized_source:\n        try:\n            ocr_pages, ocr_stats = run_external_ocr_pages(\n                pdf_path,\n                decision.ocr_engine,\n                languages,\n                config,\n                progress_cb=emit,\n                progress_base=70,\n                progress_span=20,\n            )\n            if ocr_pages:\n                pages = ocr_pages\n                if config.postprocess_markdown and not markdown.strip():\n                    markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in ocr_pages)\n        except Exception as exc:\n            LOGGER.warning(\"External OCR failed (%s): %s\", decision.ocr_engine, exc)\n    if rasterized_source and rasterized_pdf_path:\n        try:\n            os.unlink(rasterized_pdf_path)\n        except Exception:\n            pass\n\n    emit(90, \"chunking\", \"Building chunks\")\n    metadata = {\n        \"ocr_used\": decision.ocr_used,\n        \"ocr_engine\": decision.ocr_engine,\n        \"languages\": decision.languages,\n        \"route_reason\": decision.route_reason,\n        \"per_page_reason\": decision.per_page_reason,\n        \"text_layer_detected\": has_text_layer,\n        \"text_layer_low_quality\": has_text_layer and low_quality,\n        \"rasterized_source_pdf\": rasterized_source,\n        \"rasterize_failed\": bool(rasterize_error),\n        \"rasterize_error\": rasterize_error,\n        \"column_layout_detected\": column_layout.detected if column_layout else None,\n        \"column_layout_ratio\": column_layout.page_ratio if column_layout else None,\n        \"column_layout_reason\": column_layout.reason if column_layout else None,\n        \"avg_chars_per_page\": quality.avg_chars_per_page,\n        \"alpha_ratio\": quality.alpha_ratio,\n        \"suspicious_token_ratio\": quality.suspicious_token_ratio,\n        \"confidence_proxy\": quality.confidence_proxy,\n        \"dictionary_hit_ratio\": quality.dictionary_hit_ratio,\n        \"per_page_ocr\": decision.per_page_ocr,\n    }\n    metadata.update(ocr_stats)\n    emit(100, \"done\", \"Extraction complete\")\n    return DoclingConversionResult(markdown=markdown, pages=pages, metadata=metadata)\n\n\ndef build_chunks_page(\n    doc_id: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    postprocess: Optional[Callable[[str], str]] = None,\n) -> List[Dict[str, Any]]:\n    chunks: List[Dict[str, Any]] = []\n    for page in pages:\n        raw_text = str(page.get(\"text\", \"\"))\n        if postprocess:\n            raw_text = postprocess(raw_text)\n        raw_text = clean_chunk_text(raw_text, config)\n        cleaned = normalize_text(raw_text)\n        if not cleaned:\n            continue\n        page_num = int(page.get(\"page_num\", 0))\n        chunk_id = f\"p{page_num}\"\n        chunks.append({\n            \"chunk_id\": chunk_id,\n            \"text\": cleaned,\n            \"page_start\": page_num,\n            \"page_end\": page_num,\n            \"section\": \"\",\n            \"char_count\": len(cleaned),\n        })\n    return chunks\n\n\ndef build_chunks_section(\n    doc_id: str,\n    markdown: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    postprocess: Optional[Callable[[str], str]] = None,\n) -> List[Dict[str, Any]]:\n    sections = split_markdown_sections(markdown)\n    chunks: List[Dict[str, Any]] = []\n    seen_ids: Dict[str, int] = {}\n\n    if not sections:\n        return build_chunks_page(doc_id, pages, config=config)\n\n    for idx, section in enumerate(sections, start=1):\n        title = section.get(\"title\", \"\")\n        text = section.get(\"text\", \"\")\n        if postprocess:\n            text = postprocess(text)\n        text = clean_chunk_text(text, config)\n        if not text.strip():\n            continue\n        base_id = slugify(title) or f\"section-{idx}\"\n        if base_id in seen_ids:\n            seen_ids[base_id] += 1\n            base_id = f\"{base_id}-{seen_ids[base_id]}\"\n        else:\n            seen_ids[base_id] = 1\n        max_chars = config.max_chunk_chars if config else 0\n        overlap_chars = config.chunk_overlap_chars if config else 0\n        segments = split_text_by_size(text, max_chars, overlap_chars)\n        for seg_idx, segment in enumerate(segments, start=1):\n            cleaned = normalize_text(segment)\n            if not cleaned:\n                continue\n            page_start, page_end = find_page_range(cleaned, pages, config)\n            chunk_id = base_id if seg_idx == 1 else f\"{base_id}-{seg_idx}\"\n            chunks.append({\n                \"chunk_id\": chunk_id,\n                \"text\": cleaned,\n                \"page_start\": page_start,\n                \"page_end\": page_end,\n                \"section\": title,\n                \"char_count\": len(cleaned),\n            })\n    return chunks\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Extract PDF content with Docling and produce chunks.\")\n    parser.add_argument(\"--pdf\", required=True, help=\"Path to PDF\")\n    parser.add_argument(\"--doc-id\", help=\"Document identifier\")\n    parser.add_argument(\"--out-json\", help=\"Output JSON path\")\n    parser.add_argument(\"--out-md\", help=\"Output markdown path\")\n    parser.add_argument(\"--chunking\", choices=[\"page\", \"section\"], default=\"page\")\n    parser.add_argument(\"--ocr\", choices=[\"auto\", \"force\", \"off\"], default=\"auto\")\n    parser.add_argument(\"--language-hint\", help=\"Language hint for OCR/quality (e.g., eng, deu, deu+eng)\")\n    parser.add_argument(\n        \"--max-chunk-chars\",\n        type=int,\n        help=\"Max chars for section chunks before splitting (section mode only).\",\n    )\n    parser.add_argument(\n        \"--chunk-overlap-chars\",\n        type=int,\n        help=\"Overlap chars when splitting large section chunks.\",\n    )\n    parser.add_argument(\n        \"--keep-image-tags\",\n        action=\"store_true\",\n        help=\"Preserve '<!-- image -->' tags instead of removing them.\",\n    )\n    parser.add_argument(\n        \"--force-ocr-low-quality\",\n        action=\"store_true\",\n        help=\"Force OCR when text layer appears low quality\",\n    )\n    parser.add_argument(\n        \"--quality-threshold\",\n        type=float,\n        help=\"Confidence threshold for treating text as low quality (0-1)\",\n    )\n    parser.add_argument(\"--quality-only\", action=\"store_true\", help=\"Output text-layer quality JSON and exit\")\n    parser.add_argument(\"--enable-llm-cleanup\", action=\"store_true\", help=\"Enable LLM cleanup for low-quality chunks\")\n    parser.add_argument(\"--llm-cleanup-base-url\", help=\"OpenAI-compatible base URL for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-api-key\", help=\"API key for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-model\", help=\"Model name for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-temperature\", type=float, help=\"Temperature for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-max-chars\", type=int, help=\"Max chars per chunk for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-min-quality\", type=float, help=\"Min quality threshold for LLM cleanup\")\n    parser.add_argument(\"--progress\", action=\"store_true\", help=\"Emit JSON progress events to stdout\")\n    args = parser.parse_args()\n\n    logging.basicConfig(level=logging.INFO)\n\n    if not os.path.isfile(args.pdf):\n        eprint(f\"PDF not found: {args.pdf}\")\n        return 2\n\n    if args.quality_only:\n        config = DoclingProcessingConfig(ocr_mode=args.ocr)\n        if args.force_ocr_low_quality:\n            config.force_ocr_on_low_quality_text = True\n        if args.quality_threshold is not None:\n            config.quality_confidence_threshold = args.quality_threshold\n        report = build_quality_report(args.pdf, config)\n        print(json.dumps(report))\n        return 0\n\n    if not args.doc_id or not args.out_json or not args.out_md:\n        eprint(\"Missing required arguments: --doc-id, --out-json, --out-md\")\n        return 2\n\n    try:\n        out_json_dir = os.path.dirname(args.out_json)\n        out_md_dir = os.path.dirname(args.out_md)\n        if out_json_dir:\n            os.makedirs(out_json_dir, exist_ok=True)\n        if out_md_dir:\n            os.makedirs(out_md_dir, exist_ok=True)\n    except Exception as exc:\n        eprint(f\"Failed to create output directories: {exc}\")\n        return 2\n\n    config = DoclingProcessingConfig(ocr_mode=args.ocr)\n    if args.force_ocr_low_quality:\n        config.force_ocr_on_low_quality_text = True\n    if args.quality_threshold is not None:\n        config.quality_confidence_threshold = args.quality_threshold\n    if args.language_hint:\n        config.language_hint = args.language_hint\n    if args.max_chunk_chars is not None:\n        config.max_chunk_chars = args.max_chunk_chars\n    if args.chunk_overlap_chars is not None:\n        config.chunk_overlap_chars = args.chunk_overlap_chars\n    if args.keep_image_tags:\n        config.cleanup_remove_image_tags = False\n    if args.enable_llm_cleanup:\n        config.enable_llm_correction = True\n    if args.llm_cleanup_base_url:\n        config.llm_cleanup_base_url = args.llm_cleanup_base_url\n    if args.llm_cleanup_api_key:\n        config.llm_cleanup_api_key = args.llm_cleanup_api_key\n    if args.llm_cleanup_model:\n        config.llm_cleanup_model = args.llm_cleanup_model\n    if args.llm_cleanup_temperature is not None:\n        config.llm_cleanup_temperature = args.llm_cleanup_temperature\n    if args.llm_cleanup_max_chars is not None:\n        config.llm_correction_max_chars = args.llm_cleanup_max_chars\n    if args.llm_cleanup_min_quality is not None:\n        config.llm_correction_min_quality = args.llm_cleanup_min_quality\n\n    config.llm_correct = build_llm_cleanup_callback(config)\n\n    progress_cb = make_progress_emitter(bool(args.progress))\n\n    try:\n        conversion = convert_pdf_with_docling(args.pdf, config, progress_cb=progress_cb)\n    except Exception as exc:\n        eprint(f\"Docling conversion failed: {exc}\")\n        return 2\n\n    markdown = conversion.markdown\n    if config.enable_post_correction and config.postprocess_markdown and conversion.metadata.get(\"ocr_used\"):\n        wordlist = prepare_dictionary_words(config)\n        languages = conversion.metadata.get(\"languages\", config.default_lang_english)\n        markdown = postprocess_text(markdown, config, languages, wordlist)\n\n    try:\n        with open(args.out_md, \"w\", encoding=\"utf-8\") as handle:\n            handle.write(markdown)\n    except Exception as exc:\n        eprint(f\"Failed to write markdown: {exc}\")\n        return 2\n\n    try:\n        pages = conversion.pages\n        languages = conversion.metadata.get(\"languages\", config.default_lang_english)\n        postprocess_fn: Optional[Callable[[str], str]] = None\n        if config.enable_post_correction and conversion.metadata.get(\"ocr_used\"):\n            wordlist = prepare_dictionary_words(config)\n            postprocess_fn = lambda text: postprocess_text(text, config, languages, wordlist)\n\n        if postprocess_fn:\n            pages = [\n                {\"page_num\": page.get(\"page_num\", idx + 1), \"text\": postprocess_fn(str(page.get(\"text\", \"\")))}\n                for idx, page in enumerate(pages)\n            ]\n\n        if args.chunking == \"section\":\n            chunks = build_chunks_section(\n                args.doc_id,\n                markdown,\n                pages,\n                config=config,\n                postprocess=postprocess_fn,\n            )\n        else:\n            chunks = build_chunks_page(args.doc_id, pages, config=config)\n    except Exception as exc:\n        eprint(f\"Failed to build chunks: {exc}\")\n        return 2\n\n    chunks = [chunk for chunk in chunks if chunk.get(\"text\")]\n\n    payload = {\n        \"doc_id\": args.doc_id,\n        \"source_pdf\": args.pdf,\n        \"chunks\": chunks,\n        \"metadata\": conversion.metadata,\n    }\n\n    try:\n        with open(args.out_json, \"w\", encoding=\"utf-8\") as handle:\n            json.dump(payload, handle, indent=2)\n    except Exception as exc:\n        eprint(f\"Failed to write JSON: {exc}\")\n        return 2\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "index_redisearch.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.2\nimport argparse\nimport json\nimport math\nimport os\nimport struct\nimport sys\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom utils_embedding import normalize_vector, vector_to_bytes, request_embedding\nimport redis\nimport requests\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef normalize_vector(values: List[float]) -> List[float]:\n    norm = math.sqrt(sum(v * v for v in values))\n    if norm == 0:\n        return values\n    return [v / norm for v in values]\n\n\ndef vector_to_bytes(values: List[float]) -> bytes:\n    return struct.pack(\"<\" + \"f\" * len(values), *values)\n\n\ndef request_embedding(base_url: str, api_key: str, model: str, text: str) -> List[float]:\n    url = base_url.rstrip(\"/\") + \"/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    response = requests.post(url, json={\"input\": text, \"model\": model}, headers=headers, timeout=120)\n    if response.status_code >= 400:\n        raise RuntimeError(f\"Embedding request failed: {response.status_code} {response.text}\")\n    payload = response.json()\n    data = payload.get(\"data\")\n    if not data:\n        raise RuntimeError(\"Embedding response missing data field\")\n    embedding = data[0].get(\"embedding\")\n    if not embedding:\n        raise RuntimeError(\"Embedding response missing embedding\")\n    return [float(x) for x in embedding]\n\n\ndef ensure_index(client: redis.Redis, index_name: str, prefix: str) -> None:\n    try:\n        client.execute_command(\"FT.INFO\", index_name)\n        ensure_schema_fields(client, index_name)\n        return\n    except redis.exceptions.ResponseError as exc:\n        message = str(exc).lower()\n        if \"unknown index name\" not in message:\n            raise\n\n    client.execute_command(\n        \"FT.CREATE\",\n        index_name,\n        \"ON\",\n        \"HASH\",\n        \"PREFIX\",\n        \"1\",\n        prefix,\n        \"SCHEMA\",\n    \n    \n    \n        \"TAG\",\n    \n    \n    \n        \"authors\",\n    \n    \n    \n        \"NUMERIC\",\n        \"page_end\",\n        \"NUMERIC\",\n        \"section\",\n        \"TEXT\",\n        \"text\",\n        \"TEXT\",\n        \"embedding\",\n        \"VECTOR\",\n        \"HNSW\",\n        \"6\",\n        \"TYPE\",\n        \"FLOAT32\",\n        \"DIM\",\n        \"768\",\n        \"DISTANCE_METRIC\",\n        \"COSINE\",\n    )\n\n\ndef ensure_schema_fields(client: redis.Redis, index_name: str) -> None:\n    fields: List[Tuple[str, List[str]]] = [\n        (\"attachment_key\", [\"TAG\"]),\n        (\"title\", [\"TEXT\"]),\n        (\"authors\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"tags\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"year\", [\"NUMERIC\"]),\n        (\"item_type\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n    ]\n    for name, spec in fields:\n        try:\n            client.execute_command(\"FT.ALTER\", index_name, \"SCHEMA\", \"ADD\", name, *spec)\n        except redis.exceptions.ResponseError as exc:\n            message = str(exc).lower()\n            if \"duplicate\" in message or \"already exists\" in message:\n                continue\n            raise\n\n\ndef infer_item_json_path(chunks_json: str, doc_id: str) -> Optional[str]:\n    base_name = f\"{doc_id}.json\"\n    chunks_dir = os.path.dirname(chunks_json)\n    candidates: List[str] = []\n    if os.path.basename(chunks_dir) == \"chunks\":\n        candidates.append(os.path.join(os.path.dirname(chunks_dir), \"items\", base_name))\n    marker = f\"{os.sep}chunks{os.sep}\"\n    if marker in chunks_json:\n        candidates.append(chunks_json.replace(marker, f\"{os.sep}items{os.sep}\"))\n    candidates.append(os.path.join(chunks_dir, base_name))\n    for candidate in candidates:\n        if os.path.isfile(candidate):\n            return candidate\n    return None\n\n\ndef parse_item_metadata(item_payload: Dict[str, Any]) -> Dict[str, Any]:\n    data = item_payload.get(\"data\") if isinstance(item_payload.get(\"data\"), dict) else item_payload\n    title = str(data.get(\"title\", \"\")).strip()\n    item_type = str(data.get(\"itemType\", \"\")).strip()\n    tags: List[str] = []\n    for tag in data.get(\"tags\", []) or []:\n        if isinstance(tag, dict):\n            value = str(tag.get(\"tag\", \"\")).strip()\n        else:\n            value = str(tag).strip()\n        if value:\n            tags.append(value)\n\n    creators = data.get(\"creators\", []) or []\n    authors: List[str] = []\n    for creator in creators:\n        if not isinstance(creator, dict):\n            continue\n        name = \"\"\n        if creator.get(\"name\"):\n            name = str(creator.get(\"name\", \"\")).strip()\n        else:\n            first = str(creator.get(\"firstName\", \"\")).strip()\n            last = str(creator.get(\"lastName\", \"\")).strip()\n            name = \" \".join(part for part in (first, last) if part)\n        if name:\n            authors.append(name)\n\n    year = 0\n    date_field = str(data.get(\"date\", \"\")).strip()\n    match = None\n    if date_field:\n        match = next(iter(__import__(\"re\").findall(r\"(1[5-9]\\\\d{2}|20\\\\d{2})\", date_field)), None)\n    if match:\n        try:\n            year = int(match)\n        except ValueError:\n            year = 0\n    elif isinstance(data.get(\"year\"), (int, float)):\n        year = int(data.get(\"year\"))\n\n    return {\n        \"title\": title,\n        \"authors\": \"|\".join(authors),\n        \"tags\": \"|\".join(tags),\n        \"year\": year,\n        \"item_type\": item_type,\n    }\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Index Docling chunks into RedisSearch.\")\n    parser.add_argument(\"--chunks-json\", required=True)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--item-json\")\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--upsert\", action=\"store_true\")\n    parser.add_argument(\"--progress\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if not os.path.isfile(args.chunks_json):\n        eprint(f\"Chunks JSON not found: {args.chunks_json}\")\n        return 2\n\n    try:\n        with open(args.chunks_json, \"r\", encoding=\"utf-8\") as handle:\n            payload = json.load(handle)\n    except Exception as exc:\n        eprint(f\"Failed to read chunks JSON: {exc}\")\n        return 2\n\n\n    doc_id = payload.get(\"doc_id\")\n    chunks = payload.get(\"chunks\")\n    if not doc_id or not isinstance(chunks, list):\n        eprint(\"Invalid chunks JSON schema\")\n        return 2\n\n    # Delete all existing chunk keys for this doc_id before indexing\n    pattern = f\"{args.prefix}{doc_id}:*\"\n    try:\n        keys_to_delete = client.keys(pattern)\n        if keys_to_delete:\n            client.delete(*keys_to_delete)\n            eprint(f\"Deleted {len(keys_to_delete)} existing chunk keys for doc_id {doc_id}\")\n    except Exception as exc:\n        eprint(f\"Failed to delete old chunk keys for doc_id {doc_id}: {exc}\")\n\n    attachment_key = None\n    try:\n        meta = payload.get(\"metadata\") if isinstance(payload.get(\"metadata\"), dict) else {}\n        key_val = meta.get(\"attachment_key\") if isinstance(meta, dict) else None\n        if isinstance(key_val, str) and key_val.strip():\n            attachment_key = key_val.strip()\n    except Exception:\n        attachment_key = None\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n\n    try:\n        ensure_index(client, args.index, args.prefix)\n    except Exception as exc:\n        eprint(f\"Failed to ensure index: {exc}\")\n        return 2\n\n    item_metadata: Dict[str, Any] = {}\n    item_json_path = args.item_json or infer_item_json_path(args.chunks_json, str(doc_id))\n    if item_json_path and os.path.isfile(item_json_path):\n        try:\n            with open(item_json_path, \"r\", encoding=\"utf-8\") as handle:\n                item_payload = json.load(handle)\n            item_metadata = parse_item_metadata(item_payload)\n        except Exception as exc:\n            eprint(f\"Failed to read item JSON metadata: {exc}\")\n\n    valid_chunks = []\n    for chunk in chunks:\n        text = str(chunk.get(\"text\", \"\"))\n        if not text.strip():\n            continue\n        chunk_id = chunk.get(\"chunk_id\")\n        if not chunk_id:\n            continue\n        valid_chunks.append(chunk)\n\n    total = len(valid_chunks)\n    current = 0\n\n    for chunk in valid_chunks:\n        current += 1\n        text = str(chunk.get(\"text\", \"\"))\n        chunk_id = chunk.get(\"chunk_id\")\n\n        stable_chunk_id = f\"{doc_id}:{chunk_id}\"\n        key = f\"{args.prefix}{stable_chunk_id}\"\n\n        if not args.upsert and client.exists(key):\n            continue\n\n        try:\n            embedding = request_embedding(args.embed_base_url, args.embed_api_key, args.embed_model, text)\n            if len(embedding) != 768:\n                raise RuntimeError(f\"Embedding dim mismatch: {len(embedding)}\")\n            embedding = normalize_vector(embedding)\n        except Exception as exc:\n            eprint(f\"Embedding failed for chunk {stable_chunk_id}: {exc}\")\n            return 2\n\n        fields: Dict[str, Any] = {\n            \"doc_id\": str(doc_id),\n            \"chunk_id\": stable_chunk_id,\n            \"attachment_key\": str(attachment_key or \"\"),\n            \"title\": str(item_metadata.get(\"title\", \"\")),\n            \"authors\": str(item_metadata.get(\"authors\", \"\")),\n            \"tags\": str(item_metadata.get(\"tags\", \"\")),\n            \"year\": int(item_metadata.get(\"year\", 0)),\n            \"item_type\": str(item_metadata.get(\"item_type\", \"\")),\n            \"source_pdf\": str(payload.get(\"source_pdf\", \"\")),\n            \"page_start\": int(chunk.get(\"page_start\", 0)),\n            \"page_end\": int(chunk.get(\"page_end\", 0)),\n            \"section\": str(chunk.get(\"section\", \"\")),\n            \"text\": text,\n            \"embedding\": vector_to_bytes(embedding),\n        }\n\n        try:\n            client.hset(key, mapping=fields)\n        except Exception as exc:\n            eprint(f\"Failed to index chunk {stable_chunk_id}: {exc}\")\n            return 2\n\n        if args.progress:\n            print(json.dumps({\"type\": \"progress\", \"current\": current, \"total\": total}), flush=True)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "rag_query_redisearch.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.2\n\nimport argparse\nimport json\nimport math\nfrom utils_embedding import normalize_vector, vector_to_bytes, request_embedding\nimport re\nimport struct\nimport sys\nfrom typing import Any, Dict, List\n\nimport redis\nimport requests\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\ndef is_temperature_unsupported(message: str) -> bool:\n    lowered = message.lower()\n    return \"temperature\" in lowered and (\n        \"not supported\" in lowered or \"unsupported\" in lowered or \"unknown parameter\" in lowered\n    )\n\n\ndef is_stream_unsupported(message: str) -> bool:\n    lowered = message.lower()\n    return \"stream\" in lowered and (\"not supported\" in lowered or \"unsupported\" in lowered or \"unknown parameter\" in lowered)\n\n\ndef request_chat(\n    base_url: str,\n    api_key: str,\n    model: str,\n    temperature: float,\n    system_prompt: str,\n    user_prompt: str,\n) -> str:\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    base_payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n    }\n    payload = dict(base_payload)\n    payload[\"temperature\"] = temperature\n\n    response = requests.post(url, json=payload, headers=headers, timeout=120)\n    response.encoding = \"utf-8\"\n    if response.status_code >= 400:\n        error_text = response.text\n        if is_temperature_unsupported(error_text):\n            response = requests.post(url, json=base_payload, headers=headers, timeout=120)\n            response.encoding = \"utf-8\"\n            if response.status_code >= 400:\n                raise RuntimeError(f\"Chat request failed: {response.status_code} {response.text}\")\n        else:\n            raise RuntimeError(f\"Chat request failed: {response.status_code} {error_text}\")\n\n    data = response.json()\n    choices = data.get(\"choices\")\n    if not choices:\n        raise RuntimeError(\"Chat response missing choices\")\n    message = choices[0].get(\"message\") or {}\n    content = message.get(\"content\")\n    if not content:\n        raise RuntimeError(\"Chat response missing content\")\n    return content\n\n\ndef request_chat_stream(\n    base_url: str,\n    api_key: str,\n    model: str,\n    temperature: float,\n    system_prompt: str,\n    user_prompt: str,\n    on_delta,\n) -> str:\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    base_payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        \"stream\": True,\n    }\n    payload = dict(base_payload)\n    payload[\"temperature\"] = temperature\n\n    response = requests.post(url, json=payload, headers=headers, timeout=120, stream=True)\n    response.encoding = \"utf-8\"\n    if response.status_code >= 400:\n        error_text = response.text\n        if is_temperature_unsupported(error_text):\n            response = requests.post(url, json=base_payload, headers=headers, timeout=120, stream=True)\n            response.encoding = \"utf-8\"\n            if response.status_code >= 400:\n                raise RuntimeError(f\"Chat request failed: {response.status_code} {response.text}\")\n        else:\n            raise RuntimeError(f\"Chat request failed: {response.status_code} {error_text}\")\n\n    content_parts: List[str] = []\n    for raw_line in response.iter_lines(decode_unicode=True):\n        if not raw_line:\n            continue\n        line = raw_line.strip()\n        if not line.startswith(\"data:\"):\n            continue\n        data = line[5:].strip()\n        if data == \"[DONE]\":\n            break\n        try:\n            payload = json.loads(data)\n        except Exception:\n            continue\n        choices = payload.get(\"choices\") or []\n        if not choices:\n            continue\n        delta = choices[0].get(\"delta\") or {}\n        piece = delta.get(\"content\")\n        if not piece:\n            continue\n        content_parts.append(piece)\n        on_delta(piece)\n\n    return \"\".join(content_parts)\n\n\ndef decode_value(value: Any) -> Any:\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\", errors=\"ignore\")\n    return value\n\n\ndef parse_results(raw: List[Any]) -> List[Dict[str, Any]]:\n    results: List[Dict[str, Any]] = []\n    if not raw or len(raw) < 2:\n        return results\n\n    for idx in range(1, len(raw), 2):\n        if idx + 1 >= len(raw):\n            break\n        fields_raw = raw[idx + 1]\n        if not isinstance(fields_raw, list):\n            continue\n        field_map: Dict[str, Any] = {}\n        for i in range(0, len(fields_raw), 2):\n            key = decode_value(fields_raw[i])\n            value = decode_value(fields_raw[i + 1]) if i + 1 < len(fields_raw) else \"\"\n            field_map[str(key)] = value\n        results.append(field_map)\n    return results\n\n\n_QUERY_STOPWORDS = {\n    \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"into\", \"over\",\n    \"under\", \"after\", \"before\", \"were\", \"was\", \"are\", \"is\", \"its\", \"their\",\n    \"then\", \"than\", \"which\", \"when\", \"where\", \"have\", \"has\", \"had\", \"onto\",\n    \"upon\", \"your\", \"yours\", \"they\", \"them\", \"these\", \"those\", \"will\", \"would\",\n    \"could\", \"should\", \"about\", \"there\", \"here\", \"while\", \"what\", \"why\", \"how\",\n    \"not\", \"but\", \"you\", \"your\", \"our\", \"ours\", \"his\", \"her\", \"she\", \"him\",\n    \"also\", \"such\", \"been\", \"being\", \"out\", \"one\", \"two\", \"three\", \"four\",\n    \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"more\", \"most\", \"some\",\n    \"many\", \"few\", \"each\", \"per\", \"was\", \"were\", \"did\", \"does\", \"do\",\n}\n\n\ndef extract_keywords(query: str) -> List[str]:\n    raw_tokens = re.findall(r\"[A-Za-z0-9][A-Za-z0-9'\\\\-]{2,}\", query)\n    keywords: List[str] = []\n    for token in raw_tokens:\n        cleaned = re.sub(r\"[^A-Za-z0-9]\", \"\", token)\n        if not cleaned:\n            continue\n        lower = cleaned.lower()\n        if lower in _QUERY_STOPWORDS:\n            continue\n        if token[:1].isupper() or len(cleaned) >= 5:\n            keywords.append(lower)\n    seen = set()\n    ordered: List[str] = []\n    for token in keywords:\n        if token in seen:\n            continue\n        seen.add(token)\n        ordered.append(token)\n    return ordered\n\n\ndef run_lexical_search(\n    client: redis.Redis,\n    index: str,\n    keywords: List[str],\n    limit: int,\n) -> List[Dict[str, Any]]:\n    if not keywords or limit <= 0:\n        return []\n    tokens = [re.sub(r\"[^A-Za-z0-9]\", \"\", token) for token in keywords]\n    tokens = [token for token in tokens if token]\n    if not tokens:\n        return []\n    query = \"@text:(\" + \"|\".join(tokens) + \")\"\n    try:\n        raw = client.execute_command(\n            \"FT.SEARCH\",\n            index,\n            query,\n            \"LIMIT\",\n            \"0\",\n            str(limit),\n            \"RETURN\",\n            \"9\",\n            \"doc_id\",\n            \"chunk_id\",\n            \"attachment_key\",\n            \"source_pdf\",\n            \"page_start\",\n            \"page_end\",\n            \"section\",\n            \"text\",\n            \"score\",\n            \"DIALECT\",\n            \"2\",\n        )\n    except Exception:\n        return []\n    return parse_results(raw)\n\ndef is_content_chunk(chunk: Dict[str, Any]) -> bool:\n    text = chunk.get(\"text\", \"\")\n    if not text:\n        return False\n\n    # 1. Minimum length (filters title pages, citations)\n    if len(text) < 500:\n        return False\n\n    # 2. Must contain narrative sentences\n    # (bibliographies rarely have multiple full sentences)\n    if text.count(\". \") < 3:\n        return False\n\n    return True\n\ndef looks_narrative(text: str) -> bool:\n    if not text:\n        return False\n\n    # Must contain several complete sentences\n    if text.count(\". \") < 4:\n        return False\n\n    # Optional: avoid list-like text\n    if text.count(\"\\n\") > len(text) / 80:\n        return False\n\n    return True\n\ndef build_context(retrieved: List[Dict[str, Any]]) -> str:\n    blocks = []\n    for chunk in retrieved:\n        doc_id = chunk.get(\"doc_id\", \"\")\n        chunk_id = chunk.get(\"chunk_id\", \"\")\n        source_pdf = chunk.get(\"source_pdf\", \"\")\n        page_start = chunk.get(\"page_start\", \"\")\n        page_end = chunk.get(\"page_end\", \"\")\n        score = chunk.get(\"score\", \"\")\n        text = chunk.get(\"text\", \"\")\n        pages = f\"{page_start}-{page_end}\"\n        block = (\n            f\"<Document source='{source_pdf}' pages='{pages}' doc_id='{doc_id}' \"\n            f\"chunk_id='{chunk_id}' score='{score}'>\\n{text}\\n</Document>\"\n        )\n        blocks.append(block)\n    return \"\\n\\n\".join(blocks)\n\n\ndef load_history_messages(path: str) -> List[Dict[str, Any]]:\n    if not path:\n        return []\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as handle:\n            payload = json.load(handle)\n    except Exception:\n        return []\n    if isinstance(payload, list):\n        return [item for item in payload if isinstance(item, dict)]\n    messages = payload.get(\"messages\") if isinstance(payload, dict) else None\n    if isinstance(messages, list):\n        return [item for item in messages if isinstance(item, dict)]\n    return []\n\n\ndef format_history_block(messages: List[Dict[str, Any]]) -> str:\n    lines: List[str] = []\n    for message in messages:\n        role = str(message.get(\"role\", \"\")).strip().lower()\n        content = str(message.get(\"content\", \"\")).strip()\n        if not content:\n            continue\n        if role not in (\"user\", \"assistant\"):\n            role = \"user\"\n        label = \"User\" if role == \"user\" else \"Assistant\"\n        lines.append(f\"{label}: {content}\")\n    return \"\\n\".join(lines)\n\n\ndef extract_annotation_key(chunk_id: str) -> str:\n    if not chunk_id:\n        return \"\"\n    if \":\" in chunk_id:\n        chunk_id = chunk_id.split(\":\", 1)[1]\n    candidate = chunk_id.strip().upper()\n    if re.fullmatch(r\"[A-Z0-9]{8}\", candidate):\n        return candidate\n    return \"\"\n\n\ndef build_citations(retrieved: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    seen = set()\n    citations: List[Dict[str, Any]] = []\n    for chunk in retrieved:\n        doc_id = chunk.get(\"doc_id\", \"\")\n        chunk_id = chunk.get(\"chunk_id\", \"\")\n        attachment_key = chunk.get(\"attachment_key\", \"\")\n        page_start = chunk.get(\"page_start\", \"\")\n        page_end = chunk.get(\"page_end\", \"\")\n        source_pdf = chunk.get(\"source_pdf\", \"\")\n        key = (doc_id, attachment_key, page_start, page_end, source_pdf)\n        if key in seen:\n            continue\n        seen.add(key)\n        annotation_key = extract_annotation_key(str(chunk_id))\n        citations.append({\n            \"doc_id\": doc_id,\n            \"chunk_id\": chunk_id,\n            \"attachment_key\": attachment_key,\n            \"annotation_key\": annotation_key or None,\n            \"page_start\": page_start,\n            \"page_end\": page_end,\n            \"pages\": f\"{page_start}-{page_end}\",\n            \"source_pdf\": source_pdf,\n        })\n    return citations\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Query RedisSearch and answer with RAG.\")\n    parser.add_argument(\"--query\", required=True)\n    parser.add_argument(\"--k\", type=int, default=10)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--chat-base-url\", required=True)\n    parser.add_argument(\"--chat-api-key\", default=\"\")\n    parser.add_argument(\"--chat-model\", required=True)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--stream\", action=\"store_true\")\n    parser.add_argument(\"--history-file\", help=\"Optional JSON file with recent chat history\")\n    args = parser.parse_args()\n\n    try:\n        embedding = request_embedding(args.embed_base_url, args.embed_api_key, args.embed_model, args.query)\n        if len(embedding) != 768:\n            raise RuntimeError(f\"Embedding dim mismatch: {len(embedding)}\")\n        embedding = normalize_vector(embedding)\n        vec = vector_to_bytes(embedding)\n    except Exception as exc:\n        eprint(f\"Failed to embed query: {exc}\")\n        return 2\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n\n    try:\n        raw = client.execute_command(\n            \"FT.SEARCH\",\n            args.index,\n            f\"*=>[KNN {args.k} @embedding $vec AS score]\",\n            \"PARAMS\",\n            \"2\",\n            \"vec\",\n            vec,\n            \"SORTBY\",\n            \"score\",\n            \"RETURN\",\n            \"9\",\n            \"doc_id\",\n            \"chunk_id\",\n            \"attachment_key\",\n            \"source_pdf\",\n            \"page_start\",\n            \"page_end\",\n            \"section\",\n            \"text\",\n            \"score\",\n            \"DIALECT\",\n            \"2\",\n        )\n    except Exception as exc:\n        eprint(f\"RedisSearch query failed: {exc}\")\n        return 2\n\n    retrieved = parse_results(raw)\n\n    # merge lexical results (unchanged)\n    keywords = extract_keywords(args.query)\n    lexical_limit = max(args.k, 5)\n    lexical_results = run_lexical_search(client, args.index, keywords, lexical_limit)\n    if lexical_results:\n        seen = {item.get(\"chunk_id\") for item in retrieved if item.get(\"chunk_id\")}\n        for item in lexical_results:\n            chunk_id = item.get(\"chunk_id\")\n            if not chunk_id or chunk_id in seen:\n                continue\n            retrieved.append(item)\n            seen.add(chunk_id)\n\n        max_total = args.k + lexical_limit\n        if len(retrieved) > max_total:\n            retrieved = retrieved[:max_total]\n\n    # Strict filter\n    filtered = [\n        c for c in retrieved\n        if is_content_chunk(c) and looks_narrative(c.get(\"text\", \"\"))\n    ]\n\n    # Fallback: if too strict, keep \"contentful\" chunks at least (from ORIGINAL retrieved)\n    if not filtered:\n        filtered = [c for c in retrieved if is_content_chunk(c)]\n\n    retrieved = filtered\n\n    context = build_context(retrieved)\n\n    system_prompt = (\n        \"Use ONLY the provided context for factual claims. If insufficient, say you do not know. \"\n        \"Chat history is only for conversational continuity or for providing concepts to be retrieved. \"\n        \"Add inline citations using this exact format: [[cite:DOC_ID:PAGE_START-PAGE_END]]. \"\n        \"Example: ... [[cite:ABC123:12-13]].\"\n    )\n    history_messages = load_history_messages(args.history_file) if args.history_file else []\n    history_block = format_history_block(history_messages)\n    if history_block:\n        history_block = f\"Chat history (for reference only):\\n{history_block}\\n\\n\"\n    user_prompt = f\"{history_block}Question: {args.query}\\n\\nContext:\\n{context}\"\n\n    citations = build_citations(retrieved)\n\n    answer = \"\"\n    streamed = False\n    if args.stream:\n        def emit(obj: Dict[str, Any]) -> None:\n            print(json.dumps(obj, ensure_ascii=False), flush=True)\n\n        try:\n            answer = request_chat_stream(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                args.temperature,\n                system_prompt,\n                user_prompt,\n                lambda chunk: emit({\"type\": \"delta\", \"content\": chunk}),\n            )\n            streamed = True\n        except Exception as exc:\n            if is_stream_unsupported(str(exc)):\n                streamed = False\n            else:\n                eprint(f\"Chat request failed: {exc}\")\n                return 2\n\n    if not streamed:\n        try:\n            answer = request_chat(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                args.temperature,\n                system_prompt,\n                user_prompt,\n            )\n        except Exception as exc:\n            eprint(f\"Chat request failed: {exc}\")\n            return 2\n\n    output = {\n        \"query\": args.query,\n        \"answer\": answer,\n        \"citations\": citations,\n        \"retrieved\": retrieved,\n    }\n\n    if args.stream and streamed:\n        print(json.dumps({\"type\": \"final\", **output}, ensure_ascii=False), flush=True)\n    else:\n        print(json.dumps(output, ensure_ascii=False))\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "batch_index_pyzotero.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.2\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Set\n\nfrom pyzotero import zotero\nfrom tqdm import tqdm\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef ensure_dir(path: Path) -> None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef load_checkpoint(path: Path) -> Set[str]:\n    if not path.exists():\n        return set()\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        items = data.get(\"processed\", [])\n        return set(str(x) for x in items)\n    except Exception:\n        return set()\n\n\ndef save_checkpoint(path: Path, processed: Set[str]) -> None:\n    payload = {\"processed\": sorted(processed)}\n    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n\n\ndef run_script(script: Path, args: List[str]) -> None:\n    command = [sys.executable, str(script)] + args\n    result = subprocess.run(command, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise RuntimeError(result.stderr.strip() or f\"Command failed: {' '.join(command)}\")\n\n\ndef fetch_parent_item(client: zotero.Zotero, parent_key: str) -> Dict[str, Any]:\n    try:\n        item = client.item(parent_key)\n        if isinstance(item, list):\n            return item[0] if item else {}\n        return item\n    except Exception:\n        return {}\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Batch index a Zotero library with Docling and RedisSearch.\")\n    parser.add_argument(\"--library-id\", required=True)\n    parser.add_argument(\"--library-type\", choices=[\"user\", \"group\"], required=True)\n    parser.add_argument(\"--api-key\", required=True)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--out-dir\", default=\"./data\")\n    parser.add_argument(\"--ocr\", choices=[\"auto\", \"force\", \"off\"], default=\"auto\")\n    parser.add_argument(\"--chunking\", choices=[\"page\", \"section\"], default=\"page\")\n    parser.add_argument(\"--limit\", type=int)\n    parser.add_argument(\"--since\", type=int)\n    parser.add_argument(\"--reindex\", action=\"store_true\")\n    args = parser.parse_args()\n\n    out_dir = Path(args.out_dir).resolve()\n    pdf_dir = out_dir / \"pdfs\"\n    item_dir = out_dir / \"items\"\n    doc_dir = out_dir / \"docs\"\n    chunk_dir = out_dir / \"chunks\"\n    checkpoint_path = out_dir / \"checkpoint.json\"\n\n    for folder in (pdf_dir, item_dir, doc_dir, chunk_dir):\n        ensure_dir(folder)\n\n    processed = set() if args.reindex else load_checkpoint(checkpoint_path)\n\n    client = zotero.Zotero(args.library_id, args.library_type, args.api_key)\n\n    params: Dict[str, Any] = {\"itemType\": \"attachment\"}\n    if args.limit:\n        params[\"limit\"] = args.limit\n    if args.since:\n        params[\"since\"] = args.since\n\n    try:\n        attachments = client.everything(client.items(**params))\n    except Exception as exc:\n        eprint(f\"Failed to fetch Zotero items: {exc}\")\n        return 2\n\n    pdf_items = []\n    for item in attachments:\n        data = item.get(\"data\", {})\n        content_type = data.get(\"contentType\", \"\") or \"\"\n        if content_type.startswith(\"application/pdf\"):\n            pdf_items.append(item)\n\n    script_dir = Path(__file__).resolve().parent\n    docling_script = script_dir / \"docling_extract.py\"\n    index_script = script_dir / \"index_redisearch.py\"\n\n    errors: List[str] = []\n\n    for item in tqdm(pdf_items, desc=\"Indexing PDFs\"):\n        attachment_key = item.get(\"key\")\n        if not attachment_key:\n            continue\n        parent_key = item.get(\"data\", {}).get(\"parentItem\")\n        doc_id = parent_key or attachment_key\n\n        if doc_id in processed:\n            continue\n\n        pdf_path = pdf_dir / f\"{attachment_key}.pdf\"\n        item_path = item_dir / f\"{doc_id}.json\"\n        doc_path = doc_dir / f\"{doc_id}.md\"\n        chunk_path = chunk_dir / f\"{doc_id}.json\"\n\n        try:\n            content = client.file(attachment_key)\n            if not content:\n                raise RuntimeError(\"Empty PDF content\")\n            pdf_path.write_bytes(content)\n        except Exception as exc:\n            errors.append(f\"{doc_id}: download failed ({exc})\")\n            continue\n\n        try:\n            metadata = fetch_parent_item(client, parent_key) if parent_key else item\n            item_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n        except Exception as exc:\n            errors.append(f\"{doc_id}: metadata write failed ({exc})\")\n            continue\n\n        try:\n            run_script(\n                docling_script,\n                [\n                    \"--pdf\",\n                    str(pdf_path),\n                    \"--doc-id\",\n                    doc_id,\n                    \"--out-json\",\n                    str(chunk_path),\n                    \"--out-md\",\n                    str(doc_path),\n                    \"--chunking\",\n                    args.chunking,\n                    \"--ocr\",\n                    args.ocr,\n                ],\n            )\n        except Exception as exc:\n            errors.append(f\"{doc_id}: docling failed ({exc})\")\n            continue\n\n        try:\n            run_script(\n                index_script,\n                [\n                    \"--chunks-json\",\n                    str(chunk_path),\n                    \"--redis-url\",\n                    args.redis_url,\n                    \"--index\",\n                    args.index,\n                    \"--prefix\",\n                    args.prefix,\n                    \"--embed-base-url\",\n                    args.embed_base_url,\n                    \"--embed-api-key\",\n                    args.embed_api_key,\n                    \"--embed-model\",\n                    args.embed_model,\n                ],\n            )\n        except Exception as exc:\n            errors.append(f\"{doc_id}: redis index failed ({exc})\")\n            continue\n\n        processed.add(doc_id)\n        save_checkpoint(checkpoint_path, processed)\n\n    if errors:\n        eprint(\"Failures:\")\n        for entry in errors:\n            eprint(f\"- {entry}\")\n\n    eprint(f\"Processed {len(processed)} items. Errors: {len(errors)}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "utils_embedding.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.2\nimport math\nimport struct\nimport requests\nfrom typing import List\n\ndef normalize_vector(values: List[float]) -> List[float]:\n    norm = math.sqrt(sum(v * v for v in values))\n    if norm == 0:\n        return values\n    return [v / norm for v in values]\n\ndef vector_to_bytes(values: List[float]) -> bytes:\n    return struct.pack(\"<\" + \"f\" * len(values), *values)\n\ndef request_embedding(base_url: str, api_key: str, model: str, text: str) -> List[float]:\n    url = base_url.rstrip(\"/\") + \"/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n    response = requests.post(url, json={\"input\": text, \"model\": model}, headers=headers, timeout=120)\n    if response.status_code >= 400:\n        raise RuntimeError(f\"Embedding request failed: {response.status_code} {response.text}\")\n    payload = response.json()\n    data = payload.get(\"data\")\n    if not data:\n        raise RuntimeError(\"Embedding response missing data field\")\n    embedding = data[0].get(\"embedding\")\n    if not embedding:\n        raise RuntimeError(\"Embedding response missing embedding\")\n    return [float(x) for x in embedding]\n",
  "ocr_wordlist.txt": "# zotero-redisearch-rag tool version: 0.2.2\n# Minimal English/German wordlist for optional OCR correction\nabstract\nanalysis\nappendix\nbibliography\nchapter\nconclusion\ndata\ndocument\ndiscussion\nexample\nfigure\nhistory\nintroduction\nlibrary\nmethod\nmethods\nmodel\nnumber\npage\npages\nreference\nresearch\nresults\nscience\nsection\nstudy\nsystem\ntable\ntext\nand\nare\nfor\nfrom\nin\nof\nthe\nto\nwith\naber\naus\nbei\nder\ndie\ndas\nein\neine\nfuer\ngeschichte\nist\nkann\nkoennen\nmit\nmuessen\nnicht\nueber\nund\nwurde\nwerden\nzum\nzur\nzusammenfassung\nabbildung\ntabelle\nmethode\nanalyse\n",
  "docker-compose.yml": "# zotero-redisearch-rag tool version: 0.2.2\nservices:\n  redis-stack:\n    image: redis/redis-stack-server:latest\n    command: [\"redis-stack-server\", \"/redis-stack.conf\", \"--dir\", \"/data\"]\n    environment:\n      - REDIS_ARGS=\n    ports:\n      - \"${ZRR_PORT:-6379}:6379\"\n    volumes:\n      - \"${ZRR_DATA_DIR:-./.zotero-redisearch-rag/redis-data}:/data\"\n      - \"./redis-stack.conf:/redis-stack.conf:ro\"\n",
  "redis-stack.conf": "# zotero-redisearch-rag tool version: 0.2.2\n# Redis Stack persistence config for local RAG index\nappendonly yes\nappendfsync everysec\n\ndir /data\n",
};