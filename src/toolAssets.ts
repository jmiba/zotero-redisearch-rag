export const TOOL_ASSETS: Record<string, string> = {
  "docling_extract.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.3\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport shutil\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple\nimport langcodes\nimport warnings\n\n# Reduce noisy warnings and route them to logging\nlogging.captureWarnings(True)\ntry:\n    from PIL import Image as _PILImage  # type: ignore\n    # Disable DecompressionBomb warnings (we control DPI); still safe for local PDFs\n    _PILImage.MAX_IMAGE_PIXELS = None  # type: ignore[attr-defined]\n    if hasattr(_PILImage, \"DecompressionBombWarning\"):\n        warnings.filterwarnings(\"ignore\", category=_PILImage.DecompressionBombWarning)  # type: ignore[attr-defined]\nexcept Exception:\n    pass\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\nLOGGER = logging.getLogger(\"docling_extract\")\n\n# Stores details about the last spellchecker built (backend and dictionary files)\n# Example: {\"backend\": \"spylls\", \"aff\": \"/path/en_GB.aff\", \"dic\": \"/path/en_GB.dic\"}\nLAST_SPELLCHECKER_INFO: Dict[str, Any] = {}\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\nProgressCallback = Callable[[int, str, str], None]\n\n\ndef make_progress_emitter(enabled: bool) -> ProgressCallback:\n    if not enabled:\n        def _noop(percent: int, stage: str, message: str) -> None:\n            return None\n        return _noop\n\n    def _emit(percent: int, stage: str, message: str) -> None:\n        payload = {\n            \"type\": \"progress\",\n            \"percent\": max(0, min(100, int(percent))),\n            \"stage\": stage,\n            \"message\": message,\n        }\n        print(json.dumps(payload), flush=True)\n\n    return _emit\n\n\n@dataclass\nclass DoclingProcessingConfig:\n    ocr_mode: str = \"auto\"\n    prefer_ocr_engine: str = \"paddle\"\n    fallback_ocr_engine: str = \"tesseract\"\n    language_hint: Optional[str] = None\n    default_lang_german: str = \"deu+eng\"\n    default_lang_english: str = \"eng\"\n    min_text_chars_per_page: int = 40\n    min_text_pages_ratio: float = 0.3\n    quality_alpha_ratio_threshold: float = 0.6\n    quality_suspicious_token_threshold: float = 0.25\n    quality_min_avg_chars_per_page: int = 80\n    quality_confidence_threshold: float = 0.5\n    quality_use_wordfreq: bool = True\n    quality_wordfreq_min_zipf: float = 3.0\n    column_detect_enable: bool = True\n    column_detect_dpi: int = 150\n    column_detect_max_pages: int = 3\n    column_detect_crop_top_ratio: float = 0.08\n    column_detect_crop_bottom_ratio: float = 0.08\n    column_detect_threshold_std_mult: float = 1.0\n    column_detect_threshold_min: int = 120\n    column_detect_threshold_max: int = 210\n    column_detect_text_percentile: float = 0.7\n    column_detect_min_text_density: float = 0.02\n    column_detect_gap_threshold_ratio: float = 0.2\n    column_detect_min_gap_density: float = 0.01\n    column_detect_min_gap_ratio: float = 0.03\n    column_detect_min_pages_ratio: float = 0.4\n    column_detect_smooth_window: int = 5\n    page_range_sample_tokens: int = 200\n    page_range_min_overlap: float = 0.02\n    page_range_min_hits: int = 5\n    page_range_top_k: int = 5\n    page_range_peak_ratio: float = 0.5\n    page_range_cluster_gap: int = 1\n    page_range_max_span_ratio: float = 0.7\n    max_chunk_chars: int = 3000\n    chunk_overlap_chars: int = 250\n    cleanup_remove_image_tags: bool = True\n    per_page_ocr_on_low_quality: bool = True\n    force_ocr_on_low_quality_text: bool = False\n    enable_post_correction: bool = True\n    enable_dictionary_correction: bool = False\n    dictionary_path: Optional[str] = None\n    dictionary_words: Optional[Sequence[str]] = None\n    default_dictionary_name: str = \"ocr_wordlist.txt\"\n    enable_llm_correction: bool = False\n    llm_correct: Optional[Callable[[str], str]] = None\n    llm_cleanup_base_url: Optional[str] = None\n    llm_cleanup_api_key: Optional[str] = None\n    llm_cleanup_model: Optional[str] = None\n    llm_cleanup_temperature: float = 0.0\n    llm_cleanup_timeout_sec: int = 60\n    llm_correction_min_quality: float = 0.35\n    llm_correction_max_chars: int = 2000\n    postprocess_markdown: bool = False\n    analysis_max_pages: int = 5\n    analysis_sample_strategy: str = \"middle\"\n    ocr_dpi: int = 300\n    # Optional Hunspell integration\n    enable_hunspell: bool = True\n    hunspell_aff_path: Optional[str] = None\n    hunspell_dic_path: Optional[str] = None\n\n\n@dataclass\nclass OcrRouteDecision:\n    ocr_used: bool\n    ocr_engine: str\n    languages: str\n    route_reason: str\n    use_external_ocr: bool\n    per_page_ocr: bool\n    per_page_reason: str\n\n\n@dataclass\nclass TextQuality:\n    avg_chars_per_page: float\n    alpha_ratio: float\n    suspicious_token_ratio: float\n    confidence_proxy: float\n    dictionary_hit_ratio: Optional[float] = None\n\n@dataclass\nclass ColumnLayoutDetection:\n    detected: bool\n    page_ratio: float\n    reason: str\n\n@dataclass\nclass DoclingConversionResult:\n    markdown: str\n    pages: List[Dict[str, Any]]\n    metadata: Dict[str, Any]\n\n\ndef normalize_text(text: str) -> str:\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n\ndef remove_image_placeholders(text: str) -> str:\n    return re.sub(r\"<!--\\s*image\\s*-->\", \"\", text, flags=re.IGNORECASE)\n\n\ndef clean_chunk_text(text: str, config: Optional[DoclingProcessingConfig]) -> str:\n    if not text:\n        return \"\"\n    cleaned = text\n    if config and config.cleanup_remove_image_tags:\n        cleaned = remove_image_placeholders(cleaned)\n    return cleaned\n\n\ndef normalize_whitespace(text: str) -> str:\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()\n\n\ndef dehyphenate_text(text: str) -> str:\n    return re.sub(r\"(?<=\\w)-\\s*\\n\\s*(?=\\w)\", \"\", text)\n\n\ndef replace_ligatures(text: str) -> str:\n    return (\n        text.replace(\"\\ufb01\", \"fi\")\n        .replace(\"\\ufb02\", \"fl\")\n        .replace(\"\\ufb03\", \"ffi\")\n        .replace(\"\\ufb04\", \"ffl\")\n    )\n\n\ndef split_paragraphs(text: str) -> List[str]:\n    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n    return [para.strip() for para in paragraphs if para.strip()]\n\n\ndef split_long_text(text: str, max_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n    if len(sentences) <= 1:\n        return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n    for sentence in sentences:\n        sent = sentence.strip()\n        if not sent:\n            continue\n        if current_len + len(sent) + 1 > max_chars and current:\n            chunks.append(\" \".join(current).strip())\n            current = [sent]\n            current_len = len(sent)\n        else:\n            current.append(sent)\n            current_len += len(sent) + 1\n    if current:\n        chunks.append(\" \".join(current).strip())\n    return chunks\n\n\ndef split_text_by_size(text: str, max_chars: int, overlap_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    paragraphs = split_paragraphs(text) or [text]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n\n    def flush() -> None:\n        nonlocal current, current_len\n        if not current:\n            return\n        chunk = \"\\n\\n\".join(current).strip()\n        chunks.append(chunk)\n        current = []\n        current_len = 0\n\n    for para in paragraphs:\n        for piece in split_long_text(para, max_chars):\n            piece_len = len(piece)\n            if current_len + piece_len + 2 > max_chars and current:\n                flush()\n            current.append(piece)\n            current_len += piece_len + 2\n\n    flush()\n\n    if overlap_chars <= 0 or len(chunks) <= 1:\n        return chunks\n\n    overlapped: List[str] = []\n    previous = \"\"\n    for chunk in chunks:\n        if previous:\n            overlap = previous[-overlap_chars:]\n            combined = f\"{overlap}\\n{chunk}\".strip()\n        else:\n            combined = chunk\n        overlapped.append(combined)\n        previous = chunk\n    return overlapped\n\n\ndef select_wordfreq_languages(languages: str) -> List[str]:\n    lang = (languages or \"\").lower()\n    selected: List[str] = []\n    if any(token in lang for token in (\"deu\", \"ger\", \"de\", \"german\", \"deutsch\")):\n        selected.append(\"de\")\n    if any(token in lang for token in (\"eng\", \"en\", \"english\")):\n        selected.append(\"en\")\n    if not selected:\n        selected.append(\"en\")\n    return selected\n\n\ndef compute_dictionary_hit_ratio(\n    tokens: Sequence[str],\n    languages: str,\n    min_zipf: float,\n) -> Optional[float]:\n    try:\n        from wordfreq import zipf_frequency\n    except Exception:\n        return None\n\n    if not tokens:\n        return None\n    lang_codes = select_wordfreq_languages(languages)\n    hits = 0\n    total = 0\n    for token in tokens:\n        lower = token.lower()\n        if len(lower) < 2:\n            continue\n        total += 1\n        if any(zipf_frequency(lower, lang) >= min_zipf for lang in lang_codes):\n            hits += 1\n    if not total:\n        return None\n    return hits / total\n\n\ndef estimate_text_quality(\n    pages: Sequence[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    languages: Optional[str] = None,\n) -> TextQuality:\n    if not pages:\n        return TextQuality(0.0, 0.0, 1.0, 0.0, None)\n\n    texts = [str(page.get(\"text\", \"\")) for page in pages]\n    total_chars = sum(len(text) for text in texts)\n    alpha_chars = sum(sum(char.isalpha() for char in text) for text in texts)\n    alpha_ratio = alpha_chars / max(1, total_chars)\n\n    tokens = re.findall(r\"[A-Za-z0-9]+\", \" \".join(texts))\n    suspicious_tokens = [\n        token for token in tokens\n        if (sum(char.isdigit() for char in token) / max(1, len(token))) > 0.5\n        or re.search(r\"(.)\\1\\1\", token)\n    ]\n    suspicious_ratio = len(suspicious_tokens) / max(1, len(tokens))\n\n    avg_chars = total_chars / max(1, len(pages))\n    dictionary_hit_ratio = None\n    if config and config.quality_use_wordfreq and languages:\n        dictionary_hit_ratio = compute_dictionary_hit_ratio(\n            tokens,\n            languages,\n            config.quality_wordfreq_min_zipf,\n        )\n    confidence = alpha_ratio * (1.0 - suspicious_ratio)\n    if dictionary_hit_ratio is not None:\n        confidence *= 0.4 + (0.6 * dictionary_hit_ratio)\n    confidence = max(0.0, min(1.0, confidence))\n    return TextQuality(avg_chars, alpha_ratio, suspicious_ratio, confidence, dictionary_hit_ratio)\n\n\ndef detect_text_layer_from_pages(pages: Sequence[Dict[str, Any]], config: DoclingProcessingConfig) -> bool:\n    if not pages:\n        return False\n    pages_with_text = 0\n    for page in pages:\n        cleaned = normalize_text(str(page.get(\"text\", \"\")))\n        if len(cleaned) >= config.min_text_chars_per_page:\n            pages_with_text += 1\n    ratio = pages_with_text / max(1, len(pages))\n    return ratio >= config.min_text_pages_ratio\n\n\ndef is_low_quality(quality: TextQuality, config: DoclingProcessingConfig) -> bool:\n    if quality.confidence_proxy < config.quality_confidence_threshold:\n        return True\n    return (\n        quality.avg_chars_per_page < config.quality_min_avg_chars_per_page\n        or quality.alpha_ratio < config.quality_alpha_ratio_threshold\n        or quality.suspicious_token_ratio > config.quality_suspicious_token_threshold\n    )\n\n\ndef should_rasterize_text_layer(has_text_layer: bool, low_quality: bool, config: DoclingProcessingConfig) -> bool:\n    if config.ocr_mode == \"force\":\n        return True\n    return bool(has_text_layer and low_quality and config.force_ocr_on_low_quality_text)\n\n\ndef decide_per_page_ocr(\n    has_text_layer: bool,\n    quality: TextQuality,\n    config: DoclingProcessingConfig,\n) -> Tuple[bool, str]:\n    if not config.per_page_ocr_on_low_quality:\n        return False, \"Per-page OCR disabled by config\"\n    if not has_text_layer and is_low_quality(quality, config):\n        return True, \"Low-quality scan detected\"\n    if quality.suspicious_token_ratio > config.quality_suspicious_token_threshold:\n        return True, \"High suspicious token ratio\"\n    if quality.avg_chars_per_page < config.quality_min_avg_chars_per_page:\n        return True, \"Low text density\"\n    return False, \"Quality metrics acceptable\"\n\n\ndef select_language_set(\n    language_hint: Optional[str],\n    filename: str,\n    config: DoclingProcessingConfig,\n) -> str:\n    hint = (language_hint or \"\").lower().strip()\n    name = os.path.basename(filename).lower()\n\n    # import langcodes\n\n    def normalize_hint(h: str) -> str:\n        if not h:\n            return \"\"\n        try:\n            lang = langcodes.find(h)\n            code = lang.to_alpha3()\n            if code == \"deu\":\n                return config.default_lang_german\n            if code == \"eng\":\n                return config.default_lang_english\n            if code == \"fra\":\n                return \"fra+eng\"  # French + English fallback\n            if code == \"pol\":\n                return \"pol+eng\"  # Polish + English fallback\n            # Add more as needed\n            return code\n        except Exception:\n            return h\n\n    if hint:\n        return normalize_hint(hint)\n\n    # Try to infer from filename using langcodes\n    for pattern, lang_code in [\n        (r\"(\\bde\\b|_de\\b|-de\\b|deu|german|deutsch)\", config.default_lang_german),\n        (r\"(\\bfr\\b|_fr\\b|-fr\\b|fra|french|francais|français)\", \"fra+eng\"),\n        (r\"(\\bit\\b|_it\\b|-it\\b|ita|italian|italiano)\", \"ita+eng\"),\n        (r\"(\\bes\\b|_es\\b|-es\\b|spa|spanish|espanol|español)\", \"spa+eng\"),\n        (r\"(\\bpl\\b|_pl\\b|-pl\\b|pol|polish|polski)\", \"pol+eng\"),\n    ]:\n        if re.search(pattern, name):\n            return lang_code\n    return config.default_lang_english\n\n\ndef normalize_languages_for_engine(languages: str, engine: str) -> str:\n    lang = languages.lower()\n    if engine == \"paddle\":\n        # PaddleOCR expects ISO 639-1 or specific language names (e.g., 'german', 'french', etc.)\n        try:\n            # Use the first language if multiple are given\n            first_lang = lang.split('+')[0].strip()\n            code = langcodes.find(first_lang)\n            paddle_map = {\n                \"de\": \"german\",\n                \"deu\": \"german\",\n                \"fr\": \"french\",\n                \"fra\": \"french\",\n                \"en\": \"en\",\n                \"eng\": \"en\",\n                \"it\": \"italian\",\n                \"ita\": \"italian\",\n                \"es\": \"spanish\",\n                \"spa\": \"spanish\",\n                \"pl\": \"polish\",\n                \"pol\": \"polish\",\n            }\n            alpha2 = code.to_alpha2()\n            alpha3 = code.to_alpha3()\n            if alpha2 in paddle_map:\n                return paddle_map[alpha2]\n            if alpha3 in paddle_map:\n                return paddle_map[alpha3]\n        except Exception:\n            return \"en\"\n        return \"en\"\n    return languages\n\n\ndef decide_ocr_route(\n    has_text_layer: bool,\n    quality: TextQuality,\n    available_engines: Sequence[str],\n    config: DoclingProcessingConfig,\n    languages: str,\n) -> OcrRouteDecision:\n    low_quality = is_low_quality(quality, config)\n    if config.ocr_mode == \"off\":\n        return OcrRouteDecision(\n            False,\n            \"none\",\n            languages,\n            \"OCR disabled by config\",\n            False,\n            False,\n            \"Per-page OCR disabled by config\",\n        )\n\n    if config.ocr_mode == \"force\":\n        ocr_used = True\n        route_reason = \"OCR forced by config\"\n    elif has_text_layer and not (config.force_ocr_on_low_quality_text and low_quality):\n        return OcrRouteDecision(\n            False,\n            \"none\",\n            languages,\n            \"Text layer detected\",\n            False,\n            False,\n            \"Per-page OCR not applicable (text layer)\",\n        )\n    else:\n        ocr_used = True\n        if has_text_layer:\n            route_reason = \"Text layer detected but low quality\"\n        else:\n            route_reason = \"No usable text layer detected\"\n\n    engine = \"docling\"\n    use_external = False\n    if ocr_used:\n        if config.prefer_ocr_engine in available_engines:\n            engine = config.prefer_ocr_engine\n            use_external = True\n        elif config.fallback_ocr_engine in available_engines:\n            engine = config.fallback_ocr_engine\n            use_external = True\n\n    per_page = False\n    per_page_reason = \"Per-page OCR not applicable\"\n    if use_external:\n        per_page, per_page_reason = decide_per_page_ocr(has_text_layer, quality, config)\n    if low_quality and not has_text_layer:\n        route_reason = f\"{route_reason}; low-quality scan suspected\"\n\n    return OcrRouteDecision(ocr_used, engine, languages, route_reason, use_external, per_page, per_page_reason)\n\n\ndef detect_available_ocr_engines() -> List[str]:\n    available: List[str] = []\n    try:\n        import paddleocr  # noqa: F401\n        from pdf2image import convert_from_path  # noqa: F401\n        available.append(\"paddle\")\n    except Exception:\n        pass\n    try:\n        import pytesseract  # noqa: F401\n        from pdf2image import convert_from_path  # noqa: F401\n        available.append(\"tesseract\")\n    except Exception:\n        pass\n    return available\n\n\ndef load_default_wordlist(config: DoclingProcessingConfig) -> Sequence[str]:\n    path = config.dictionary_path\n    if not path:\n        path = os.path.join(os.path.dirname(__file__), config.default_dictionary_name)\n    if not path or not os.path.isfile(path):\n        return []\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as handle:\n            return [line.strip() for line in handle if line.strip() and not line.startswith(\"#\")]\n    except Exception as exc:\n        LOGGER.warning(\"Failed to load dictionary word list: %s\", exc)\n        return []\n\n\ndef prepare_dictionary_words(config: DoclingProcessingConfig) -> Sequence[str]:\n    if not config.enable_dictionary_correction:\n        return []\n    if config.dictionary_words:\n        return [word.strip() for word in config.dictionary_words if word and word.strip()]\n    words = load_default_wordlist(config)\n    if not words:\n        LOGGER.warning(\"Dictionary correction enabled but no wordlist was loaded.\")\n    return words\n\n\ndef build_spellchecker_for_languages(config: DoclingProcessingConfig, languages: str):\n    \"\"\"\n    Build a cross-platform spellchecker adapter with a .spell(word) method.\n    Tries:\n      1) hunspell (C binding) if available\n      2) spylls (pure Python) if available\n    Returns an object with .spell(str)->bool, or None if unavailable.\n    \"\"\"\n    if not config.enable_hunspell:\n        return None\n\n    # Resolve aff/dic paths (explicit or auto in tools/hunspell)\n    def resolve_paths() -> List[Tuple[str, str]]:\n        pairs: List[Tuple[str, str]] = []\n        aff = config.hunspell_aff_path\n        dic = config.hunspell_dic_path\n        if aff and dic and os.path.isfile(aff) and os.path.isfile(dic):\n            pairs.append((aff, dic))\n            return pairs\n        base_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n        lang = (languages or \"\").lower()\n        try_codes: List[str] = []\n        if any(t in lang for t in (\"de\", \"deu\", \"german\", \"deutsch\")):\n            try_codes += [\"de_DE\", \"de_AT\", \"de_CH\"]\n        if any(t in lang for t in (\"en\", \"eng\", \"english\")):\n            try_codes += [\"en_US\", \"en_GB\"]\n        if not try_codes:\n            try_codes = [\"en_US\"]\n        # Exact matches first\n        for code in try_codes:\n            aff_path = os.path.join(base_dir, f\"{code}.aff\")\n            dic_path = os.path.join(base_dir, f\"{code}.dic\")\n            if os.path.isfile(aff_path) and os.path.isfile(dic_path):\n                pairs.append((aff_path, dic_path))\n        if pairs:\n            return pairs\n\n        # Flexible matching: accept stems like de_DE_frami.* or en_US-large.* when both files share the same stem\n        try:\n            names = os.listdir(base_dir)\n        except Exception:\n            names = []\n        stems_with_aff = {n[:-4] for n in names if n.endswith(\".aff\")}\n        stems_with_dic = {n[:-4] for n in names if n.endswith(\".dic\")}\n        common_stems = list(stems_with_aff & stems_with_dic)\n\n        def stem_priority(stem: str, code: str) -> int:\n            # Higher number = higher priority\n            if stem == code:\n                return 3\n            if stem.startswith(code + \"_\"):\n                return 2\n            if code in stem:\n                return 1\n            return 0\n\n        for code in try_codes:\n            candidates = sorted(\n                [s for s in common_stems if stem_priority(s, code) > 0],\n                key=lambda s: stem_priority(s, code),\n                reverse=True,\n            )\n            for stem in candidates:\n                aff_path = os.path.join(base_dir, f\"{stem}.aff\")\n                dic_path = os.path.join(base_dir, f\"{stem}.dic\")\n                if os.path.isfile(aff_path) and os.path.isfile(dic_path):\n                    pairs.append((aff_path, dic_path))\n                    break\n        return pairs\n\n\n    pairs = resolve_paths()\n    # If no pairs found, try to download on demand\n    if not pairs:\n        # Map special cases for repo structure\n        repo_map = {\n            \"de_DE\": (\"de\", \"de_DE_frami\"),\n            \"de_AT\": (\"de\", \"de_AT\"),\n            \"de_CH\": (\"de\", \"de_CH\"),\n            \"en_US\": (\"en\", \"en_US\"),\n            \"en_GB\": (\"en\", \"en_GB\"),\n            \"fr_FR\": (\"fr_FR\", \"fr\"),\n        }\n        lang_code = None\n        lang = (languages or \"\").lower()\n        if any(t in lang for t in (\"de\", \"deu\", \"german\", \"deutsch\")):\n            lang_code = \"de_DE\"\n        elif any(t in lang for t in (\"en\", \"eng\", \"english\")):\n            lang_code = \"en_US\"\n        elif any(t in lang for t in (\"fr\", \"fra\", \"french\", \"francais\")):\n            lang_code = \"fr_FR\"\n        if not lang_code:\n            lang_code = \"en_US\"\n        folder, prefix = repo_map.get(lang_code, (lang_code, lang_code))\n        base_url = f\"https://raw.githubusercontent.com/LibreOffice/dictionaries/master/{folder}/\"\n        aff_name = f\"{prefix}.aff\"\n        dic_name = f\"{prefix}.dic\"\n        aff_url = base_url + aff_name\n        dic_url = base_url + dic_name\n        out_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n        os.makedirs(out_dir, exist_ok=True)\n        aff_path = os.path.join(out_dir, f\"{lang_code}.aff\")\n        dic_path = os.path.join(out_dir, f\"{lang_code}.dic\")\n        def download(url, out_path):\n            try:\n                import urllib.request\n                print(f\"Downloading {url} -> {out_path}\")\n                urllib.request.urlretrieve(url, out_path)\n                return True\n            except Exception as exc:\n                print(f\"Failed to download {url}: {exc}\")\n                return False\n        ok_aff = download(aff_url, aff_path)\n        ok_dic = download(dic_url, dic_path)\n        if ok_aff and ok_dic:\n            print(f\"Successfully downloaded Hunspell dictionary for {lang_code} to {out_dir}\")\n        # Try to resolve again\n        pairs = resolve_paths()\n\n    # Attempt hunspell binding first\n    try:\n        import hunspell  # type: ignore\n\n        for aff_path, dic_path in pairs:\n            try:\n                hs = hunspell.HunSpell(dic_path, aff_path)\n                LOGGER.info(\n                    \"Spellchecker: using hunspell binding (%s, %s)\",\n                    os.path.basename(dic_path),\n                    os.path.basename(aff_path),\n                )\n                try:\n                    # Record details for external visibility\n                    LAST_SPELLCHECKER_INFO.update({\n                        \"backend\": \"hunspell\",\n                        \"dic\": dic_path,\n                        \"aff\": aff_path,\n                    })\n                except Exception:\n                    pass\n                return hs\n            except Exception:\n                continue\n    except Exception:\n        pass\n\n    # Attempt spylls fallback (pure Python)\n    try:\n        from spylls.hunspell import Dictionary as SpyllsDictionary  # type: ignore\n\n        class SpyllsWrapper:\n            def __init__(self, d):\n                self.d = d\n\n            def spell(self, word: str) -> bool:\n                # Try common case variants to recognize lowercased nouns etc.\n                variants = [word, word.lower(), word.capitalize(), word.title(), word.upper()]\n                seen = set()\n                for v in variants:\n                    if v in seen:\n                        continue\n                    seen.add(v)\n                    try:\n                        if hasattr(self.d, \"lookup\") and self.d.lookup(v):\n                            return True\n                    except Exception:\n                        pass\n                    try:\n                        sugg = self.d.suggest(v)\n                        if isinstance(sugg, (list, tuple)) and v in sugg:\n                            return True\n                    except Exception:\n                        pass\n                return False\n\n        for aff_path, dic_path in pairs:\n            try:\n                d = None\n                errors: List[str] = []\n                # Variant A: (aff, dic)\n                try:\n                    d = SpyllsDictionary.from_files(aff_path, dic_path)\n                except Exception as eA:\n                    errors.append(f\"A(aff,dic): {eA}\")\n                # Variant B: directory containing both\n                if d is None:\n                    try:\n                        d = SpyllsDictionary.from_files(os.path.dirname(dic_path))\n                    except Exception as eB:\n                        errors.append(f\"B(dir): {eB}\")\n                # Variant C: stem without extension\n                if d is None:\n                    try:\n                        stem = os.path.splitext(dic_path)[0]\n                        d = SpyllsDictionary.from_files(stem)\n                    except Exception as eC:\n                        errors.append(f\"C(stem): {eC}\")\n                # Variant D: single-path dic\n                if d is None:\n                    try:\n                        d = SpyllsDictionary.from_files(dic_path)\n                    except Exception as eD:\n                        errors.append(f\"D(dic): {eD}\")\n                # Variant E: single-path aff\n                if d is None:\n                    try:\n                        d = SpyllsDictionary.from_files(aff_path)\n                    except Exception as eE:\n                        errors.append(f\"E(aff): {eE}\")\n\n                if d is None:\n                    raise RuntimeError(\"spylls load failed: \" + \"; \".join(errors))\n\n                LOGGER.info(\n                    \"Spellchecker: using spylls fallback (%s, %s)\",\n                    os.path.basename(dic_path),\n                    os.path.basename(aff_path),\n                )\n                try:\n                    LAST_SPELLCHECKER_INFO.update({\n                        \"backend\": \"spylls\",\n                        \"dic\": dic_path,\n                        \"aff\": aff_path,\n                    })\n                except Exception:\n                    pass\n                return SpyllsWrapper(d)\n            except Exception:\n                continue\n    except Exception:\n        pass\n\n    # Naive .dic fallback (no affix rules) when hunspell/spylls are unavailable\n    try:\n        class NaiveDicWrapper:\n            def __init__(self, words: Sequence[str]):\n                self.words = set(w.lower() for w in words if w)\n\n            def spell(self, word: str) -> bool:\n                variants = [word, word.lower(), word.capitalize(), word.title(), word.upper()]\n                for v in variants:\n                    if v.lower() in self.words:\n                        return True\n                return False\n\n        def load_naive_dic(path: str) -> Optional[NaiveDicWrapper]:\n            try:\n                entries: List[str] = []\n                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n                    first = True\n                    for raw in fh:\n                        line = raw.strip().lstrip(\"\\ufeff\")\n                        if not line:\n                            continue\n                        if first and line.isdigit():\n                            first = False\n                            continue\n                        first = False\n                        base = line.split(\"/\")[0].strip()\n                        if base:\n                            entries.append(base)\n                if entries:\n                    LOGGER.info(\"Spellchecker: using naive .dic (%s) entries=%d\", os.path.basename(path), len(entries))\n                    return NaiveDicWrapper(entries)\n            except Exception as exc:\n                LOGGER.warning(\"Naive .dic load failed for %s: %s\", path, exc)\n            return None\n\n        # Prefer .dic paths discovered via resolve_paths(); otherwise scan tools/hunspell\n        dic_paths: List[str] = []\n        for _aff, _dic in pairs:\n            if os.path.isfile(_dic):\n                dic_paths.append(_dic)\n        if not dic_paths:\n            base_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n            try:\n                candidates = [os.path.join(base_dir, name) for name in os.listdir(base_dir) if name.endswith(\".dic\")]\n            except Exception:\n                candidates = []\n            lang = (languages or \"\").lower()\n            filtered: List[str] = []\n            for p in candidates:\n                name = os.path.basename(p).lower()\n                if (\"en\" in lang or \"eng\" in lang) and (name.startswith(\"en_\") or name.startswith(\"en\")):\n                    filtered.append(p)\n                if (\"de\" in lang or \"deu\" in lang or \"german\" in lang or \"deutsch\" in lang) and (name.startswith(\"de_\") or name.startswith(\"de\")):\n                    filtered.append(p)\n            dic_paths = filtered or candidates\n\n        for dic_path in dic_paths:\n            wrapper = load_naive_dic(dic_path)\n            if wrapper is not None:\n                return wrapper\n    except Exception:\n        pass\n\n    LOGGER.info(\"Spellchecker: no hunspell/spylls dictionary available\")\n    try:\n        LAST_SPELLCHECKER_INFO.update({\"backend\": \"none\"})\n    except Exception:\n        pass\n    return None\n\n\ndef apply_dictionary_correction(text: str, wordlist: Sequence[str], hs=None) -> str:\n    if not wordlist:\n        # If Hunspell available, do a minimal pass using it only\n        if hs is None:\n            return text\n        dictionary = set()\n    else:\n        dictionary = {word.lower() for word in wordlist}\n    token_re = re.compile(r\"[A-Za-z0-9]+\")\n\n    def match_case(candidate: str, original: str) -> str:\n        if original.isupper():\n            return candidate.upper()\n        if original[:1].isupper():\n            return candidate.capitalize()\n        return candidate\n\n    def generate_candidates(token: str) -> Iterable[str]:\n        candidates: List[str] = []\n        if any(char.isdigit() for char in token) and any(char.isalpha() for char in token):\n            candidates.append(token.replace(\"0\", \"o\"))\n            candidates.append(token.replace(\"1\", \"l\"))\n            candidates.append(token.replace(\"5\", \"s\"))\n        if \"rn\" in token:\n            candidates.append(token.replace(\"rn\", \"m\"))\n        return candidates\n\n    def replace_token(match: re.Match) -> str:\n        token = match.group(0)\n        lower = token.lower()\n        if lower in dictionary or (hs is not None and hs.spell(token)):\n            return token\n        for candidate in generate_candidates(token):\n            cand_lower = candidate.lower()\n            if cand_lower in dictionary or (hs is not None and hs.spell(candidate)):\n                replaced = match_case(candidate, token)\n                try:\n                    LOGGER.info(\"Dict correction: %s -> %s\", token, replaced)\n                except Exception:\n                    pass\n                return replaced\n        return token\n\n    return token_re.sub(replace_token, text)\n\n\ndef apply_umlaut_corrections(text: str, languages: str, wordlist: Sequence[str], hs=None) -> str:\n    \"\"\"\n    Convert ASCII digraphs ae/oe/ue to German umlauts ä/ö/ü more comprehensively.\n\n    Strategy:\n    - If a dictionary is provided, prefer candidates that appear in it.\n    - Otherwise, use word frequency (wordfreq.zipf_frequency) for German to\n      select candidates whose frequency noticeably exceeds the original.\n    - Preserve original casing (UPPER, Title, lower).\n    - Only operate when language is German.\n    - Keep conservative: if no strong signal, leave token unchanged.\n    \"\"\"\n    lang = (languages or \"\").lower()\n    if not any(token in lang for token in (\"de\", \"deu\", \"german\", \"deutsch\")):\n        return text\n\n    dictionary = {word.lower() for word in (wordlist or [])}\n\n    try:\n        from wordfreq import zipf_frequency as _zipf\n    except Exception:\n        _zipf = None  # wordfreq optional\n\n    ascii_to_umlaut = ((\"ae\", \"\\u00e4\"), (\"oe\", \"\\u00f6\"), (\"ue\", \"\\u00fc\"))\n\n    def case_match(candidate: str, original: str) -> str:\n        if original.isupper():\n            return candidate.upper()\n        if original[:1].isupper() and original[1:].islower():\n            return candidate.capitalize()\n        return candidate\n\n    def generate_variants(token_lower: str) -> List[str]:\n        # Generate all unique variants by replacing any subset of ae/oe/ue occurrences\n        indices: List[Tuple[int, str, str]] = []\n        for ascii_seq, uml in ascii_to_umlaut:\n            start = 0\n            while True:\n                idx = token_lower.find(ascii_seq, start)\n                if idx == -1:\n                    break\n                # Heuristic: avoid replacing \"ue\" when preceded by 'e' (e.g., \"neue\", \"Treue\")\n                if ascii_seq == \"ue\" and idx > 0 and token_lower[idx - 1] == \"e\":\n                    pass\n                else:\n                    indices.append((idx, ascii_seq, uml))\n                start = idx + 1 if idx != -1 else start\n\n        if not indices:\n            return []\n\n        # Build combinations\n        variants = {token_lower}\n        for idx, ascii_seq, uml in indices:\n            new_set = set()\n            for base in variants:\n                # Replace at the same position if still matching\n                if base[idx:idx + len(ascii_seq)] == ascii_seq:\n                    new_set.add(base[:idx] + uml + base[idx + len(ascii_seq):])\n                new_set.add(base)\n            variants = new_set\n        return [v for v in variants if v != token_lower]\n\n    def pick_best(token: str) -> str:\n        lower = token.lower()\n        # Quick path: if already contains umlaut, skip\n        if any(ch in lower for ch in (\"ä\", \"ö\", \"ü\")):\n            return token\n\n        # Generate candidate variants\n        candidates = generate_variants(lower)\n        if not candidates:\n            return token\n\n        # Score candidates\n        best = None\n        best_score = float(\"-inf\")\n        # Base frequency for original\n        base_freq = _zipf(lower, \"de\") if _zipf else 0.0\n        for cand in candidates:\n            score = 0.0\n            if cand in dictionary or (hs is not None and hs.spell(cand)):\n                score += 10.0  # strong signal from dictionary\n            if _zipf:\n                freq = _zipf(cand, \"de\")\n                # Prefer if notably more frequent than original\n                score += (freq - base_freq)\n            # Prefer shorter (umlaut variant shortens by 1 char per replacement)\n            score += (len(lower) - len(cand)) * 0.05\n            if score > best_score:\n                best = cand\n                best_score = score\n\n        # Acceptance threshold: either in dictionary or frequency improved by >= 0.5\n        accept = False\n        if best is not None:\n            if best in dictionary or (hs is not None and hs.spell(best)):\n                accept = True\n            elif _zipf:\n                if (_zipf(best, \"de\") - base_freq) >= 0.5:\n                    accept = True\n\n        if not accept or not best:\n            return token\n        replaced = case_match(best, token)\n        try:\n            LOGGER.info(\"Umlaut correction: %s -> %s\", token, replaced)\n        except Exception:\n            pass\n        return replaced\n\n    # Replace word tokens conservatively (length >= 4 to avoid short codes)\n    return re.sub(r\"[A-Za-zÄÖÜäöüß]{4,}\", lambda m: pick_best(m.group(0)), text)\n\n\ndef restore_missing_spaces(text: str, languages: str, hs=None) -> str:\n    \"\"\"\n    Conservatively insert spaces inside overlong tokens when a split yields two\n    valid words (by Hunspell/Splylls or by wordfreq Zipf >= 3.0 for target langs).\n\n    Heuristics:\n    - Consider tokens of length >= 12 with only letters (incl. German chars).\n    - Prefer camelCase boundaries (a…zA…Z) when both sides are valid.\n    - Otherwise, try a single split; accept only if BOTH parts look valid.\n    - Log accepted splits.\n    \"\"\"\n    try:\n        from wordfreq import zipf_frequency as _zipf\n    except Exception:\n        _zipf = None\n\n    lang_codes = select_wordfreq_languages(languages)\n\n    def score_word(w: str) -> Tuple[float, bool]:\n        spelled = False\n        try:\n            if hs is not None and hs.spell(w):\n                spelled = True\n        except Exception:\n            pass\n        if spelled:\n            return 4.0, True\n        if _zipf is None:\n            return 0.0, False\n        try:\n            z = max(_zipf(w.lower(), lc) for lc in lang_codes)\n        except Exception:\n            z = 0.0\n        return float(z), False\n\n    token_re = re.compile(r\"[A-Za-zÄÖÜäöüß]{12,}\")\n\n    def consider_split(tok: str) -> str:\n        best = None  # type: Optional[Tuple[str, float, bool, str, float, bool]]\n\n        # Try camelCase boundary first: a…zA…Z\n        for m in re.finditer(r\"([a-zäöüß])([A-ZÄÖÜ])\", tok):\n            i = m.start(2)\n            left, right = tok[:i], tok[i:]\n            if len(left) < 3 or len(right) < 3:\n                continue\n            s1, d1 = score_word(left)\n            s2, d2 = score_word(right)\n            if (d1 or s1 >= 3.0) and (d2 or s2 >= 3.0):\n                combined = s1 + s2\n                best = (left, s1, d1, right, s2, d2)\n                break\n\n        # Otherwise, try single split positions\n        if best is None:\n            n = len(tok)\n            for i in range(3, n - 2):\n                left, right = tok[:i], tok[i:]\n                if len(left) < 3 or len(right) < 3:\n                    continue\n                s1, d1 = score_word(left)\n                s2, d2 = score_word(right)\n                if (d1 or s1 >= 3.0) and (d2 or s2 >= 3.0):\n                    combined = s1 + s2\n                    if best is None or combined > (best[1] + best[4]):\n                        best = (left, s1, d1, right, s2, d2)\n\n        if best is None:\n            return tok\n\n        left, s1, d1, right, s2, d2 = best\n        replacement = f\"{left} {right}\"\n        try:\n            LOGGER.info(\n                \"Inserted space: %s -> %s (scores=%.2f/%.2f, dict=%s/%s)\",\n                tok,\n                replacement,\n                s1,\n                s2,\n                d1,\n                d2,\n            )\n        except Exception:\n            pass\n        return replacement\n\n    return token_re.sub(lambda m: consider_split(m.group(0)), text)\n\n\ndef should_apply_llm_correction(text: str, config: DoclingProcessingConfig) -> bool:\n    if not config.enable_llm_correction:\n        return False\n    if not config.llm_correct:\n        return False\n    if config.llm_correction_max_chars and len(text) > config.llm_correction_max_chars:\n        return False\n    languages = select_language_set(config.language_hint, \"\", config)\n    quality = estimate_text_quality([{\"text\": text}], config, languages)\n    return quality.confidence_proxy < config.llm_correction_min_quality\n\n\ndef build_llm_cleanup_callback(config: DoclingProcessingConfig) -> Optional[Callable[[str], str]]:\n    if not config.enable_llm_correction:\n        return None\n    if not config.llm_cleanup_base_url or not config.llm_cleanup_model:\n        LOGGER.warning(\"LLM cleanup enabled but base URL or model is missing.\")\n        return None\n\n    base_url = config.llm_cleanup_base_url.rstrip(\"/\")\n    endpoint = f\"{base_url}/chat/completions\"\n    api_key = (config.llm_cleanup_api_key or \"\").strip()\n\n    def _call(text: str) -> str:\n        try:\n            import requests\n        except Exception as exc:\n            LOGGER.warning(\"requests not available for LLM cleanup: %s\", exc)\n            return text\n\n        headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n        payload = {\n            \"model\": config.llm_cleanup_model,\n            \"temperature\": config.llm_cleanup_temperature,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are an OCR cleanup assistant. Fix OCR errors without changing meaning. \"\n                        \"Do not add content. Return corrected text only.\"\n                    ),\n                },\n                {\"role\": \"user\", \"content\": text},\n            ],\n        }\n        try:\n            response = requests.post(endpoint, headers=headers, json=payload, timeout=config.llm_cleanup_timeout_sec)\n            response.raise_for_status()\n            data = response.json()\n            content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\")\n            if content:\n                return str(content).strip()\n        except Exception as exc:\n            LOGGER.warning(\"LLM cleanup failed: %s\", exc)\n        return text\n\n    return _call\n\n\ndef postprocess_text(\n    text: str,\n    config: DoclingProcessingConfig,\n    languages: str,\n    wordlist: Sequence[str],\n) -> str:\n    if not text:\n        return text\n    cleaned = dehyphenate_text(text)\n    cleaned = replace_ligatures(cleaned)\n    cleaned = normalize_whitespace(cleaned)\n    hs = build_spellchecker_for_languages(config, languages) if config.enable_hunspell else None\n    # Attempt to restore missing spaces before word-level corrections\n    try:\n        restored = restore_missing_spaces(cleaned, languages, hs)\n        if restored != cleaned:\n            LOGGER.info(\"Applied missing-space restoration pass\")\n        cleaned = restored\n    except Exception as exc:\n        LOGGER.warning(\"Missing-space restoration failed: %s\", exc)\n    if config.enable_dictionary_correction or hs is not None:\n        cleaned = apply_dictionary_correction(cleaned, wordlist, hs)\n    cleaned = apply_umlaut_corrections(cleaned, languages, wordlist, hs)\n    if should_apply_llm_correction(cleaned, config) and config.llm_correct:\n        cleaned = config.llm_correct(cleaned)\n    return cleaned\n\ndef export_markdown(doc: Any) -> str:\n    for method_name in (\"export_to_markdown\", \"to_markdown\", \"export_to_md\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    for method_name in (\"export_to_text\", \"to_text\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    return str(doc)\n\n\ndef export_text(doc: Any) -> str:\n    for method_name in (\"export_to_text\", \"to_text\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    return str(doc)\n\n\ndef extract_pages(doc: Any) -> List[Dict[str, Any]]:\n    pages: List[Dict[str, Any]] = []\n    pages_attr = getattr(doc, \"pages\", None)\n    if pages_attr is not None and not isinstance(pages_attr, (str, bytes, dict)):\n        try:\n            pages_list = list(pages_attr)\n        except TypeError:\n            pages_list = []\n        if pages_list:\n            for idx, page in enumerate(pages_list, start=1):\n                page_num = getattr(page, \"page_number\", None) or getattr(page, \"number\", None) or idx\n                text = None\n                for attr in (\"text\", \"content\", \"markdown\", \"md\"):\n                    if hasattr(page, attr):\n                        value = getattr(page, attr)\n                        text = value() if callable(value) else value\n                        break\n                if text is None and hasattr(page, \"export_to_text\"):\n                    text = page.export_to_text()\n                if text is None:\n                    text = str(page)\n                pages.append({\"page_num\": int(page_num), \"text\": str(text)})\n            return pages\n\n    full_text = export_text(doc)\n    if full_text:\n        pages.append({\"page_num\": 1, \"text\": full_text})\n    return pages\n\n\ndef select_analysis_page_indices(\n    total_pages: int,\n    max_pages: Optional[int],\n    sample_strategy: str,\n) -> List[int]:\n    if total_pages <= 0:\n        return []\n    if not max_pages or max_pages <= 0 or total_pages <= max_pages:\n        return list(range(1, total_pages + 1))\n\n    strategy = (sample_strategy or \"first\").lower()\n    if strategy == \"middle\":\n        start = max(1, (total_pages - max_pages) // 2 + 1)\n        end = min(total_pages, start + max_pages - 1)\n        return list(range(start, end + 1))\n    return list(range(1, max_pages + 1))\n\n\ndef extract_pages_from_pdf(\n    pdf_path: str,\n    max_pages: Optional[int] = None,\n    sample_strategy: str = \"first\",\n) -> List[Dict[str, Any]]:\n    try:\n        from pypdf import PdfReader\n    except Exception as exc:\n        eprint(f\"pypdf is not available for fallback page extraction: {exc}\")\n        return []\n\n    pages: List[Dict[str, Any]] = []\n    try:\n        reader = PdfReader(pdf_path)\n        page_indices = select_analysis_page_indices(len(reader.pages), max_pages, sample_strategy)\n        for idx in page_indices:\n            page = reader.pages[idx - 1]\n            try:\n                text = page.extract_text() or \"\"\n            except Exception:\n                text = \"\"\n            pages.append({\"page_num\": idx, \"text\": text})\n    except Exception as exc:\n        eprint(f\"Failed to extract pages with pypdf: {exc}\")\n        return []\n\n    return pages\n\n\ndef split_markdown_sections(markdown: str) -> List[Dict[str, Any]]:\n    sections: List[Dict[str, Any]] = []\n    current_title = \"\"\n    current_lines: List[str] = []\n\n    def flush() -> None:\n        nonlocal current_title, current_lines\n        if current_title or current_lines:\n            sections.append({\n                \"title\": current_title.strip(),\n                \"text\": \"\\n\".join(current_lines).strip(),\n            })\n        current_title = \"\"\n        current_lines = []\n\n    for line in markdown.splitlines():\n        if line.startswith(\"#\"):\n            flush()\n            current_title = line.lstrip(\"#\").strip()\n        else:\n            current_lines.append(line)\n\n    flush()\n    return sections\n\n\n_PAGE_RANGE_STOPWORDS = {\n    \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"into\", \"over\",\n    \"under\", \"after\", \"before\", \"were\", \"was\", \"are\", \"is\", \"its\", \"their\",\n    \"then\", \"than\", \"than\", \"which\", \"when\", \"where\", \"have\", \"has\", \"had\",\n    \"into\", \"onto\", \"upon\", \"your\", \"yours\", \"they\", \"them\", \"these\", \"those\",\n    \"will\", \"would\", \"could\", \"should\", \"about\", \"there\", \"here\", \"while\",\n    \"what\", \"why\", \"how\", \"not\", \"but\", \"you\", \"your\", \"our\", \"ours\", \"his\",\n    \"her\", \"she\", \"him\", \"she\", \"him\", \"its\", \"also\", \"such\", \"been\", \"being\",\n    \"out\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n    \"nine\", \"ten\", \"more\", \"most\", \"some\", \"many\", \"few\", \"each\", \"per\",\n}\n\n\ndef tokenize_for_page_range(text: str) -> List[str]:\n    tokens = re.findall(r\"[A-Za-z0-9]{3,}\", text.lower())\n    return [token for token in tokens if token not in _PAGE_RANGE_STOPWORDS]\n\n\ndef sample_tokens(tokens: Sequence[str], max_tokens: int) -> List[str]:\n    if max_tokens <= 0 or len(tokens) <= max_tokens:\n        return list(tokens)\n    step = max(1, len(tokens) // max_tokens)\n    return list(tokens[::step])\n\n\ndef compute_page_overlap(\n    section_text: str,\n    pages: List[Dict[str, Any]],\n    config: DoclingProcessingConfig,\n) -> List[Tuple[float, int, int]]:\n    section_tokens = tokenize_for_page_range(section_text)\n    if not section_tokens:\n        return []\n    sample = sample_tokens(section_tokens, config.page_range_sample_tokens)\n    sample_set = set(sample)\n    total = len(sample_set)\n    results: List[Tuple[float, int, int]] = []\n    for page in pages:\n        page_text = str(page.get(\"text\", \"\"))\n        page_tokens = set(tokenize_for_page_range(page_text))\n        hits = len(sample_set & page_tokens)\n        ratio = hits / max(1, total)\n        results.append((ratio, hits, int(page.get(\"page_num\", 0))))\n    return results\n\n\ndef select_overlap_cluster(\n    overlap_scores: Sequence[Tuple[float, int, int]],\n    config: DoclingProcessingConfig,\n) -> List[int]:\n    if not overlap_scores:\n        return []\n    max_ratio = max(score[0] for score in overlap_scores)\n    max_hits = max(score[1] for score in overlap_scores)\n    ratio_cutoff = max(config.page_range_min_overlap, max_ratio * config.page_range_peak_ratio)\n    hits_cutoff = max(config.page_range_min_hits, int(max_hits * config.page_range_peak_ratio))\n    candidates = [\n        (ratio, hits, page_num)\n        for ratio, hits, page_num in overlap_scores\n        if ratio >= ratio_cutoff or hits >= hits_cutoff\n    ]\n    if not candidates:\n        candidates = sorted(overlap_scores, reverse=True)[: config.page_range_top_k]\n\n    candidates.sort(key=lambda item: item[2])\n    clusters: List[List[Tuple[float, int, int]]] = []\n    current: List[Tuple[float, int, int]] = []\n    for entry in candidates:\n        if not current:\n            current.append(entry)\n            continue\n        if entry[2] - current[-1][2] <= config.page_range_cluster_gap:\n            current.append(entry)\n        else:\n            clusters.append(current)\n            current = [entry]\n    if current:\n        clusters.append(current)\n\n    def cluster_score(cluster: Sequence[Tuple[float, int, int]]) -> Tuple[float, float]:\n        ratios = [item[0] for item in cluster]\n        return (sum(ratios), max(ratios))\n\n    best_cluster = max(clusters, key=cluster_score)\n    page_nums = [item[2] for item in best_cluster]\n    if len(page_nums) > 1:\n        span_ratio = (max(page_nums) - min(page_nums) + 1) / max(1, len(overlap_scores))\n        if span_ratio > config.page_range_max_span_ratio:\n            trimmed = sorted(best_cluster, reverse=True)[: config.page_range_top_k]\n            page_nums = [item[2] for item in trimmed]\n    return page_nums\n\n\ndef find_page_range(\n    section_text: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n) -> Tuple[int, int]:\n    if not pages:\n        return 0, 0\n\n    cleaned = normalize_text(section_text)\n    if not cleaned:\n        return 0, 0\n\n    snippet_start = cleaned[:200]\n    snippet_end = cleaned[-200:]\n\n    page_start = 0\n    page_end = 0\n\n    for page in pages:\n        page_clean = normalize_text(page.get(\"text\", \"\"))\n        if snippet_start and snippet_start in page_clean:\n            page_start = page.get(\"page_num\", 0)\n            break\n\n    for page in reversed(pages):\n        page_clean = normalize_text(page.get(\"text\", \"\"))\n        if snippet_end and snippet_end in page_clean:\n            page_end = page.get(\"page_num\", 0)\n            break\n\n    if page_start == 0 or page_end == 0:\n        config = config or DoclingProcessingConfig()\n        overlap_scores = compute_page_overlap(cleaned, pages, config)\n        page_nums = select_overlap_cluster(overlap_scores, config)\n        if page_nums:\n            if page_start == 0:\n                page_start = min(page_nums)\n            if page_end == 0:\n                page_end = max(page_nums)\n\n    if page_start == 0:\n        page_start = pages[0].get(\"page_num\", 0)\n    if page_end == 0:\n        page_end = pages[-1].get(\"page_num\", 0)\n\n    return int(page_start), int(page_end)\n\n\ndef slugify(text: str) -> str:\n    slug = re.sub(r\"[^a-z0-9]+\", \"-\", text.lower()).strip(\"-\")\n    return slug\n\n\ndef configure_layout_options(pipeline_options: Any) -> None:\n    if hasattr(pipeline_options, \"layout_mode\"):\n        pipeline_options.layout_mode = \"accurate\"\n    if hasattr(pipeline_options, \"detect_layout\"):\n        pipeline_options.detect_layout = True\n    if hasattr(pipeline_options, \"extract_tables\"):\n        pipeline_options.extract_tables = True\n    if hasattr(pipeline_options, \"table_structure\"):\n        pipeline_options.table_structure = True\n    layout_options = getattr(pipeline_options, \"layout_options\", None)\n    if layout_options is not None:\n        for name, value in (\n            (\"detect_columns\", True),\n            (\"detect_tables\", True),\n            (\"enable_table_structure\", True),\n            (\"max_columns\", 3),\n        ):\n            if hasattr(layout_options, name):\n                setattr(layout_options, name, value)\n\n\ndef build_converter(config: DoclingProcessingConfig, decision: OcrRouteDecision):\n    from docling.document_converter import DocumentConverter\n\n    try:\n        from docling.datamodel.base_models import InputFormat\n        from docling.datamodel.pipeline_options import PdfPipelineOptions, OCRMode\n        from docling.document_converter import PdfFormatOption\n    except Exception:\n        return DocumentConverter()\n\n    pipeline_options = PdfPipelineOptions()\n    if not decision.ocr_used:\n        if hasattr(pipeline_options, \"do_ocr\"):\n            pipeline_options.do_ocr = False\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.DISABLED\n    elif config.ocr_mode == \"force\":\n        if hasattr(pipeline_options, \"do_ocr\"):\n            pipeline_options.do_ocr = True\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.FORCE\n    else:\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.AUTO\n\n    if decision.ocr_used:\n        if hasattr(pipeline_options, \"ocr_engine\"):\n            pipeline_options.ocr_engine = decision.ocr_engine\n        if hasattr(pipeline_options, \"ocr_languages\"):\n            pipeline_options.ocr_languages = decision.languages\n        if hasattr(pipeline_options, \"ocr_lang\"):\n            pipeline_options.ocr_lang = decision.languages\n\n    configure_layout_options(pipeline_options)\n\n    format_options = {InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n    return DocumentConverter(format_options=format_options)\n\n\ndef find_poppler_path() -> Optional[str]:\n    env_path = os.environ.get(\"POPPLER_PATH\")\n    if env_path and os.path.isfile(os.path.join(env_path, \"pdftoppm\")):\n        return env_path\n    pdftoppm = shutil.which(\"pdftoppm\")\n    if pdftoppm:\n        return os.path.dirname(pdftoppm)\n    for candidate in (\"/opt/homebrew/bin\", \"/usr/local/bin\", \"/usr/bin\"):\n        if os.path.isfile(os.path.join(candidate, \"pdftoppm\")):\n            return candidate\n    return None\n\n\nPOPPLER_LOGGED_ONCE = False\n\n\ndef render_pdf_pages(pdf_path: str, dpi: int) -> List[Any]:\n    from pdf2image import convert_from_path\n\n    poppler_path = find_poppler_path()\n    if poppler_path:\n        global POPPLER_LOGGED_ONCE\n        if shutil.which(\"pdftoppm\") is None and not POPPLER_LOGGED_ONCE:\n            LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n            POPPLER_LOGGED_ONCE = True\n        return convert_from_path(pdf_path, dpi=dpi, poppler_path=poppler_path)\n    return convert_from_path(pdf_path, dpi=dpi)\n\n\ndef render_pdf_pages_sample(pdf_path: str, dpi: int, max_pages: int) -> List[Any]:\n    from pdf2image import convert_from_path\n\n    if max_pages <= 0:\n        return []\n    poppler_path = find_poppler_path()\n    kwargs = {\"dpi\": dpi, \"first_page\": 1, \"last_page\": max_pages}\n    if poppler_path:\n        global POPPLER_LOGGED_ONCE\n        if shutil.which(\"pdftoppm\") is None and not POPPLER_LOGGED_ONCE:\n            LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n            POPPLER_LOGGED_ONCE = True\n        kwargs[\"poppler_path\"] = poppler_path\n    return convert_from_path(pdf_path, **kwargs)\n\n\ndef get_pdf_page_count(pdf_path: str) -> int:\n    \"\"\"Return total number of pages using pypdf (fast and light).\"\"\"\n    try:\n        from pypdf import PdfReader  # type: ignore\n        reader = PdfReader(pdf_path)\n        return int(len(reader.pages))\n    except Exception:\n        return 0\n\n\ndef select_column_sample_indices(total_pages: int, max_pages: int) -> List[int]:\n    \"\"\"Pick up to max_pages page indices spread across the document (1-based).\"\"\"\n    if total_pages <= 0:\n        return []\n    k = max(1, max_pages)\n    k = min(k, total_pages)\n    if k == 1:\n        return [max(1, (total_pages + 1) // 2)]\n    if k == 2:\n        return [1, total_pages]\n    # Spread evenly including first and last\n    step = (total_pages - 1) / (k - 1)\n    return [int(round(1 + i * step)) for i in range(k)]\n\n\ndef render_pdf_pages_at_indices(pdf_path: str, dpi: int, indices: Sequence[int]) -> List[Any]:\n    \"\"\"Render specific 1-based page indices to images. May call pdf2image multiple times.\"\"\"\n    from pdf2image import convert_from_path\n    images: List[Any] = []\n    if not indices:\n        return images\n    poppler_path = find_poppler_path()\n    for idx in indices:\n        kwargs = {\"dpi\": dpi, \"first_page\": int(idx), \"last_page\": int(idx)}\n        if poppler_path:\n            global POPPLER_LOGGED_ONCE\n            if shutil.which(\"pdftoppm\") is None and not POPPLER_LOGGED_ONCE:\n                LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n                POPPLER_LOGGED_ONCE = True\n            kwargs[\"poppler_path\"] = poppler_path\n        try:\n            imgs = convert_from_path(pdf_path, **kwargs)\n            if imgs:\n                images.append(imgs[0])\n        except Exception:\n            continue\n    return images\n\n\ndef compute_column_density(\n    image: Any,\n    config: DoclingProcessingConfig,\n    target_width: int = 300,\n) -> List[float]:\n    gray = image.convert(\"L\")\n    width, height = gray.size\n    if width > target_width:\n        scale = target_width / max(1, width)\n        gray = gray.resize((target_width, max(1, int(height * scale))))\n    width, height = gray.size\n    crop_top = int(height * config.column_detect_crop_top_ratio)\n    crop_bottom = int(height * config.column_detect_crop_bottom_ratio)\n    if crop_top + crop_bottom < height - 1:\n        gray = gray.crop((0, crop_top, width, height - crop_bottom))\n\n    try:\n        import numpy as np\n    except Exception:\n        pixels = list(gray.getdata())\n        w, h = gray.size\n        if w == 0 or h == 0:\n            return []\n        sorted_pixels = sorted(pixels)\n        median = sorted_pixels[len(sorted_pixels) // 2]\n        mean = sum(pixels) / max(1, len(pixels))\n        variance = sum((value - mean) ** 2 for value in pixels) / max(1, len(pixels))\n        std = variance ** 0.5\n        threshold = median - (std * config.column_detect_threshold_std_mult)\n        threshold = min(threshold, config.column_detect_threshold_max)\n        threshold = max(threshold, config.column_detect_threshold_min)\n        densities = [0] * w\n        for y in range(h):\n            row = pixels[y * w:(y + 1) * w]\n            for x, value in enumerate(row):\n                if value < threshold:\n                    densities[x] += 1\n        return [count / h for count in densities]\n\n    arr = np.asarray(gray)\n    if arr.size == 0:\n        return []\n    # Build a robust binarization threshold: combine median-std rule with Otsu\n    median = float(np.median(arr))\n    std = float(arr.std())\n    thr_a = median - (std * config.column_detect_threshold_std_mult)\n    thr_a = min(thr_a, float(config.column_detect_threshold_max))\n    thr_a = max(thr_a, float(config.column_detect_threshold_min))\n\n    # Otsu threshold (fast implementation without external deps)\n    try:\n        hist, _ = np.histogram(arr, bins=256, range=(0, 255))\n        hist = hist.astype(np.float64)\n        total = hist.sum()\n        if total > 0:\n            prob = hist / total\n            omega = np.cumsum(prob)\n            mu = np.cumsum(prob * np.arange(256))\n            mu_t = mu[-1]\n            sigma_b2 = (mu_t * omega - mu) ** 2 / np.maximum(omega * (1.0 - omega), 1e-9)\n            k = int(np.nanargmax(sigma_b2))\n            thr_b = float(k)\n        else:\n            thr_b = thr_a\n    except Exception:\n        thr_b = thr_a\n\n    threshold = 0.5 * (thr_a + thr_b)\n    mask = arr < threshold\n\n    # Focus on the vertical band with the most text-like pixels to avoid full-width pictures at top\n    h = mask.shape[0]\n    band_h = max(1, int(h * 0.6))  # use central 60% by default (adaptive below)\n    if band_h < h:\n        step = max(1, int(h * 0.04))\n        best_y = 0\n        best_score = -1.0\n        # Slide a window to find the densest text band\n        for y in range(0, h - band_h + 1, step):\n            score = mask[y : y + band_h, :].mean()\n            if score > best_score:\n                best_score = score\n                best_y = y\n        mask = mask[best_y : best_y + band_h, :]\n\n    return mask.mean(axis=0).tolist()\n\n\ndef smooth_density(density: Sequence[float], window: int) -> List[float]:\n    if window <= 1 or not density:\n        return list(density)\n    size = max(1, int(window))\n    half = size // 2\n    smoothed: List[float] = []\n    for idx in range(len(density)):\n        start = max(0, idx - half)\n        end = min(len(density), idx + half + 1)\n        smoothed.append(sum(density[start:end]) / max(1, end - start))\n    return smoothed\n\n\ndef density_percentile(density: Sequence[float], percentile: float) -> float:\n    if not density:\n        return 0.0\n    clamped = max(0.0, min(1.0, percentile))\n    sorted_vals = sorted(density)\n    idx = int(round(clamped * (len(sorted_vals) - 1)))\n    return sorted_vals[idx]\n\n\ndef count_column_gaps(\n    density: Sequence[float],\n    config: DoclingProcessingConfig,\n) -> int:\n    if not density:\n        return 0\n    total = len(density)\n    margin = max(1, int(total * 0.05))\n    start = margin\n    end = max(start + 1, total - margin)\n    core = density[start:end]\n    if not core:\n        return 0\n    text_level = density_percentile(core, config.column_detect_text_percentile)\n    if text_level < config.column_detect_min_text_density:\n        return 0\n    threshold = max(config.column_detect_min_gap_density, text_level * config.column_detect_gap_threshold_ratio)\n    min_gap = max(1, int(len(core) * config.column_detect_min_gap_ratio))\n\n    gaps = 0\n    idx = 0\n    while idx < len(core):\n        if core[idx] < threshold:\n            gap_start = idx\n            while idx < len(core) and core[idx] < threshold:\n                idx += 1\n            if idx - gap_start >= min_gap:\n                gaps += 1\n        else:\n            idx += 1\n    return gaps\n\n\ndef detect_multicolumn_layout(\n    images: Sequence[Any],\n    config: DoclingProcessingConfig,\n) -> ColumnLayoutDetection:\n    if not images:\n        return ColumnLayoutDetection(False, 0.0, \"No pages available\")\n    sample = list(images[: config.column_detect_max_pages])\n    if not sample:\n        return ColumnLayoutDetection(False, 0.0, \"No sample pages\")\n\n    hits = 0\n    for image in sample:\n        density = compute_column_density(image, config)\n        density = smooth_density(density, config.column_detect_smooth_window)\n        gaps = count_column_gaps(density, config)\n        if gaps >= 1:\n            hits += 1\n    ratio = hits / max(1, len(sample))\n    detected = ratio >= config.column_detect_min_pages_ratio\n    reason = f\"{hits}/{len(sample)} pages show column gutters\"\n    return ColumnLayoutDetection(detected, ratio, reason)\n\n\ndef rasterize_pdf_to_temp(pdf_path: str, dpi: int) -> str:\n    from tempfile import NamedTemporaryFile\n\n    images = render_pdf_pages(pdf_path, dpi)\n    if not images:\n        raise RuntimeError(\"Failed to render PDF pages for rasterization.\")\n\n    temp_file = NamedTemporaryFile(delete=False, suffix=\".pdf\")\n    temp_file.close()\n    first = images[0]\n    rest = images[1:]\n    first.save(temp_file.name, format=\"PDF\", save_all=True, append_images=rest)\n    return temp_file.name\n\n\ndef ocr_pages_with_paddle(\n    images: Sequence[Any],\n    languages: str,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    from paddleocr import PaddleOCR\n\n    try:\n        import numpy as np\n    except Exception as exc:\n        raise RuntimeError(f\"numpy is required for PaddleOCR: {exc}\") from exc\n\n    # Newer PaddleOCR prefers use_textline_orientation; older uses use_angle_cls\n    try:\n        ocr = PaddleOCR(use_textline_orientation=True, lang=languages)\n    except TypeError:\n        ocr = PaddleOCR(use_angle_cls=True, lang=languages)\n    pages: List[Dict[str, Any]] = []\n    confidences: List[float] = []\n\n    def _bbox_from_quad(quad: Sequence[Sequence[float]]) -> Tuple[float, float, float, float, float]:\n        xs = [p[0] for p in quad]\n        ys = [p[1] for p in quad]\n        x0, y0, x1, y1 = float(min(xs)), float(min(ys)), float(max(xs)), float(max(ys))\n        xc = 0.5 * (x0 + x1)\n        return x0, y0, x1, y1, xc\n\n    def _order_blocks_into_columns(blocks: List[Dict[str, Any]]) -> str:\n        if not blocks:\n            return \"\"\n        # Robust grouping by x-center: find one or two big gaps -> 2 or 3 columns\n        xs = sorted(b[\"xc\"] for b in blocks)\n        x_min, x_max = xs[0], xs[-1]\n        span = max(1.0, x_max - x_min)\n        widths = sorted((b[\"x1\"] - b[\"x0\"]) for b in blocks)\n        w_med = widths[len(widths)//2] if widths else 1.0\n        # Lower threshold than before: helps separate three narrow columns\n        gap_thr = max(0.06 * span, 0.5 * w_med)\n\n        # Compute gaps between consecutive x-centers\n        diffs: List[Tuple[float, int]] = []\n        for i in range(1, len(xs)):\n            diffs.append((xs[i] - xs[i-1], i))  # (gap, split_index)\n        # Candidate split positions are those with large gaps\n        candidates = [idx for (gap, idx) in diffs if gap >= gap_thr]\n\n        # Build columns by splitting at up to two largest valid gaps ensuring min size per group\n        min_lines = max(3, len(blocks) // 20 or 1)\n        columns: List[List[Dict[str, Any]]] = []\n        blocks_sorted = sorted(blocks, key=lambda b: b[\"xc\"])  # align with xs order\n        used_splits: List[int] = []\n        if candidates:\n            # Prefer two-gap (3-column) split if possible\n            cands_sorted = sorted(((xs[i-1], xs[i], i) for i in candidates), key=lambda t: t[1]-t[0], reverse=True)\n            # Try all pairs of split indices to form 3 groups\n            tried = False\n            for _a in range(min(5, len(cands_sorted))):\n                for _b in range(_a+1, min(6, len(cands_sorted))):\n                    i1 = cands_sorted[_a][2]\n                    i2 = cands_sorted[_b][2]\n                    a, b = sorted([i1, i2])\n                    if a < min_lines or (b - a) < min_lines or (len(blocks) - b) < min_lines:\n                        continue\n                    used_splits = [a, b]\n                    tried = True\n                    break\n                if tried:\n                    break\n            if not used_splits:\n                # Fall back to single split (2 columns)\n                # pick the largest valid gap that yields two groups of minimum size\n                for _, _, i in cands_sorted:\n                    if i >= min_lines and (len(blocks) - i) >= min_lines:\n                        used_splits = [i]\n                        break\n\n        if used_splits:\n            used_splits = sorted(set(used_splits))\n            start = 0\n            for s in used_splits:\n                columns.append(blocks_sorted[start:s])\n                start = s\n            columns.append(blocks_sorted[start:])\n        else:\n            # Fallback threshold grouping\n            cur: List[Dict[str, Any]] = []\n            prev_xc: Optional[float] = None\n            for b in blocks_sorted:\n                if prev_xc is None or abs(b[\"xc\"] - prev_xc) <= gap_thr:\n                    cur.append(b)\n                else:\n                    if cur:\n                        columns.append(cur)\n                    cur = [b]\n                prev_xc = b[\"xc\"]\n            if cur:\n                columns.append(cur)\n\n        # Sort columns left-to-right by median x center\n        def col_key(col: List[Dict[str, Any]]) -> float:\n            centers = sorted(b[\"xc\"] for b in col)\n            return centers[len(centers)//2]\n        columns = [col for col in columns if col]\n        columns.sort(key=col_key)\n        try:\n            LOGGER.info(\"Paddle column grouping: k=%d (gap_thr=%.2f, span=%.1f)\", len(columns), gap_thr, span)\n        except Exception:\n            pass\n        # Within each column, sort top-down and join\n        col_texts: List[str] = []\n        for col in columns:\n            col_sorted = sorted(col, key=lambda b: (b[\"y0\"], b[\"x0\"]))\n            col_texts.append(\"\\n\".join(b[\"text\"] for b in col_sorted if b[\"text\"]))\n        # Read columns left to right\n        return \"\\n\\n\".join(t for t in col_texts if t)\n\n    total = max(1, len(images))\n    for idx, image in enumerate(images, start=1):\n        # Prefer new API: predict(); fall back to ocr() with/without cls\n        try:\n            result = ocr.predict(np.array(image))  # type: ignore[attr-defined]\n        except Exception:\n            try:\n                result = ocr.ocr(np.array(image), cls=True)\n            except TypeError:\n                result = ocr.ocr(np.array(image))\n        blocks: List[Dict[str, Any]] = []\n        if result:\n            entries = result[0] if isinstance(result, list) else result\n            for entry in entries:\n                if not entry or not isinstance(entry, (list, tuple)):\n                    continue\n                quad = entry[0] if len(entry) > 0 else None\n                text_part = entry[1] if len(entry) > 1 else None\n                if quad is None or text_part is None:\n                    continue\n                try:\n                    x0, y0, x1, y1, xc = _bbox_from_quad(quad)\n                except Exception:\n                    continue\n                text_val = \"\"\n                conf_val = None\n                if isinstance(text_part, (list, tuple)) and text_part:\n                    text_val = str(text_part[0] or \"\").strip()\n                    if len(text_part) > 1 and isinstance(text_part[1], (float, int)):\n                        conf_val = float(text_part[1])\n                else:\n                    text_val = str(text_part or \"\").strip()\n                if conf_val is not None:\n                    confidences.append(conf_val)\n                if text_val:\n                    blocks.append({\"x0\": x0, \"y0\": y0, \"x1\": x1, \"y1\": y1, \"xc\": xc, \"text\": text_val})\n        ordered_text = _order_blocks_into_columns(blocks)\n        pages.append({\"page_num\": idx, \"text\": ordered_text})\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"OCR page {idx}/{total}\")\n\n    avg_conf = sum(confidences) / len(confidences) if confidences else None\n    return pages, {\"ocr_confidence_avg\": avg_conf}\n\n\ndef ocr_pages_with_tesseract(\n    images: Sequence[Any],\n    languages: str,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    import pytesseract\n\n    pages: List[Dict[str, Any]] = []\n    total = max(1, len(images))\n    for idx, image in enumerate(images, start=1):\n        text = pytesseract.image_to_string(image, lang=languages)\n        pages.append({\"page_num\": idx, \"text\": text})\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"OCR page {idx}/{total}\")\n    return pages, {}\n\n\ndef run_external_ocr_pages(\n    pdf_path: str,\n    engine: str,\n    languages: str,\n    config: DoclingProcessingConfig,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    images = render_pdf_pages(pdf_path, config.ocr_dpi)\n    if engine == \"paddle\":\n        return ocr_pages_with_paddle(\n            images,\n            normalize_languages_for_engine(languages, engine),\n            progress_cb,\n            progress_base,\n            progress_span,\n        )\n    if engine == \"tesseract\":\n        return ocr_pages_with_tesseract(\n            images,\n            normalize_languages_for_engine(languages, engine),\n            progress_cb,\n            progress_base,\n            progress_span,\n        )\n    return [], {}\n\n\ndef build_quality_report(pdf_path: str, config: DoclingProcessingConfig) -> Dict[str, Any]:\n    analysis_pages = extract_pages_from_pdf(\n        pdf_path,\n        max_pages=config.analysis_max_pages,\n        sample_strategy=config.analysis_sample_strategy,\n    )\n    has_text_layer = detect_text_layer_from_pages(analysis_pages, config)\n    languages = select_language_set(config.language_hint, pdf_path, config)\n    quality = estimate_text_quality(analysis_pages, config, languages)\n    low_quality = is_low_quality(quality, config)\n    return {\n        \"text_layer_detected\": has_text_layer,\n        \"text_layer_low_quality\": has_text_layer and low_quality,\n        \"avg_chars_per_page\": quality.avg_chars_per_page,\n        \"alpha_ratio\": quality.alpha_ratio,\n        \"suspicious_token_ratio\": quality.suspicious_token_ratio,\n        \"confidence_proxy\": quality.confidence_proxy,\n        \"dictionary_hit_ratio\": quality.dictionary_hit_ratio,\n    }\n\n\ndef convert_pdf_with_docling(\n    pdf_path: str,\n    config: DoclingProcessingConfig,\n    progress_cb: Optional[ProgressCallback] = None,\n) -> DoclingConversionResult:\n    emit = progress_cb or (lambda _p, _s, _m: None)\n    emit(5, \"analysis\", \"Analyzing text layer\")\n    analysis_pages = extract_pages_from_pdf(\n        pdf_path,\n        max_pages=config.analysis_max_pages,\n        sample_strategy=config.analysis_sample_strategy,\n    )\n    has_text_layer = detect_text_layer_from_pages(analysis_pages, config)\n    languages = select_language_set(config.language_hint, pdf_path, config)\n    quality = estimate_text_quality(analysis_pages, config, languages)\n    low_quality = is_low_quality(quality, config)\n    available_engines = detect_available_ocr_engines()\n    decision = decide_ocr_route(has_text_layer, quality, available_engines, config, languages)\n    emit(15, \"route\", \"Selecting OCR route\")\n    rasterized_source = False\n    rasterized_pdf_path = \"\"\n    rasterize_error: Optional[str] = None\n    column_layout: Optional[ColumnLayoutDetection] = None\n    if should_rasterize_text_layer(has_text_layer, low_quality, config):\n        try:\n            rasterized_pdf_path = rasterize_pdf_to_temp(pdf_path, config.ocr_dpi)\n            rasterized_source = True\n            emit(25, \"rasterize\", \"Rasterized PDF for OCR\")\n            LOGGER.info(\"Rasterized low-quality text layer for Docling OCR.\")\n        except Exception as exc:\n            rasterize_error = str(exc)\n            LOGGER.warning(\"Failed to rasterize PDF for OCR: %s\", exc)\n    if rasterized_source:\n        decision.per_page_ocr = False\n        decision.per_page_reason = \"Rasterized PDF for Docling OCR\"\n\n    if config.column_detect_enable and decision.ocr_used and (rasterized_source or not has_text_layer):\n        try:\n            # Spread sampling across document to avoid false negatives on front-matter\n            total_pages = get_pdf_page_count(pdf_path)\n            sample_indices = select_column_sample_indices(total_pages, config.column_detect_max_pages)\n            if not sample_indices:\n                sample_indices = list(range(1, min(3, total_pages or 3) + 1))\n            LOGGER.info(\"Column layout sample pages: %s\", sample_indices)\n\n            sample_images = render_pdf_pages_at_indices(pdf_path, config.column_detect_dpi, sample_indices)\n            column_layout = detect_multicolumn_layout(sample_images, config)\n            # If not detected, retry at a higher DPI once\n            if not column_layout.detected and config.column_detect_dpi < 220:\n                hi_dpi = 300\n                hi_images = render_pdf_pages_at_indices(pdf_path, hi_dpi, sample_indices)\n                hi_layout = detect_multicolumn_layout(hi_images, config)\n                if hi_layout.detected:\n                    column_layout = hi_layout\n                    LOGGER.info(\"Column layout detection (hi-dpi %d): %s (%s)\", hi_dpi, column_layout.detected, column_layout.reason)\n            LOGGER.info(\n                \"Column layout detection: %s (%s)\",\n                column_layout.detected,\n                column_layout.reason,\n            )\n            emit(30, \"layout\", \"Checked column layout\")\n            if column_layout.detected and decision.use_external_ocr and decision.per_page_ocr:\n                decision.per_page_ocr = False\n                decision.per_page_reason = \"Columns detected; keep Docling layout\"\n        except Exception as exc:\n            LOGGER.warning(\"Column layout detection failed: %s\", exc)\n\n    dict_ratio = \"n/a\" if quality.dictionary_hit_ratio is None else f\"{quality.dictionary_hit_ratio:.2f}\"\n    LOGGER.info(\n        \"Text-layer check: %s (avg_chars=%.1f, alpha_ratio=%.2f, suspicious=%.2f, dict=%s)\",\n        has_text_layer,\n        quality.avg_chars_per_page,\n        quality.alpha_ratio,\n        quality.suspicious_token_ratio,\n        dict_ratio,\n    )\n    if available_engines:\n        LOGGER.info(\"Available OCR engines: %s\", \", \".join(available_engines))\n    else:\n        LOGGER.info(\"Available OCR engines: none (external OCR disabled)\")\n\n    LOGGER.info(\n        \"Docling OCR route: %s (engine=%s, languages=%s)\",\n        decision.route_reason,\n        decision.ocr_engine,\n        decision.languages,\n    )\n    LOGGER.info(\"Per-page OCR: %s (%s)\", decision.per_page_ocr, decision.per_page_reason)\n    if decision.ocr_used and not decision.use_external_ocr:\n        LOGGER.info(\"External OCR unavailable; relying on Docling OCR.\")\n\n    converter = build_converter(config, decision)\n    docling_input = rasterized_pdf_path or pdf_path\n    emit(40, \"docling\", \"Docling conversion running\")\n    result = converter.convert(docling_input)\n    doc = result.document if hasattr(result, \"document\") else result\n    markdown = export_markdown(doc)\n    pages = extract_pages(doc)\n    if len(pages) <= 1:\n        fallback_pages = extract_pages_from_pdf(pdf_path)\n        if len(fallback_pages) > len(pages):\n            pages = fallback_pages\n    emit(70, \"docling\", \"Docling conversion complete\")\n\n    ocr_stats: Dict[str, Any] = {}\n    # Always allow external OCR if selected, even when the PDF was rasterized for Docling,\n    # so we can prefer column-aware ordering from Paddle/Tesseract when desired.\n    if decision.ocr_used and decision.use_external_ocr:\n        try:\n            ocr_pages, ocr_stats = run_external_ocr_pages(\n                pdf_path,\n                decision.ocr_engine,\n                languages,\n                config,\n                progress_cb=emit,\n                progress_base=70,\n                progress_span=20,\n            )\n            if ocr_pages:\n                pages = ocr_pages\n                if config.postprocess_markdown and not markdown.strip():\n                    markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in ocr_pages)\n        except Exception as exc:\n            LOGGER.warning(\"External OCR failed (%s): %s\", decision.ocr_engine, exc)\n    if rasterized_source and rasterized_pdf_path:\n        try:\n            os.unlink(rasterized_pdf_path)\n        except Exception:\n            pass\n\n    emit(90, \"chunking\", \"Building chunks\")\n    metadata = {\n        \"ocr_used\": decision.ocr_used,\n        \"ocr_engine\": decision.ocr_engine,\n        \"languages\": decision.languages,\n        \"route_reason\": decision.route_reason,\n        \"per_page_reason\": decision.per_page_reason,\n        \"text_layer_detected\": has_text_layer,\n        \"text_layer_low_quality\": has_text_layer and low_quality,\n        \"rasterized_source_pdf\": rasterized_source,\n        \"rasterize_failed\": bool(rasterize_error),\n        \"rasterize_error\": rasterize_error,\n        \"column_layout_detected\": column_layout.detected if column_layout else None,\n        \"column_layout_ratio\": column_layout.page_ratio if column_layout else None,\n        \"column_layout_reason\": column_layout.reason if column_layout else None,\n        \"avg_chars_per_page\": quality.avg_chars_per_page,\n        \"alpha_ratio\": quality.alpha_ratio,\n        \"suspicious_token_ratio\": quality.suspicious_token_ratio,\n        \"confidence_proxy\": quality.confidence_proxy,\n        \"dictionary_hit_ratio\": quality.dictionary_hit_ratio,\n        \"per_page_ocr\": decision.per_page_ocr,\n    }\n    # Attach spellchecker backend info if available\n    if LAST_SPELLCHECKER_INFO:\n        try:\n            metadata.update({\n                \"spellchecker_backend\": LAST_SPELLCHECKER_INFO.get(\"backend\"),\n                \"spellchecker_dic\": LAST_SPELLCHECKER_INFO.get(\"dic\"),\n                \"spellchecker_aff\": LAST_SPELLCHECKER_INFO.get(\"aff\"),\n            })\n        except Exception:\n            pass\n    metadata.update(ocr_stats)\n    emit(100, \"done\", \"Extraction complete\")\n    return DoclingConversionResult(markdown=markdown, pages=pages, metadata=metadata)\n\n\ndef build_chunks_page(\n    doc_id: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    postprocess: Optional[Callable[[str], str]] = None,\n) -> List[Dict[str, Any]]:\n    chunks: List[Dict[str, Any]] = []\n    for page in pages:\n        raw_text = str(page.get(\"text\", \"\"))\n        if postprocess:\n            raw_text = postprocess(raw_text)\n        raw_text = clean_chunk_text(raw_text, config)\n        cleaned = normalize_text(raw_text)\n        if not cleaned:\n            continue\n        page_num = int(page.get(\"page_num\", 0))\n        chunk_id = f\"p{page_num}\"\n        chunks.append({\n            \"chunk_id\": chunk_id,\n            \"text\": cleaned,\n            \"page_start\": page_num,\n            \"page_end\": page_num,\n            \"section\": \"\",\n            \"char_count\": len(cleaned),\n        })\n    return chunks\n\n\ndef build_chunks_section(\n    doc_id: str,\n    markdown: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    postprocess: Optional[Callable[[str], str]] = None,\n) -> List[Dict[str, Any]]:\n    sections = split_markdown_sections(markdown)\n    chunks: List[Dict[str, Any]] = []\n    seen_ids: Dict[str, int] = {}\n\n    if not sections:\n        return build_chunks_page(doc_id, pages, config=config)\n\n    for idx, section in enumerate(sections, start=1):\n        title = section.get(\"title\", \"\")\n        text = section.get(\"text\", \"\")\n        if postprocess:\n            text = postprocess(text)\n        text = clean_chunk_text(text, config)\n        if not text.strip():\n            continue\n        base_id = slugify(title) or f\"section-{idx}\"\n        if base_id in seen_ids:\n            seen_ids[base_id] += 1\n            base_id = f\"{base_id}-{seen_ids[base_id]}\"\n        else:\n            seen_ids[base_id] = 1\n        max_chars = config.max_chunk_chars if config else 0\n        overlap_chars = config.chunk_overlap_chars if config else 0\n        segments = split_text_by_size(text, max_chars, overlap_chars)\n        for seg_idx, segment in enumerate(segments, start=1):\n            cleaned = normalize_text(segment)\n            if not cleaned:\n                continue\n            page_start, page_end = find_page_range(cleaned, pages, config)\n            chunk_id = base_id if seg_idx == 1 else f\"{base_id}-{seg_idx}\"\n            chunks.append({\n                \"chunk_id\": chunk_id,\n                \"text\": cleaned,\n                \"page_start\": page_start,\n                \"page_end\": page_end,\n                \"section\": title,\n                \"char_count\": len(cleaned),\n            })\n    return chunks\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Extract PDF content with Docling and produce chunks.\")\n    parser.add_argument(\"--download-hunspell\", metavar=\"LANG_CODE\", type=str, help=\"Download Hunspell dictionary for given language code (e.g. de_DE, en_US, fr_FR)\")\n    parser.add_argument(\"--pdf\", required=False, help=\"Path to PDF\")\n    parser.add_argument(\"--doc-id\", help=\"Document identifier\")\n    parser.add_argument(\"--out-json\", help=\"Output JSON path\")\n    parser.add_argument(\"--out-md\", help=\"Output markdown path\")\n    parser.add_argument(\"--log-file\", help=\"Optional path to write a detailed log file\")\n    parser.add_argument(\"--spellchecker-info-out\", help=\"Optional path to write spellchecker backend info JSON\")\n    parser.add_argument(\"--chunking\", choices=[\"page\", \"section\"], default=\"page\")\n    parser.add_argument(\"--ocr\", choices=[\"auto\", \"force\", \"off\"], default=\"auto\")\n    parser.add_argument(\"--language-hint\", help=\"Language hint for OCR/quality (e.g., eng, deu, deu+eng)\")\n    parser.add_argument(\n        \"--max-chunk-chars\",\n        type=int,\n        help=\"Max chars for section chunks before splitting (section mode only).\",\n    )\n    parser.add_argument(\n        \"--chunk-overlap-chars\",\n        type=int,\n        help=\"Overlap chars when splitting large section chunks.\",\n    )\n    parser.add_argument(\n        \"--keep-image-tags\",\n        action=\"store_true\",\n        help=\"Preserve '<!-- image -->' tags instead of removing them.\",\n    )\n    parser.add_argument(\n        \"--force-ocr-low-quality\",\n        action=\"store_true\",\n        help=\"Force OCR when text layer appears low quality\",\n    )\n    parser.add_argument(\n        \"--quality-threshold\",\n        type=float,\n        help=\"Confidence threshold for treating text as low quality (0-1)\",\n    )\n    parser.add_argument(\"--quality-only\", action=\"store_true\", help=\"Output text-layer quality JSON and exit\")\n    parser.add_argument(\"--enable-llm-cleanup\", action=\"store_true\", help=\"Enable LLM cleanup for low-quality chunks\")\n    parser.add_argument(\"--llm-cleanup-base-url\", help=\"OpenAI-compatible base URL for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-api-key\", help=\"API key for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-model\", help=\"Model name for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-temperature\", type=float, help=\"Temperature for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-max-chars\", type=int, help=\"Max chars per chunk for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-min-quality\", type=float, help=\"Min quality threshold for LLM cleanup\")\n    parser.add_argument(\"--progress\", action=\"store_true\", help=\"Emit JSON progress events to stdout\")\n    parser.add_argument(\"--enable-dictionary-correction\", action=\"store_true\", help=\"Enable dictionary-based OCR corrections\")\n    parser.add_argument(\"--dictionary-path\", help=\"Path to dictionary wordlist (one word per line)\")\n    parser.add_argument(\"--enable-hunspell\", action=\"store_true\", help=\"Enable Hunspell dictionary support if available\")\n    parser.add_argument(\"--hunspell-aff\", help=\"Path to Hunspell .aff file\")\n    parser.add_argument(\"--hunspell-dic\", help=\"Path to Hunspell .dic file\")\n\n    # Parse only known args to allow --download-hunspell to work standalone\n    args, _ = parser.parse_known_args()\n\n    if args.download_hunspell:\n        lang_code = args.download_hunspell\n        # Map special cases for repo structure\n        repo_map = {\n            \"de_DE\": (\"de\", \"de_DE_frami\"),\n            \"de_AT\": (\"de\", \"de_AT\"),\n            \"de_CH\": (\"de\", \"de_CH\"),\n            \"en_US\": (\"en\", \"en_US\"),\n            \"en_GB\": (\"en\", \"en_GB\"),\n            \"fr_FR\": (\"fr_FR\", \"fr\"),\n        }\n        # Default: folder and file prefix are lang_code\n        folder, prefix = repo_map.get(lang_code, (lang_code, lang_code))\n        base_url = f\"https://raw.githubusercontent.com/LibreOffice/dictionaries/master/{folder}/\"\n        aff_name = f\"{prefix}.aff\"\n        dic_name = f\"{prefix}.dic\"\n        aff_url = base_url + aff_name\n        dic_url = base_url + dic_name\n        out_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n        os.makedirs(out_dir, exist_ok=True)\n        aff_path = os.path.join(out_dir, f\"{lang_code}.aff\")\n        dic_path = os.path.join(out_dir, f\"{lang_code}.dic\")\n        def download(url, out_path):\n            try:\n                import urllib.request\n                urllib.request.urlretrieve(url, out_path)\n                return True\n            except Exception as exc:\n                print(f\"Failed to download {url}: {exc}\")\n                return False\n        print(f\"Downloading {aff_url} -> {aff_path}\")\n        ok_aff = download(aff_url, aff_path)\n        print(f\"Downloading {dic_url} -> {dic_path}\")\n        ok_dic = download(dic_url, dic_path)\n        if ok_aff and ok_dic:\n            print(f\"Successfully downloaded Hunspell dictionary for {lang_code} to {out_dir}\")\n            return 0\n        else:\n            print(f\"Failed to download Hunspell dictionary for {lang_code}. Check the language code or try manually.\")\n            return 1\n\n    # Require --pdf for normal operation\n    if not args.pdf:\n        parser.print_help()\n        return 2\n\n    logging.basicConfig(level=logging.INFO)\n    # If a log file was requested, add a file handler\n    if args.log_file:\n        try:\n            fh = logging.FileHandler(args.log_file, encoding=\"utf-8\")\n            fh.setLevel(logging.INFO)\n            formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(name)s: %(message)s\")\n            fh.setFormatter(formatter)\n            logging.getLogger().addHandler(fh)\n            LOGGER.info(\"Logging to file: %s\", args.log_file)\n        except Exception as exc:\n            eprint(f\"Failed to set up log file {args.log_file}: {exc}\")\n\n\n    if not os.path.isfile(args.pdf):\n        eprint(f\"PDF not found: {args.pdf}\")\n        return 2\n\n    if args.quality_only:\n        config = DoclingProcessingConfig(ocr_mode=args.ocr)\n        if args.force_ocr_low_quality:\n            config.force_ocr_on_low_quality_text = True\n        if args.quality_threshold is not None:\n            config.quality_confidence_threshold = args.quality_threshold\n        report = build_quality_report(args.pdf, config)\n        print(json.dumps(report))\n        return 0\n\n    if not args.doc_id or not args.out_json or not args.out_md:\n        eprint(\"Missing required arguments: --doc-id, --out-json, --out-md\")\n        return 2\n\n    try:\n        out_json_dir = os.path.dirname(args.out_json)\n        out_md_dir = os.path.dirname(args.out_md)\n        if out_json_dir:\n            os.makedirs(out_json_dir, exist_ok=True)\n        if out_md_dir:\n            os.makedirs(out_md_dir, exist_ok=True)\n    except Exception as exc:\n        eprint(f\"Failed to create output directories: {exc}\")\n        return 2\n\n    config = DoclingProcessingConfig(ocr_mode=args.ocr)\n    if args.force_ocr_low_quality:\n        config.force_ocr_on_low_quality_text = True\n    if args.quality_threshold is not None:\n        config.quality_confidence_threshold = args.quality_threshold\n    if args.language_hint:\n        config.language_hint = args.language_hint\n    if args.max_chunk_chars is not None:\n        config.max_chunk_chars = args.max_chunk_chars\n    if args.chunk_overlap_chars is not None:\n        config.chunk_overlap_chars = args.chunk_overlap_chars\n    if args.keep_image_tags:\n        config.cleanup_remove_image_tags = False\n    if args.enable_llm_cleanup:\n        config.enable_llm_correction = True\n    if args.enable_dictionary_correction:\n        config.enable_dictionary_correction = True\n    if args.dictionary_path:\n        config.dictionary_path = args.dictionary_path\n    if args.enable_hunspell:\n        config.enable_hunspell = True\n    if args.hunspell_aff:\n        config.hunspell_aff_path = args.hunspell_aff\n    if args.hunspell_dic:\n        config.hunspell_dic_path = args.hunspell_dic\n    if args.llm_cleanup_base_url:\n        config.llm_cleanup_base_url = args.llm_cleanup_base_url\n    if args.llm_cleanup_api_key:\n        config.llm_cleanup_api_key = args.llm_cleanup_api_key\n    if args.llm_cleanup_model:\n        config.llm_cleanup_model = args.llm_cleanup_model\n    if args.llm_cleanup_temperature is not None:\n        config.llm_cleanup_temperature = args.llm_cleanup_temperature\n    if args.llm_cleanup_max_chars is not None:\n        config.llm_correction_max_chars = args.llm_cleanup_max_chars\n    if args.llm_cleanup_min_quality is not None:\n        config.llm_correction_min_quality = args.llm_cleanup_min_quality\n\n    config.llm_correct = build_llm_cleanup_callback(config)\n\n    # Proactively build spellchecker once to record backend info; will be reused lazily later\n    spell_langs = select_language_set(config.language_hint, args.pdf, config)\n    if config.enable_hunspell:\n        try:\n            _ = build_spellchecker_for_languages(config, spell_langs)\n        except Exception:\n            pass\n\n    # Optionally write spellchecker backend info to a file\n    if args.spellchecker_info_out:\n        try:\n            info = dict(LAST_SPELLCHECKER_INFO)\n            info[\"languages\"] = spell_langs\n            out_dir = os.path.dirname(args.spellchecker_info_out)\n            if out_dir:\n                os.makedirs(out_dir, exist_ok=True)\n            with open(args.spellchecker_info_out, \"w\", encoding=\"utf-8\") as fh:\n                json.dump(info, fh, indent=2)\n            LOGGER.info(\"Wrote spellchecker info to %s\", args.spellchecker_info_out)\n        except Exception as exc:\n            LOGGER.warning(\"Failed to write spellchecker info file: %s\", exc)\n\n    progress_cb = make_progress_emitter(bool(args.progress))\n\n    try:\n        conversion = convert_pdf_with_docling(args.pdf, config, progress_cb=progress_cb)\n    except Exception as exc:\n        eprint(f\"Docling conversion failed: {exc}\")\n        return 2\n\n    markdown = conversion.markdown\n    if config.enable_post_correction and config.postprocess_markdown and conversion.metadata.get(\"ocr_used\"):\n        wordlist = prepare_dictionary_words(config)\n        languages = conversion.metadata.get(\"languages\", config.default_lang_english)\n        markdown = postprocess_text(markdown, config, languages, wordlist)\n\n    try:\n        with open(args.out_md, \"w\", encoding=\"utf-8\") as handle:\n            handle.write(markdown)\n    except Exception as exc:\n        eprint(f\"Failed to write markdown: {exc}\")\n        return 2\n\n    try:\n        pages = conversion.pages\n        languages = conversion.metadata.get(\"languages\", config.default_lang_english)\n        postprocess_fn: Optional[Callable[[str], str]] = None\n        if config.enable_post_correction and conversion.metadata.get(\"ocr_used\"):\n            wordlist = prepare_dictionary_words(config)\n            postprocess_fn = lambda text: postprocess_text(text, config, languages, wordlist)\n\n        if postprocess_fn:\n            pages = [\n                {\"page_num\": page.get(\"page_num\", idx + 1), \"text\": postprocess_fn(str(page.get(\"text\", \"\")))}\n                for idx, page in enumerate(pages)\n            ]\n\n        if args.chunking == \"section\":\n            chunks = build_chunks_section(\n                args.doc_id,\n                markdown,\n                pages,\n                config=config,\n                postprocess=postprocess_fn,\n            )\n        else:\n            chunks = build_chunks_page(args.doc_id, pages, config=config)\n    except Exception as exc:\n        eprint(f\"Failed to build chunks: {exc}\")\n        return 2\n\n    chunks = [chunk for chunk in chunks if chunk.get(\"text\")]\n\n    payload = {\n        \"doc_id\": args.doc_id,\n        \"source_pdf\": args.pdf,\n        \"chunks\": chunks,\n        \"metadata\": conversion.metadata,\n    }\n\n    try:\n        with open(args.out_json, \"w\", encoding=\"utf-8\") as handle:\n            json.dump(payload, handle, indent=2)\n    except Exception as exc:\n        eprint(f\"Failed to write JSON: {exc}\")\n        return 2\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "index_redisearch.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.3\nimport argparse\nimport json\nimport math\nimport os\nimport struct\nimport sys\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom utils_embedding import normalize_vector, vector_to_bytes, request_embedding\nimport redis\nimport requests\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef normalize_vector(values: List[float]) -> List[float]:\n    norm = math.sqrt(sum(v * v for v in values))\n    if norm == 0:\n        return values\n    return [v / norm for v in values]\n\n\ndef vector_to_bytes(values: List[float]) -> bytes:\n    return struct.pack(\"<\" + \"f\" * len(values), *values)\n\n\ndef request_embedding(base_url: str, api_key: str, model: str, text: str) -> List[float]:\n    url = base_url.rstrip(\"/\") + \"/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    response = requests.post(url, json={\"input\": text, \"model\": model}, headers=headers, timeout=120)\n    if response.status_code >= 400:\n        raise RuntimeError(f\"Embedding request failed: {response.status_code} {response.text}\")\n    payload = response.json()\n    data = payload.get(\"data\")\n    if not data:\n        raise RuntimeError(\"Embedding response missing data field\")\n    embedding = data[0].get(\"embedding\")\n    if not embedding:\n        raise RuntimeError(\"Embedding response missing embedding\")\n    return [float(x) for x in embedding]\n\n\ndef ensure_index(client: redis.Redis, index_name: str, prefix: str) -> None:\n    try:\n        client.execute_command(\"FT.INFO\", index_name)\n        ensure_schema_fields(client, index_name)\n        return\n    except redis.exceptions.ResponseError as exc:\n        message = str(exc).lower()\n        if \"unknown index name\" not in message:\n            raise\n\n    client.execute_command(\n        \"FT.CREATE\",\n        index_name,\n        \"ON\",\n        \"HASH\",\n        \"PREFIX\",\n        \"1\",\n        prefix,\n        \"SCHEMA\",\n    \n    \n    \n        \"TAG\",\n    \n    \n    \n        \"authors\",\n    \n    \n    \n        \"NUMERIC\",\n        \"page_end\",\n        \"NUMERIC\",\n        \"section\",\n        \"TEXT\",\n        \"text\",\n        \"TEXT\",\n        \"embedding\",\n        \"VECTOR\",\n        \"HNSW\",\n        \"6\",\n        \"TYPE\",\n        \"FLOAT32\",\n        \"DIM\",\n        \"768\",\n        \"DISTANCE_METRIC\",\n        \"COSINE\",\n    )\n\n\ndef ensure_schema_fields(client: redis.Redis, index_name: str) -> None:\n    fields: List[Tuple[str, List[str]]] = [\n        (\"attachment_key\", [\"TAG\"]),\n        (\"title\", [\"TEXT\"]),\n        (\"authors\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"tags\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"year\", [\"NUMERIC\"]),\n        (\"item_type\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n    ]\n    for name, spec in fields:\n        try:\n            client.execute_command(\"FT.ALTER\", index_name, \"SCHEMA\", \"ADD\", name, *spec)\n        except redis.exceptions.ResponseError as exc:\n            message = str(exc).lower()\n            if \"duplicate\" in message or \"already exists\" in message:\n                continue\n            raise\n\n\ndef infer_item_json_path(chunks_json: str, doc_id: str) -> Optional[str]:\n    base_name = f\"{doc_id}.json\"\n    chunks_dir = os.path.dirname(chunks_json)\n    candidates: List[str] = []\n    if os.path.basename(chunks_dir) == \"chunks\":\n        candidates.append(os.path.join(os.path.dirname(chunks_dir), \"items\", base_name))\n    marker = f\"{os.sep}chunks{os.sep}\"\n    if marker in chunks_json:\n        candidates.append(chunks_json.replace(marker, f\"{os.sep}items{os.sep}\"))\n    candidates.append(os.path.join(chunks_dir, base_name))\n    for candidate in candidates:\n        if os.path.isfile(candidate):\n            return candidate\n    return None\n\n\ndef parse_item_metadata(item_payload: Dict[str, Any]) -> Dict[str, Any]:\n    data = item_payload.get(\"data\") if isinstance(item_payload.get(\"data\"), dict) else item_payload\n    title = str(data.get(\"title\", \"\")).strip()\n    item_type = str(data.get(\"itemType\", \"\")).strip()\n    tags: List[str] = []\n    for tag in data.get(\"tags\", []) or []:\n        if isinstance(tag, dict):\n            value = str(tag.get(\"tag\", \"\")).strip()\n        else:\n            value = str(tag).strip()\n        if value:\n            tags.append(value)\n\n    creators = data.get(\"creators\", []) or []\n    authors: List[str] = []\n    for creator in creators:\n        if not isinstance(creator, dict):\n            continue\n        name = \"\"\n        if creator.get(\"name\"):\n            name = str(creator.get(\"name\", \"\")).strip()\n        else:\n            first = str(creator.get(\"firstName\", \"\")).strip()\n            last = str(creator.get(\"lastName\", \"\")).strip()\n            name = \" \".join(part for part in (first, last) if part)\n        if name:\n            authors.append(name)\n\n    year = 0\n    date_field = str(data.get(\"date\", \"\")).strip()\n    match = None\n    if date_field:\n        match = next(iter(__import__(\"re\").findall(r\"(1[5-9]\\\\d{2}|20\\\\d{2})\", date_field)), None)\n    if match:\n        try:\n            year = int(match)\n        except ValueError:\n            year = 0\n    elif isinstance(data.get(\"year\"), (int, float)):\n        year = int(data.get(\"year\"))\n\n    return {\n        \"title\": title,\n        \"authors\": \"|\".join(authors),\n        \"tags\": \"|\".join(tags),\n        \"year\": year,\n        \"item_type\": item_type,\n    }\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Index Docling chunks into RedisSearch.\")\n    parser.add_argument(\"--chunks-json\", required=True)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--item-json\")\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--upsert\", action=\"store_true\")\n    parser.add_argument(\"--progress\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if not os.path.isfile(args.chunks_json):\n        eprint(f\"Chunks JSON not found: {args.chunks_json}\")\n        return 2\n\n    try:\n        with open(args.chunks_json, \"r\", encoding=\"utf-8\") as handle:\n            payload = json.load(handle)\n    except Exception as exc:\n        eprint(f\"Failed to read chunks JSON: {exc}\")\n        return 2\n\n\n    doc_id = payload.get(\"doc_id\")\n    chunks = payload.get(\"chunks\")\n    if not doc_id or not isinstance(chunks, list):\n        eprint(\"Invalid chunks JSON schema\")\n        return 2\n\n    # Delete all existing chunk keys for this doc_id before indexing\n    pattern = f\"{args.prefix}{doc_id}:*\"\n    try:\n        keys_to_delete = client.keys(pattern)\n        if keys_to_delete:\n            client.delete(*keys_to_delete)\n            eprint(f\"Deleted {len(keys_to_delete)} existing chunk keys for doc_id {doc_id}\")\n    except Exception as exc:\n        eprint(f\"Failed to delete old chunk keys for doc_id {doc_id}: {exc}\")\n\n    attachment_key = None\n    try:\n        meta = payload.get(\"metadata\") if isinstance(payload.get(\"metadata\"), dict) else {}\n        key_val = meta.get(\"attachment_key\") if isinstance(meta, dict) else None\n        if isinstance(key_val, str) and key_val.strip():\n            attachment_key = key_val.strip()\n    except Exception:\n        attachment_key = None\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n\n    try:\n        ensure_index(client, args.index, args.prefix)\n    except Exception as exc:\n        eprint(f\"Failed to ensure index: {exc}\")\n        return 2\n\n    item_metadata: Dict[str, Any] = {}\n    item_json_path = args.item_json or infer_item_json_path(args.chunks_json, str(doc_id))\n    if item_json_path and os.path.isfile(item_json_path):\n        try:\n            with open(item_json_path, \"r\", encoding=\"utf-8\") as handle:\n                item_payload = json.load(handle)\n            item_metadata = parse_item_metadata(item_payload)\n        except Exception as exc:\n            eprint(f\"Failed to read item JSON metadata: {exc}\")\n\n    valid_chunks = []\n    for chunk in chunks:\n        text = str(chunk.get(\"text\", \"\"))\n        if not text.strip():\n            continue\n        chunk_id = chunk.get(\"chunk_id\")\n        if not chunk_id:\n            continue\n        valid_chunks.append(chunk)\n\n    total = len(valid_chunks)\n    current = 0\n\n    for chunk in valid_chunks:\n        current += 1\n        text = str(chunk.get(\"text\", \"\"))\n        chunk_id = chunk.get(\"chunk_id\")\n\n        stable_chunk_id = f\"{doc_id}:{chunk_id}\"\n        key = f\"{args.prefix}{stable_chunk_id}\"\n\n        if not args.upsert and client.exists(key):\n            continue\n\n        try:\n            embedding = request_embedding(args.embed_base_url, args.embed_api_key, args.embed_model, text)\n            if len(embedding) != 768:\n                raise RuntimeError(f\"Embedding dim mismatch: {len(embedding)}\")\n            embedding = normalize_vector(embedding)\n        except Exception as exc:\n            eprint(f\"Embedding failed for chunk {stable_chunk_id}: {exc}\")\n            return 2\n\n        fields: Dict[str, Any] = {\n            \"doc_id\": str(doc_id),\n            \"chunk_id\": stable_chunk_id,\n            \"attachment_key\": str(attachment_key or \"\"),\n            \"title\": str(item_metadata.get(\"title\", \"\")),\n            \"authors\": str(item_metadata.get(\"authors\", \"\")),\n            \"tags\": str(item_metadata.get(\"tags\", \"\")),\n            \"year\": int(item_metadata.get(\"year\", 0)),\n            \"item_type\": str(item_metadata.get(\"item_type\", \"\")),\n            \"source_pdf\": str(payload.get(\"source_pdf\", \"\")),\n            \"page_start\": int(chunk.get(\"page_start\", 0)),\n            \"page_end\": int(chunk.get(\"page_end\", 0)),\n            \"section\": str(chunk.get(\"section\", \"\")),\n            \"text\": text,\n            \"embedding\": vector_to_bytes(embedding),\n        }\n\n        try:\n            client.hset(key, mapping=fields)\n        except Exception as exc:\n            eprint(f\"Failed to index chunk {stable_chunk_id}: {exc}\")\n            return 2\n\n        if args.progress:\n            print(json.dumps({\"type\": \"progress\", \"current\": current, \"total\": total}), flush=True)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "rag_query_redisearch.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.3\n\nimport argparse\nimport json\nimport math\nfrom utils_embedding import normalize_vector, vector_to_bytes, request_embedding\nimport re\nimport struct\nimport sys\nfrom typing import Any, Dict, List\n\nimport redis\nimport requests\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\ndef is_temperature_unsupported(message: str) -> bool:\n    lowered = message.lower()\n    return \"temperature\" in lowered and (\n        \"not supported\" in lowered or \"unsupported\" in lowered or \"unknown parameter\" in lowered\n    )\n\n\ndef is_stream_unsupported(message: str) -> bool:\n    lowered = message.lower()\n    return \"stream\" in lowered and (\"not supported\" in lowered or \"unsupported\" in lowered or \"unknown parameter\" in lowered)\n\n\ndef request_chat(\n    base_url: str,\n    api_key: str,\n    model: str,\n    temperature: float,\n    system_prompt: str,\n    user_prompt: str,\n) -> str:\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    base_payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n    }\n    payload = dict(base_payload)\n    payload[\"temperature\"] = temperature\n\n    response = requests.post(url, json=payload, headers=headers, timeout=120)\n    response.encoding = \"utf-8\"\n    if response.status_code >= 400:\n        error_text = response.text\n        if is_temperature_unsupported(error_text):\n            response = requests.post(url, json=base_payload, headers=headers, timeout=120)\n            response.encoding = \"utf-8\"\n            if response.status_code >= 400:\n                raise RuntimeError(f\"Chat request failed: {response.status_code} {response.text}\")\n        else:\n            raise RuntimeError(f\"Chat request failed: {response.status_code} {error_text}\")\n\n    data = response.json()\n    choices = data.get(\"choices\")\n    if not choices:\n        raise RuntimeError(\"Chat response missing choices\")\n    message = choices[0].get(\"message\") or {}\n    content = message.get(\"content\")\n    if not content:\n        raise RuntimeError(\"Chat response missing content\")\n    return content\n\n\ndef request_chat_stream(\n    base_url: str,\n    api_key: str,\n    model: str,\n    temperature: float,\n    system_prompt: str,\n    user_prompt: str,\n    on_delta,\n) -> str:\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    base_payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        \"stream\": True,\n    }\n    payload = dict(base_payload)\n    payload[\"temperature\"] = temperature\n\n    response = requests.post(url, json=payload, headers=headers, timeout=120, stream=True)\n    response.encoding = \"utf-8\"\n    if response.status_code >= 400:\n        error_text = response.text\n        if is_temperature_unsupported(error_text):\n            response = requests.post(url, json=base_payload, headers=headers, timeout=120, stream=True)\n            response.encoding = \"utf-8\"\n            if response.status_code >= 400:\n                raise RuntimeError(f\"Chat request failed: {response.status_code} {response.text}\")\n        else:\n            raise RuntimeError(f\"Chat request failed: {response.status_code} {error_text}\")\n\n    content_parts: List[str] = []\n    for raw_line in response.iter_lines(decode_unicode=True):\n        if not raw_line:\n            continue\n        line = raw_line.strip()\n        if not line.startswith(\"data:\"):\n            continue\n        data = line[5:].strip()\n        if data == \"[DONE]\":\n            break\n        try:\n            payload = json.loads(data)\n        except Exception:\n            continue\n        choices = payload.get(\"choices\") or []\n        if not choices:\n            continue\n        delta = choices[0].get(\"delta\") or {}\n        piece = delta.get(\"content\")\n        if not piece:\n            continue\n        content_parts.append(piece)\n        on_delta(piece)\n\n    return \"\".join(content_parts)\n\n\ndef decode_value(value: Any) -> Any:\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\", errors=\"ignore\")\n    return value\n\n\ndef parse_results(raw: List[Any]) -> List[Dict[str, Any]]:\n    results: List[Dict[str, Any]] = []\n    if not raw or len(raw) < 2:\n        return results\n\n    for idx in range(1, len(raw), 2):\n        if idx + 1 >= len(raw):\n            break\n        fields_raw = raw[idx + 1]\n        if not isinstance(fields_raw, list):\n            continue\n        field_map: Dict[str, Any] = {}\n        for i in range(0, len(fields_raw), 2):\n            key = decode_value(fields_raw[i])\n            value = decode_value(fields_raw[i + 1]) if i + 1 < len(fields_raw) else \"\"\n            field_map[str(key)] = value\n        results.append(field_map)\n    return results\n\n\n_QUERY_STOPWORDS = {\n    \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"into\", \"over\",\n    \"under\", \"after\", \"before\", \"were\", \"was\", \"are\", \"is\", \"its\", \"their\",\n    \"then\", \"than\", \"which\", \"when\", \"where\", \"have\", \"has\", \"had\", \"onto\",\n    \"upon\", \"your\", \"yours\", \"they\", \"them\", \"these\", \"those\", \"will\", \"would\",\n    \"could\", \"should\", \"about\", \"there\", \"here\", \"while\", \"what\", \"why\", \"how\",\n    \"not\", \"but\", \"you\", \"your\", \"our\", \"ours\", \"his\", \"her\", \"she\", \"him\",\n    \"also\", \"such\", \"been\", \"being\", \"out\", \"one\", \"two\", \"three\", \"four\",\n    \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"more\", \"most\", \"some\",\n    \"many\", \"few\", \"each\", \"per\", \"was\", \"were\", \"did\", \"does\", \"do\",\n}\n\n\ndef extract_keywords(query: str) -> List[str]:\n    raw_tokens = re.findall(r\"[A-Za-z0-9][A-Za-z0-9'\\\\-]{2,}\", query)\n    keywords: List[str] = []\n    for token in raw_tokens:\n        cleaned = re.sub(r\"[^A-Za-z0-9]\", \"\", token)\n        if not cleaned:\n            continue\n        lower = cleaned.lower()\n        if lower in _QUERY_STOPWORDS:\n            continue\n        if token[:1].isupper() or len(cleaned) >= 5:\n            keywords.append(lower)\n    seen = set()\n    ordered: List[str] = []\n    for token in keywords:\n        if token in seen:\n            continue\n        seen.add(token)\n        ordered.append(token)\n    return ordered\n\n\ndef run_lexical_search(\n    client: redis.Redis,\n    index: str,\n    keywords: List[str],\n    limit: int,\n) -> List[Dict[str, Any]]:\n    if not keywords or limit <= 0:\n        return []\n    tokens = [re.sub(r\"[^A-Za-z0-9]\", \"\", token) for token in keywords]\n    tokens = [token for token in tokens if token]\n    if not tokens:\n        return []\n    query = \"@text:(\" + \"|\".join(tokens) + \")\"\n    try:\n        raw = client.execute_command(\n            \"FT.SEARCH\",\n            index,\n            query,\n            \"LIMIT\",\n            \"0\",\n            str(limit),\n            \"RETURN\",\n            \"9\",\n            \"doc_id\",\n            \"chunk_id\",\n            \"attachment_key\",\n            \"source_pdf\",\n            \"page_start\",\n            \"page_end\",\n            \"section\",\n            \"text\",\n            \"score\",\n            \"DIALECT\",\n            \"2\",\n        )\n    except Exception:\n        return []\n    return parse_results(raw)\n\ndef is_content_chunk(chunk: Dict[str, Any]) -> bool:\n    text = chunk.get(\"text\", \"\")\n    if not text:\n        return False\n\n    # 1. Minimum length (filters title pages, citations)\n    if len(text) < 500:\n        return False\n\n    # 2. Must contain narrative sentences\n    # (bibliographies rarely have multiple full sentences)\n    if text.count(\". \") < 3:\n        return False\n\n    return True\n\ndef looks_narrative(text: str) -> bool:\n    if not text:\n        return False\n\n    # Must contain several complete sentences\n    if text.count(\". \") < 4:\n        return False\n\n    # Optional: avoid list-like text\n    if text.count(\"\\n\") > len(text) / 80:\n        return False\n\n    return True\n\ndef build_context(retrieved: List[Dict[str, Any]]) -> str:\n    blocks = []\n    for chunk in retrieved:\n        doc_id = chunk.get(\"doc_id\", \"\")\n        chunk_id = chunk.get(\"chunk_id\", \"\")\n        source_pdf = chunk.get(\"source_pdf\", \"\")\n        page_start = chunk.get(\"page_start\", \"\")\n        page_end = chunk.get(\"page_end\", \"\")\n        score = chunk.get(\"score\", \"\")\n        text = chunk.get(\"text\", \"\")\n        pages = f\"{page_start}-{page_end}\"\n        block = (\n            f\"<Document source='{source_pdf}' pages='{pages}' doc_id='{doc_id}' \"\n            f\"chunk_id='{chunk_id}' score='{score}'>\\n{text}\\n</Document>\"\n        )\n        blocks.append(block)\n    return \"\\n\\n\".join(blocks)\n\n\ndef load_history_messages(path: str) -> List[Dict[str, Any]]:\n    if not path:\n        return []\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as handle:\n            payload = json.load(handle)\n    except Exception:\n        return []\n    if isinstance(payload, list):\n        return [item for item in payload if isinstance(item, dict)]\n    messages = payload.get(\"messages\") if isinstance(payload, dict) else None\n    if isinstance(messages, list):\n        return [item for item in messages if isinstance(item, dict)]\n    return []\n\n\ndef format_history_block(messages: List[Dict[str, Any]]) -> str:\n    lines: List[str] = []\n    for message in messages:\n        role = str(message.get(\"role\", \"\")).strip().lower()\n        content = str(message.get(\"content\", \"\")).strip()\n        if not content:\n            continue\n        if role not in (\"user\", \"assistant\"):\n            role = \"user\"\n        label = \"User\" if role == \"user\" else \"Assistant\"\n        lines.append(f\"{label}: {content}\")\n    return \"\\n\".join(lines)\n\n\ndef extract_annotation_key(chunk_id: str) -> str:\n    if not chunk_id:\n        return \"\"\n    if \":\" in chunk_id:\n        chunk_id = chunk_id.split(\":\", 1)[1]\n    candidate = chunk_id.strip().upper()\n    if re.fullmatch(r\"[A-Z0-9]{8}\", candidate):\n        return candidate\n    return \"\"\n\n\ndef build_citations(retrieved: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    seen = set()\n    citations: List[Dict[str, Any]] = []\n    for chunk in retrieved:\n        doc_id = chunk.get(\"doc_id\", \"\")\n        chunk_id = chunk.get(\"chunk_id\", \"\")\n        attachment_key = chunk.get(\"attachment_key\", \"\")\n        page_start = chunk.get(\"page_start\", \"\")\n        page_end = chunk.get(\"page_end\", \"\")\n        source_pdf = chunk.get(\"source_pdf\", \"\")\n        key = (doc_id, attachment_key, page_start, page_end, source_pdf)\n        if key in seen:\n            continue\n        seen.add(key)\n        annotation_key = extract_annotation_key(str(chunk_id))\n        citations.append({\n            \"doc_id\": doc_id,\n            \"chunk_id\": chunk_id,\n            \"attachment_key\": attachment_key,\n            \"annotation_key\": annotation_key or None,\n            \"page_start\": page_start,\n            \"page_end\": page_end,\n            \"pages\": f\"{page_start}-{page_end}\",\n            \"source_pdf\": source_pdf,\n        })\n    return citations\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Query RedisSearch and answer with RAG.\")\n    parser.add_argument(\"--query\", required=True)\n    parser.add_argument(\"--k\", type=int, default=10)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--chat-base-url\", required=True)\n    parser.add_argument(\"--chat-api-key\", default=\"\")\n    parser.add_argument(\"--chat-model\", required=True)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--stream\", action=\"store_true\")\n    parser.add_argument(\"--history-file\", help=\"Optional JSON file with recent chat history\")\n    args = parser.parse_args()\n\n    try:\n        embedding = request_embedding(args.embed_base_url, args.embed_api_key, args.embed_model, args.query)\n        if len(embedding) != 768:\n            raise RuntimeError(f\"Embedding dim mismatch: {len(embedding)}\")\n        embedding = normalize_vector(embedding)\n        vec = vector_to_bytes(embedding)\n    except Exception as exc:\n        eprint(f\"Failed to embed query: {exc}\")\n        return 2\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n\n    try:\n        raw = client.execute_command(\n            \"FT.SEARCH\",\n            args.index,\n            f\"*=>[KNN {args.k} @embedding $vec AS score]\",\n            \"PARAMS\",\n            \"2\",\n            \"vec\",\n            vec,\n            \"SORTBY\",\n            \"score\",\n            \"RETURN\",\n            \"9\",\n            \"doc_id\",\n            \"chunk_id\",\n            \"attachment_key\",\n            \"source_pdf\",\n            \"page_start\",\n            \"page_end\",\n            \"section\",\n            \"text\",\n            \"score\",\n            \"DIALECT\",\n            \"2\",\n        )\n    except Exception as exc:\n        eprint(f\"RedisSearch query failed: {exc}\")\n        return 2\n\n    retrieved = parse_results(raw)\n\n    # merge lexical results (unchanged)\n    keywords = extract_keywords(args.query)\n    lexical_limit = max(args.k, 5)\n    lexical_results = run_lexical_search(client, args.index, keywords, lexical_limit)\n    if lexical_results:\n        seen = {item.get(\"chunk_id\") for item in retrieved if item.get(\"chunk_id\")}\n        for item in lexical_results:\n            chunk_id = item.get(\"chunk_id\")\n            if not chunk_id or chunk_id in seen:\n                continue\n            retrieved.append(item)\n            seen.add(chunk_id)\n\n        max_total = args.k + lexical_limit\n        if len(retrieved) > max_total:\n            retrieved = retrieved[:max_total]\n\n    # Strict filter\n    filtered = [\n        c for c in retrieved\n        if is_content_chunk(c) and looks_narrative(c.get(\"text\", \"\"))\n    ]\n\n    # Fallback: if too strict, keep \"contentful\" chunks at least (from ORIGINAL retrieved)\n    if not filtered:\n        filtered = [c for c in retrieved if is_content_chunk(c)]\n\n    retrieved = filtered\n\n    context = build_context(retrieved)\n\n    system_prompt = (\n        \"Use ONLY the provided context for factual claims. If insufficient, say you do not know. \"\n        \"Chat history is only for conversational continuity or for providing concepts to be retrieved. \"\n        \"Add inline citations using this exact format: [[cite:DOC_ID:PAGE_START-PAGE_END]]. \"\n        \"Example: ... [[cite:ABC123:12-13]].\"\n    )\n    history_messages = load_history_messages(args.history_file) if args.history_file else []\n    history_block = format_history_block(history_messages)\n    if history_block:\n        history_block = f\"Chat history (for reference only):\\n{history_block}\\n\\n\"\n    user_prompt = f\"{history_block}Question: {args.query}\\n\\nContext:\\n{context}\"\n\n    citations = build_citations(retrieved)\n\n    answer = \"\"\n    streamed = False\n    if args.stream:\n        def emit(obj: Dict[str, Any]) -> None:\n            print(json.dumps(obj, ensure_ascii=False), flush=True)\n\n        try:\n            answer = request_chat_stream(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                args.temperature,\n                system_prompt,\n                user_prompt,\n                lambda chunk: emit({\"type\": \"delta\", \"content\": chunk}),\n            )\n            streamed = True\n        except Exception as exc:\n            if is_stream_unsupported(str(exc)):\n                streamed = False\n            else:\n                eprint(f\"Chat request failed: {exc}\")\n                return 2\n\n    if not streamed:\n        try:\n            answer = request_chat(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                args.temperature,\n                system_prompt,\n                user_prompt,\n            )\n        except Exception as exc:\n            eprint(f\"Chat request failed: {exc}\")\n            return 2\n\n    output = {\n        \"query\": args.query,\n        \"answer\": answer,\n        \"citations\": citations,\n        \"retrieved\": retrieved,\n    }\n\n    if args.stream and streamed:\n        print(json.dumps({\"type\": \"final\", **output}, ensure_ascii=False), flush=True)\n    else:\n        print(json.dumps(output, ensure_ascii=False))\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "batch_index_pyzotero.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.3\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Set\n\nfrom pyzotero import zotero\nfrom tqdm import tqdm\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef ensure_dir(path: Path) -> None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef load_checkpoint(path: Path) -> Set[str]:\n    if not path.exists():\n        return set()\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        items = data.get(\"processed\", [])\n        return set(str(x) for x in items)\n    except Exception:\n        return set()\n\n\ndef save_checkpoint(path: Path, processed: Set[str]) -> None:\n    payload = {\"processed\": sorted(processed)}\n    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n\n\ndef run_script(script: Path, args: List[str]) -> None:\n    command = [sys.executable, str(script)] + args\n    result = subprocess.run(command, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise RuntimeError(result.stderr.strip() or f\"Command failed: {' '.join(command)}\")\n\n\ndef fetch_parent_item(client: zotero.Zotero, parent_key: str) -> Dict[str, Any]:\n    try:\n        item = client.item(parent_key)\n        if isinstance(item, list):\n            return item[0] if item else {}\n        return item\n    except Exception:\n        return {}\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Batch index a Zotero library with Docling and RedisSearch.\")\n    parser.add_argument(\"--library-id\", required=True)\n    parser.add_argument(\"--library-type\", choices=[\"user\", \"group\"], required=True)\n    parser.add_argument(\"--api-key\", required=True)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--out-dir\", default=\"./data\")\n    parser.add_argument(\"--ocr\", choices=[\"auto\", \"force\", \"off\"], default=\"auto\")\n    parser.add_argument(\"--chunking\", choices=[\"page\", \"section\"], default=\"page\")\n    parser.add_argument(\"--limit\", type=int)\n    parser.add_argument(\"--since\", type=int)\n    parser.add_argument(\"--reindex\", action=\"store_true\")\n    args = parser.parse_args()\n\n    out_dir = Path(args.out_dir).resolve()\n    pdf_dir = out_dir / \"pdfs\"\n    item_dir = out_dir / \"items\"\n    doc_dir = out_dir / \"docs\"\n    chunk_dir = out_dir / \"chunks\"\n    checkpoint_path = out_dir / \"checkpoint.json\"\n\n    for folder in (pdf_dir, item_dir, doc_dir, chunk_dir):\n        ensure_dir(folder)\n\n    processed = set() if args.reindex else load_checkpoint(checkpoint_path)\n\n    client = zotero.Zotero(args.library_id, args.library_type, args.api_key)\n\n    params: Dict[str, Any] = {\"itemType\": \"attachment\"}\n    if args.limit:\n        params[\"limit\"] = args.limit\n    if args.since:\n        params[\"since\"] = args.since\n\n    try:\n        attachments = client.everything(client.items(**params))\n    except Exception as exc:\n        eprint(f\"Failed to fetch Zotero items: {exc}\")\n        return 2\n\n    pdf_items = []\n    for item in attachments:\n        data = item.get(\"data\", {})\n        content_type = data.get(\"contentType\", \"\") or \"\"\n        if content_type.startswith(\"application/pdf\"):\n            pdf_items.append(item)\n\n    script_dir = Path(__file__).resolve().parent\n    docling_script = script_dir / \"docling_extract.py\"\n    index_script = script_dir / \"index_redisearch.py\"\n\n    errors: List[str] = []\n\n    for item in tqdm(pdf_items, desc=\"Indexing PDFs\"):\n        attachment_key = item.get(\"key\")\n        if not attachment_key:\n            continue\n        parent_key = item.get(\"data\", {}).get(\"parentItem\")\n        doc_id = parent_key or attachment_key\n\n        if doc_id in processed:\n            continue\n\n        pdf_path = pdf_dir / f\"{attachment_key}.pdf\"\n        item_path = item_dir / f\"{doc_id}.json\"\n        doc_path = doc_dir / f\"{doc_id}.md\"\n        chunk_path = chunk_dir / f\"{doc_id}.json\"\n\n        try:\n            content = client.file(attachment_key)\n            if not content:\n                raise RuntimeError(\"Empty PDF content\")\n            pdf_path.write_bytes(content)\n        except Exception as exc:\n            errors.append(f\"{doc_id}: download failed ({exc})\")\n            continue\n\n        try:\n            metadata = fetch_parent_item(client, parent_key) if parent_key else item\n            item_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n        except Exception as exc:\n            errors.append(f\"{doc_id}: metadata write failed ({exc})\")\n            continue\n\n        try:\n            run_script(\n                docling_script,\n                [\n                    \"--pdf\",\n                    str(pdf_path),\n                    \"--doc-id\",\n                    doc_id,\n                    \"--out-json\",\n                    str(chunk_path),\n                    \"--out-md\",\n                    str(doc_path),\n                    \"--chunking\",\n                    args.chunking,\n                    \"--ocr\",\n                    args.ocr,\n                ],\n            )\n        except Exception as exc:\n            errors.append(f\"{doc_id}: docling failed ({exc})\")\n            continue\n\n        try:\n            run_script(\n                index_script,\n                [\n                    \"--chunks-json\",\n                    str(chunk_path),\n                    \"--redis-url\",\n                    args.redis_url,\n                    \"--index\",\n                    args.index,\n                    \"--prefix\",\n                    args.prefix,\n                    \"--embed-base-url\",\n                    args.embed_base_url,\n                    \"--embed-api-key\",\n                    args.embed_api_key,\n                    \"--embed-model\",\n                    args.embed_model,\n                ],\n            )\n        except Exception as exc:\n            errors.append(f\"{doc_id}: redis index failed ({exc})\")\n            continue\n\n        processed.add(doc_id)\n        save_checkpoint(checkpoint_path, processed)\n\n    if errors:\n        eprint(\"Failures:\")\n        for entry in errors:\n            eprint(f\"- {entry}\")\n\n    eprint(f\"Processed {len(processed)} items. Errors: {len(errors)}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "utils_embedding.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.2.3\nimport math\nimport struct\nimport requests\nfrom typing import List\n\ndef normalize_vector(values: List[float]) -> List[float]:\n    norm = math.sqrt(sum(v * v for v in values))\n    if norm == 0:\n        return values\n    return [v / norm for v in values]\n\ndef vector_to_bytes(values: List[float]) -> bytes:\n    return struct.pack(\"<\" + \"f\" * len(values), *values)\n\ndef request_embedding(base_url: str, api_key: str, model: str, text: str) -> List[float]:\n    url = base_url.rstrip(\"/\") + \"/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n    response = requests.post(url, json={\"input\": text, \"model\": model}, headers=headers, timeout=120)\n    if response.status_code >= 400:\n        raise RuntimeError(f\"Embedding request failed: {response.status_code} {response.text}\")\n    payload = response.json()\n    data = payload.get(\"data\")\n    if not data:\n        raise RuntimeError(\"Embedding response missing data field\")\n    embedding = data[0].get(\"embedding\")\n    if not embedding:\n        raise RuntimeError(\"Embedding response missing embedding\")\n    return [float(x) for x in embedding]\n",
  "ocr_wordlist.txt": "# zotero-redisearch-rag tool version: 0.2.3\naai\naam\nabb\nabge\nabr\nabsol\nabteilungs\nacad\nacc\nacco\naccu\nackley\nacn\nacp\nactu\naddie\nadolphus\nadress\naf\nafew\naff\nafton\nagarwal\nagonized\nagonizing\nagrar\nagt\nagu\nahh\nahram\naicha\naigner\naip\naire\naj\nake\naken\nala\nalanus\nalbeck\nalbers\nalbornoz\nald\nalda\naleksandra\nalfaro\nalibaba\nalittle\nalla\nallard\nalli\nallin\nallman\nallright\nalongwith\nalonso\name\namelie\nameri\namg\namityville\namr\nams\nanalyze\nanalyzed\nanalyzing\nanan\nanc\nanca\nance\nande\nander\nandersonville\nandr\nandra\nandrae\nandrus\nands\nandthe\nane\nanent\nange\nangelis\nani\nania\nanish\nanneke\nanni\nannis\nantone\nantti\nanz\nao\napac\napel\naph\napl\napologize\nappl\napplegate\naq\narbei\narbeits\narcheological\narchi\nari\naris\narmando\narmor\narri\narrowsmith\nartem\nartes\narti\narvid\narxiv\nasan\naschauer\nase\nasi\naskin\naspx\nassis\nasso\nassoc\nastro\naswell\nater\natk\natla\natlan\natleast\natmos\nats\natt\natta\natten\natthe\naubry\naufge\naufl\naul\naun\naurore\nausge\nauskunfts\nauss\nauthorization\nauthorized\nauthorizing\navas\navg\navo\naw\naways\nawi\nax\nay\nayres\naz\nazhar\nbab\nbadgley\nbagwell\nbaily\nbains\nbal\nbalaji\nballston\nbama\nbanerjee\nbanz\nbarger\nbarnaby\nbaro\nbarret\nbascom\nbatchelor\nbayless\nbayne\nbaynes\nbayo\nbbe\nbeall\nbeaty\nbeca\nbeckedahl\nbeetz\nbefor\nbegleit\nbehavior\nbehavioral\nbehaviors\nbeit\nbek\nbelcher\nbellum\nbelter\nbeltran\nbemis\nbene\nbengt\nbenj\nbepreisung\nbera\nberatungs\nbergemann\nbernal\nbero\nberr\nberryman\nberthier\nbertin\nbertuch\nbesse\nbestof\nbeteiligungs\nbetz\nbeuth\nbewertungs\nbge\nbiblio\nbibliotheks\nbice\nbie\nbil\nbildungs\nbina\nbir\nbirney\nbirte\nbisbee\nbischoff\nbissell\nbiswas\nbitkom\nbitt\nbjorn\nbjörk\nblacklight\nblackwall\nblaisdell\nblakeley\nblakely\nble\nbles\nblinn\nblogspot\nblowed\nblu\nbmi\nbmwi\nboe\nboehm\nboggs\nbogue\nboj\nbol\nboland\nboldt\nboller\nboney\nbonino\nborchers\nboren\nborrego\nborrmann\nbos\nbosman\nboulanger\nboult\nbourdieu\nboysen\nbpa\nbrabeck\nbracher\nbrammer\nbrashear\nbreck\nbreuning\nbreyer\nbridgeman\nbridgette\nbrien\nbrin\nbrinker\nbriony\nbris\nbriscoe\nbrockmann\nbrok\nbrost\nbrubaker\nbrunner\nbruns\nbsi\nbu\nbuchner\nbudrich\nbui\nbuildin\nbuildup\nbuisson\nbul\nbungen\nbunn\nburchardt\nburdett\nburks\nburling\nbusi\nböhmer\nböker\nbühren\nbür\nbüttner\ncabeza\ncade\ncady\ncai\ncali\ncalif\ncallender\ncallie\ncally\ncampania\ncanan\ncandor\ncaney\ncantrell\ncao\ncapi\ncaplan\ncapt\ncaput\ncarell\ncaribe\ncarmack\ncaron\ncarondelet\ncarpathia\ncarrasco\ncarrillo\ncassel\ncassell\ncastell\ncastellanos\ncastelli\ncastleman\ncatalog\ncataloging\ncatalogs\ncate\ncategorization\ncategorize\ncategorized\ncates\ncau\ncaus\ncavalli\ncavanaugh\ncci\ncco\ncdi\ncec\ncecilie\nced\ncedrik\ncele\ncelo\ncentered\ncentering\ncenterline\ncentern\ncenterpiece\ncentimeters\ncentralized\ncept\ncera\ncerf\nchai\nchampollion\nchancellorsville\nchantel\nchao\nchapelle\nchappell\ncharacterize\ncharacterized\ncharacterizes\ncharacterizing\nchas\nchawla\nched\nchien\nchil\nchilds\nchim\nchiseled\nchristof\nchristophe\nchua\nchul\nchun\ncil\ncin\ncio\ncios\ncip\ncit\nciv\ncivilized\ncked\ncken\nclamor\nclarins\nclaussen\ncle\nclearinghouse\nclemons\ncler\nclu\ncof\ncoinbase\ncolla\ncolo\ncolonized\ncolor\ncolored\ncolorful\ncolors\ncolvin\ncolwell\ncom\ncommis\ncommu\ncommun\ncommuni\ncompl\nconant\nconceptualization\ncondi\nconf\nconfed\nconn\nconsilium\nconst\nconstans\ncontro\ncookson\ncoons\ncoord\ncordis\ncormack\ncorp\ncorrado\ncorre\ncorte\ncortez\ncostas\ncouldn\ncoun\ncour\ncourant\ncov\ncowles\ncrain\ncre\ncrea\ncremer\ncrippen\ncris\ncriticize\ncriticized\ncroom\ncros\ncrue\ncta\ncullum\nculp\ncunard\ncuno\ncupp\ncurr\ncurtin\ncus\ncutoff\ndae\ndage\ndai\ndailey\ndak\ndalit\ndalla\ndani\ndarko\ndarlin\ndarrow\ndau\ndavey\ndayal\nddi\ndecentralized\ndeepwater\ndefense\ndefenses\ndefi\ndelisle\ndelt\ndemeanor\ndemobilization\ndemocratization\ndemocratizing\ndende\ndene\ndenney\ndennison\ndeppe\ndept\ndesy\ndeve\ndevel\ndhar\ndiarrhea\ndickel\ndidi\ndidn\ndier\ndierkes\ndietze\ndif\ndigi\ndigitalization\ndigitization\ndigitize\ndigitized\ndil\ndiller\ndinan\ndinh\ndini\ndipl\ndirec\ndiw\ndle\ndoa\ndoan\ndocent\ndocu\ndoesn\ndois\ndol\ndominika\ndonatella\ndonelson\ndor\ndorman\ndors\ndorsett\ndoty\ndou\ndowell\ndowntown\ndoz\ndra\ndragan\ndrago\ndred\ndren\ndrescher\ndressler\ndreyer\ndri\ndrumm\ndrydock\ndsgvo\ndte\ndto\nduce\nduquesne\ndurin\ndurkin\neac\nead\neadie\neam\nearle\nearnshaw\neastport\nebd\nebe\nebel\neberl\nebi\nebru\necl\neco\necon\neda\nedc\nedi\nedu\neep\neer\neero\neet\nef\neffi\nefl\neggenstein\negovernment\nehem\nehr\neickhoff\neinge\neini\neir\nek\neldridge\nele\nelearning\nelec\nelectrolytic\nelek\nelevator\neley\nelihu\nelina\nelkin\neln\nels\neman\nemelia\nemer\nemerick\nemilie\nemmons\nemp\nemphasize\nemphasized\nemphasizes\nemphasizing\nena\nenb\nence\nendeavor\nendeavored\nendeavoring\nendeavors\nene\nenes\nengelhardt\nengi\nengl\nengle\nengr\nenke\nenos\nenrollment\nent\nents\nentstehungs\nentwicklungs\nenwg\neos\nepi\nepicenter\nepub\nequaled\nerc\nerd\nerfahrungs\nerick\nern\nert\nertl\nerw\nery\neso\nesq\ness\nesta\nestab\netc\nete\netl\netta\nette\neurop\neuropaea\neuropeana\nevi\nevtl\new\newr\nexc\nexpe\nexper\nexperi\ney\nez\neze\nfabienne\nfabio\nfabricius\nfabrizio\nfaelle\nfairbank\nfal\nfam\nfami\nfannie\nfarb\nfarida\nfarrar\nfarris\nfaruk\nfau\nfavor\nfavorable\nfavorably\nfavored\nfavorite\nfavorites\nfavors\nfechner\nfect\nfel\nfellner\nfennell\nfiberglass\nfid\nfidler\nfied\nfif\nfilippo\nfilson\nfinalized\nfinke\nfinkle\nfiz\nfla\nflaherty\nflam\nflavor\nfo\nfoltz\nfom\nfon\nforde\nformalized\nfors\nforschungs\nforthe\nfos\nfournier\nfrac\nfrantz\nfranzen\nfrasier\nfre\nfrede\nfsu\nfte\nfue\nfueled\nfueling\nfuer\nful\nfulfill\nfulfillment\nfung\nfurth\nfyfe\nfä\nför\nfüh\ngabbert\ngah\ngaller\ngalvanized\ngan\ngangen\ngannon\ngant\ngaray\ngarber\ngart\ngass\ngassmann\ngaynor\ngebauer\ngebhart\ngeddy\ngeert\ngehostet\ngend\ngener\ngeneralize\ngeneralized\ngennady\ngeoportal\ngeorgen\ngeorgy\ngerdes\ngerstner\ngetz\ngfa\nghosh\ngia\ngie\ngien\ngiga\ngillam\ngillen\ngini\nginn\ngivens\nglaeser\nglamor\nglaucus\ngle\nglei\ngleim\nglenwood\ngoble\ngoll\ngonzalo\ngoodell\ngoodspeed\ngorman\ngov\ngove\ngover\ngovt\ngow\ngoyal\ngra\ngradl\ngrandy\ngraßhoff\ngree\ngreenlee\ngress\ngrethe\ngriebel\ngris\ngro\ngroessten\ngroth\ngrubbs\ngrueling\ngsi\nguage\nguid\ngundlach\ngung\nguo\ngustin\ngutach\ngutknecht\ngvo\ngötting\ngünzel\nhaa\nhadad\nhadn\nhaight\nhalleck\nhalliday\nhamblin\nhammonds\nhandlungs\nhanlon\nhanni\nhanser\nhao\nhar\nharari\nharbors\nharen\nharland\nharmonia\nharpercollins\nharrassowitz\nhartig\nhartung\nhaslinger\nhasn\nhatteras\nhausers\nhav\nhavard\nhavemann\nhawken\nhayashi\nhayman\nhazzard\nhedlund\nhedrick\nhee\nheesen\nheidrich\nheinke\nheinzel\nheise\nheit\nhel\nhelbig\nhelbing\nhennessy\nhenrich\nhenrike\nherchen\nhermione\nherron\nhewes\nheyde\nhickox\nhig\nhight\nhildreth\nhillmann\nhinde\nhinman\nhinze\nhippel\nhippler\nhir\nhirt\nhisto\nhistor\nhite\nhoffmeister\nhoge\nhogrefe\nhollenbeck\nholliday\nholston\nholzer\nhom\nhoman\nhomeoffice\nhon\nhonor\nhonorably\nhonored\nhonoring\nhonors\nhoppe\nhoppin\nhor\nhoran\nhori\nhornbostel\nhorstmann\nhoskins\nhospitalization\nhospitalized\nhouten\nhowards\nhre\nhu\nhua\nhubbell\nhulbert\nhuma\nhumm\nhungen\nhup\nhur\nhusted\nhvac\nhöck\nhübner\nhülsmann\niai\niam\niana\niat\nib\nibero\nibi\nibs\nica\nican\nico\nicr\nics\nict\nident\nidf\nidi\nidl\nidlewild\niel\nife\nifyou\nigd\night\nigi\nign\nih\nihave\nihe\nij\nik\nikt\nil\nilene\nilie\nille\nilli\nillus\nils\nime\nimma\nimmortalized\nimpor\nimpro\nimt\ninan\nincase\nincl\ninclud\nindi\nindustrialization\ninfact\ninfor\ninforma\ningen\ningraham\ninhouse\ninit\ninjun\ninkl\ninl\ninnis\ninno\ninnova\ninsbes\ninslee\ninso\ninsp\ninstill\ninte\ninteragency\ninteres\ninterhyp\ninthe\nione\nior\nious\nipcc\nipp\niro\nirt\nisadore\nisc\nisin\nisla\nismay\nisn\nison\nisu\nita\nite\nithink\nitis\nity\niven\niwas\niwill\nized\njaap\njabez\njahnke\njama\njamison\njanna\njanney\njano\njantzen\njarrett\njas\njayne\njenn\njeopardize\njeopardized\njesper\njessika\njewell\njewelry\njewett\nji\njian\njie\njif\njillian\njin\njobe\njochum\njohne\njol\njolley\njoost\njopp\njordon\njos\njour\njourdan\njugg\njusti\njuventa\njyoti\njörn\nkad\nkaden\nkag\nkalman\nkaminsky\nkan\nkannt\nkaran\nkarina\nkarolin\nkarsch\nkas\nkatarzyna\nkaupp\nkawa\nkeeble\nkees\nkei\nkeiser\nkeit\nkeiten\nkelli\nkeo\nket\nketcham\nkhalsa\nkhanna\nki\nkii\nkiley\nkilometers\nkimber\nkirstie\nkis\nkiva\nklei\nkli\nkmu\nkno\nknopp\nknowl\nkoeln\nkok\nkom\nkommer\nkommis\nkommunikations\nkon\nkonstantinos\nkonstanze\nkontroll\nkonzentrations\nkoo\nkoon\nkoontz\nkoordinations\nkor\nkosel\nkpi\nkrahn\nkramm\nkrems\nkretz\nkreutzer\nkrogh\nkröger\nkröner\nkuehn\nkug\nkuk\nkul\nkun\nkura\nkwon\nkyiv\nkämper\nkön\nkönigshausen\nkönn\nköster\nlaban\nlabeled\nlabeling\nlabored\nlaborers\nlaboring\nlada\nlaf\nlafferty\nlai\nlaidlaw\nlal\nlamartine\nlames\nlamy\nlandi\nlandin\nlapointe\nlar\nlastig\nlatif\nlauber\nlaughlin\nlaun\nlda\nleaderboard\nlechler\nleclair\nleed\nleggett\nlegrand\nlehnert\nleit\nleitch\nleitungs\nlem\nlemaire\nlemay\nlemuel\nlenka\nleopoldina\nler\nlern\nletty\nleuze\nleveled\nleveln\nlevent\nlewandowski\nlhe\nlibri\nlibris\nlic\nlich\nliche\nlier\nligue\nlile\nlim\nlindell\nlinne\nlis\nlite\nlitera\nliv\nlle\nlmu\nloa\nloc\nlocalized\nlod\nloewe\nlofton\nloh\nloi\nlon\nlond\nlongtime\nlor\nloran\nlorena\nloring\nloui\nlous\nlovis\nlowden\nlowenthal\nlowrey\nlsa\nlse\nlta\nluc\nlucke\nlue\nluella\nluiz\nlum\nlus\nlusk\nluttrell\nlytle\nlän\nlö\nlöser\nmaass\nmachin\nmadita\nmaes\nmagni\nmaher\nmahmood\nmaitland\nmaj\nmak\nmakin\nmalin\nmals\nmam\nmanas\nmand\nmander\nmaneuver\nmaneuverability\nmaneuverable\nmaneuvered\nmans\nmarah\nmarginalized\nmari\nmarjan\nmarleen\nmartialed\nmartius\nmartyn\nmarveled\nmarvelous\nmaryann\nmasi\nmassie\nmasur\nmatchen\nmateus\nmathers\nmatias\nmatth\nmattison\nmaximize\nmayr\nmaysville\nmbi\nmbo\nmcadoo\nmcclanahan\nmcclelland\nmccown\nmccurdy\nmccutcheon\nmcfall\nmcginnis\nmcginty\nmcgrady\nmckeen\nmckelvey\nmckenney\nmclaurin\nmclellan\nmcloughlin\nmcmillen\nmcnabb\nmcneal\nmcnulty\nmcphail\nmcvey\nmeager\nmeas\nmechanicsburg\nmedi\nmei\nmeinel\nmeisel\nmell\nmemorialize\nmemorialized\nmende\nmense\nment\nments\nmerc\nmerce\nmerrifield\nmerriman\nmetcalf\nmeuser\nmex\nmga\nmichener\nmichi\nmie\nmil\nmili\nmilitar\nmilitarization\nmillar\nmillersville\nmilliken\nmindest\nminimize\nminimized\nminimizing\nminn\nmio\nmip\nmis\nmitscherlich\nmittermaier\nmobilitäts\nmodeler\nmodelers\nmodeling\nmodernization\nmoglich\nmohican\nmohler\nmolded\nmolloy\nmom\nmoma\nmonopolize\nmontauk\nmony\nmooc\nmoocs\nmor\nmowry\nmoxley\nmpi\nmsa\nmuenchen\nmunday\nmunsey\nmusser\nmög\nmünch\nnace\nnachdr\nnad\nnade\nnadeau\nnahme\nnal\nnang\nnapo\nnapp\nnath\nnati\nnatio\nnatu\nnaujoks\nnauvoo\nnaveen\nncbi\nnce\nneb\nnederlandse\nneer\nneff\nneher\nneighbor\nneighborhood\nneighboring\nnel\nnelles\nneto\nnevins\nnewhouse\nnewyork\nnex\nney\nnger\nnickerson\nnida\nnien\nnijmegen\nnikolay\nnikos\nnin\nnir\nnis\nnisha\nnisse\nnoe\nnom\nnomos\nnoncompliance\nnonexistent\nnormalized\nnorthrup\nnos\nnott\nnotz\nnoy\nnse\nnum\nnung\nnutt\nnuttig\nnutz\nnutzungs\nnuys\nnwo\nobj\nobs\noc\nocc\noccurence\noclock\nocto\nodebrecht\nodo\nodr\nodum\noellers\nofa\nofcourse\noffe\noffense\noffi\noffnen\noffs\nofhis\nofi\noftentimes\nofthe\nofthis\nogc\nohi\nohn\nois\nokey\nol\noli\nolmstead\nome\nona\nond\nonetime\nonl\nons\nonthe\noo\nood\noor\nopac\nopensource\nopenstack\nopi\nopr\noptimized\noram\norde\norga\norgani\norganization\norganizational\norganizations\norganize\norganized\norganizer\norganizers\norganizing\nori\norigi\nork\norl\norn\nors\nosf\nosm\nosswald\nothe\nou\nould\noup\nous\nouse\nov\nowers\nowncloud\nows\noxley\noya\noßwald\npaal\npagano\nparkhurst\nparkman\nparlors\nparrish\nparte\npasquale\npatronized\npatsey\npatta\npau\npauer\npauley\npaulina\npauly\npavillion\npawlik\npekka\npembina\npenalize\npendergast\npeo\npeop\npepe\npepin\npernambuco\nperrys\nperso\npersson\npersönlichkeits\npetsch\npez\nphe\nphila\nphilippa\nphilo\nphineas\nphong\npietsch\npii\npil\npinus\npis\npitts\npizarro\npla\nplagiarized\nplaine\nplanungs\nple\npleasants\nples\npling\nplos\nplow\nplowed\nplowing\nplows\nplugins\npnas\npoc\npoindexter\npoli\npolit\npoliti\npom\npon\npopularizing\npor\nposi\nposix\npotomac\npotosi\npotthoff\npowe\npra\nprabhakar\nprac\npractica\npracticed\npracticing\npraeger\nprather\npresi\npressurized\nprewar\npri\nprin\nprioritize\nprized\nprob\nproblema\nproc\nprofesional\nproj\npron\nproquest\nprot\nproto\nprov\npruitt\npryor\npubl\npublicized\npubmed\npurdue\npuschmann\nputtin\npöschl\nqian\nqu\nquali\nqualitäts\nquan\nque\nquel\nques\nquitman\nraddatz\nrade\nradhakrishnan\nradke\nradtke\nragan\nraghavan\nragsdale\nraju\nral\nrall\nrapha\nrapp\nrass\nrauber\nravenswood\nrawlins\nrda\nrealization\nrealize\nrealized\nrealizing\nrebekah\nreco\nrecognize\nrecognized\nrecognizing\nredaktionsteam\nredman\nredstone\nrefueled\nrefueling\nregener\nregi\nregner\nreichmann\nreimer\nreits\nrekt\nrela\nreli\nrell\nremodeled\nrenz\nrepl\nresi\nreso\nresourcen\nressources\nreto\nretz\nrevista\nrevo\nrevolutionized\nria\nric\nridgely\nrieck\nrien\nrigh\nrigor\nrijksmuseum\nrin\nris\nrisi\nriv\nriviere\nro\nrocca\nroddy\nrodolphe\nrohit\nrohrer\nrol\nroo\nroommate\nroon\nror\nrosanne\nrosenblum\nrowboat\nrse\nrubenstein\nrud\nrumors\nrumsey\nrungen\nruppert\nrylan\nryman\nrösch\nröttgen\nrück\nrülke\nrümelin\nsaas\nsach\nsachverständigenrates\nsacri\nsafford\nsager\nsahr\nsall\nsaml\nsammen\nsamu\nsandiego\nsandro\nsandt\nsani\nsanju\nsanna\nsanz\nsaro\nsaur\nsavin\nsavior\nsaylor\nsbe\nschachtner\nschaffer\nsche\nschefer\nschen\nschenck\nschland\nschlitzer\nschnepf\nscholze\nschoolcraft\nschrade\nschu\nschul\nschultes\nschulungs\nschwandt\nschäffler\nschönberger\nsci\nscientifique\nscopus\nscrutinized\nseco\nseits\nseize\nseized\nseizing\nsel\nsella\nseng\nsenger\nsengupta\nsens\nseq\nseria\nsert\nserv\nservi\nsess\nsev\nseve\nsevera\nsey\nshal\nshalini\nshan\nshar\nshaul\nsheed\nshel\nshen\nsheng\nsher\nsherrod\nshing\nsho\nshoaib\nshotwell\nshoup\nshreve\nshu\nshukla\nshuler\nshultz\nsibel\nsiche\nsicherheits\nsidewalk\nsiebeck\nsiebold\nsightlines\nsigna\nsignaled\nsignaling\nsil\nsiler\nsimonds\nsinha\nsiri\nsizable\nskaggs\nskepticism\nskeptics\nskillful\nslaven\nslaw\nsle\nsma\nsmashwords\nsme\nsmit\nsmits\nsmok\nsnelling\nsobre\nsoc\nsoep\nsoftwares\nsom\nsommerville\nsoren\nsota\nsoto\nsouthwesterly\nsowell\nsozio\nspaulding\nspeci\nspecialization\nspecialize\nspecialized\nspecialty\nspect\nspei\nspiekermann\nspiers\nsplendor\nsprech\nspurlock\nsru\nsta\nstaden\nstandardization\nstandardized\nstanek\nstansbury\nstarck\nstarnes\nstata\nstatista\nstaton\nstavros\nstegemann\nsteinke\nstel\nstellv\nstephane\nster\nsteyer\nstillman\nstimson\nsto\nstoll\nstoppin\nstor\nstra\nstraightaway\nstrate\nstreck\nstreeter\nstrother\nstruct\nstu\nstuckey\nsturges\nsturtevant\nsua\nsuc\nsuccor\nsuf\nsug\nsugimoto\nsuhr\nsui\nsul\nsuleman\nsummarization\nsummarize\nsummarized\nsummarizes\nsummarizing\nsupe\nsupp\nsupt\nsur\nsus\nsut\nsuzanna\nswantje\nsympathize\nsympathizers\nsystematized\nsöllner\nsönke\ntaggart\ntak\ntakano\ntakeda\ntaliaferro\ntalmadge\ntamir\ntamu\ntana\ntann\ntant\ntappan\ntarver\ntas\ntasso\ntaubert\ntbe\nteague\ntechn\ntei\nteichert\ntelekommunikations\ntenn\ntera\ntert\ntesti\ntha\nthacker\nthanos\nther\nthetis\nthi\nthia\nthibodeau\nthie\nthiel\nthiemann\ntho\nthoma\nthomaston\nthornburg\nthos\nthueringen\nthurber\ntice\ntidwell\ntiefergehende\ntien\ntig\ntige\ntigt\ntil\ntilson\ntion\ntions\ntis\ntite\ntitty\ntivo\ntiwari\ntke\ntla\ntle\ntna\ntober\ntoda\ntol\ntolbert\ntomasz\ntotaled\ntotaling\ntothe\ntotten\ntoussaint\ntowa\ntowson\ntra\ntradeoffs\ntral\ntraumatized\ntrav\ntraveled\ntraveler\ntravelers\ntraveling\ntre\ntremont\ntren\ntri\ntrib\ntrinh\ntro\ntru\ntrum\ntröger\ntsukuba\ntte\ntubbs\ntudo\ntung\nture\nturen\ntuscarora\ntwen\ntwente\ntwigg\ntylers\ntät\nua\nual\nub\nubc\nuber\nuc\nucd\nueber\nuel\nuhl\nuia\nuld\nuli\null\numb\nume\numg\nunderhill\nunderrepresented\nunfavorable\nuniv\nuniversidad\nuniversitaet\nuniversitäts\nunterneh\nunterstützungs\nusin\nusu\nusw\nutilization\nutilize\nutilized\nutilizing\nuu\nva\nvaca\nvadis\nvaldes\nvalor\nvania\nvann\nvapor\nvapors\nvar\nvari\nvas\nvauban\nveen\nvelden\nveritas\nverma\nvernet\nverschie\nverschiede\nverwaltungs\nvey\nviale\nvicks\nvide\nvidya\nvierkant\nvieweg\nvigor\nvin\nvinh\nvir\nvirg\nvisser\nvisualization\nvisualizations\nvisualizing\nvive\nviz\nvo\nvoight\nvorder\nvorge\nvorgehensmodell\nvos\nvossen\nvox\nvoy\nwaa\nwageningen\nwah\nwak\nwallin\nwarrenton\nwasa\nwashroom\nwasn\nwasson\nwat\nwatercolor\nwatkinson\nwaverly\nwayman\nwebinare\nwech\nwef\nwegener\nwei\nweichert\nweigel\nweils\nweingart\nwel\nwellcome\nwerf\nwescott\nwezel\nwga\nwhe\nwher\nwhi\nwhic\nwhitepaper\nwhitten\nwieviel\nwifi\nwik\nwillful\nwillson\nwindeck\nwis\nwisc\nwiss\nwissenschafts\nwiththe\nwittenburg\nwmo\nwofford\nwoll\nwom\nwooldridge\nwoolf\nwor\nwou\nwoul\nwouldn\nwulf\nwur\nwusst\nwuttke\nwäh\nwür\nwüthrich\nxia\nxiao\nyager\nyannis\nyare\nyasemin\nyi\nyoon\nyoun\nyu\nyumi\nyun\nyuval\nza\nzachariah\nzalando\nzeich\nzeidler\nzeng\nzent\nzi\nzon\nzung\nzusam\nzwi\nöffent\nöpnv\nüberarb\n",
  "docker-compose.yml": "# zotero-redisearch-rag tool version: 0.2.3\nservices:\n  redis-stack:\n    image: redis/redis-stack-server:latest\n    command: [\"redis-stack-server\", \"/redis-stack.conf\", \"--dir\", \"/data\"]\n    environment:\n      - REDIS_ARGS=\n    ports:\n      - \"${ZRR_PORT:-6379}:6379\"\n    volumes:\n      - \"${ZRR_DATA_DIR:-./.zotero-redisearch-rag/redis-data}:/data\"\n      - \"./redis-stack.conf:/redis-stack.conf:ro\"\n",
  "redis-stack.conf": "# zotero-redisearch-rag tool version: 0.2.3\n# Redis Stack persistence config for local RAG index\nappendonly yes\nappendfsync everysec\n\ndir /data\n",
};