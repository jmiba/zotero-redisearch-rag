export const TOOL_ASSETS: Record<string, string> = {
  "docling_extract.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nimport argparse\nimport atexit\nimport base64\nimport errno\nimport hashlib\nimport json\nimport math\nimport logging\nimport os\nimport random\nimport re\nimport shutil\nimport sys\nimport time\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nfrom dataclasses import dataclass, field, fields, asdict\nfrom typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple\nimport langcodes\nimport warnings\nfrom ocr_paddle import ocr_pages_with_paddle, ocr_pages_with_paddle_structure, ocr_pages_with_paddle_vl\nfrom ocr_tesseract import find_tesseract_path, ocr_pages_with_tesseract\n\n# Reduce noisy warnings and route them to logging\nlogging.captureWarnings(True)\ntry:\n    from PIL import Image as _PILImage  # type: ignore\n    # Disable DecompressionBomb warnings (we control DPI); still safe for local PDFs\n    _PILImage.MAX_IMAGE_PIXELS = None  # type: ignore[attr-defined]\n    if hasattr(_PILImage, \"DecompressionBombWarning\"):\n        warnings.filterwarnings(\"ignore\", category=_PILImage.DecompressionBombWarning)  # type: ignore[attr-defined]\nexcept Exception:\n    pass\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\nLOGGER = logging.getLogger(\"docling_extract\")\n\n# Stores details about the last spellchecker built (backend and dictionary files)\n# Example: {\"backend\": \"spylls\", \"aff\": \"/path/en_GB.aff\", \"dic\": \"/path/en_GB.dic\"}\nLAST_SPELLCHECKER_INFO: Dict[str, Any] = {}\nSPELLCHECKER_CACHE: Dict[str, Any] = {}\n\n\ndef eprint(message: str) -> None:\n    try:\n        sys.stderr.write(message + \"\\n\")\n    except BrokenPipeError:\n        return\n    except OSError as exc:\n        if exc.errno == errno.EPIPE:\n            return\n        raise\n\n\nProgressCallback = Callable[[int, str, str], None]\n\n\ndef make_progress_emitter(enabled: bool) -> ProgressCallback:\n    if not enabled:\n        def _noop(percent: int, stage: str, message: str) -> None:\n            return None\n        return _noop\n\n    broken_pipe = False\n\n    def _emit(percent: int, stage: str, message: str) -> None:\n        nonlocal broken_pipe\n        if broken_pipe:\n            return\n        payload = {\n            \"type\": \"progress\",\n            \"percent\": max(0, min(100, int(percent))),\n            \"stage\": stage,\n            \"message\": message,\n        }\n        try:\n            print(json.dumps(payload), flush=True)\n        except BrokenPipeError:\n            broken_pipe = True\n        except OSError as exc:\n            if exc.errno == errno.EPIPE:\n                broken_pipe = True\n                return\n            raise\n\n    return _emit\n\n\n@dataclass\nclass DoclingProcessingConfig:\n    ocr_mode: str = \"auto\"\n    prefer_ocr_engine: str = \"paddle\"\n    fallback_ocr_engine: str = \"tesseract\"\n    language_hint: Optional[str] = None\n    default_lang_german: str = \"deu+eng\"\n    default_lang_english: str = \"eng\"\n    min_text_chars_per_page: int = 40\n    min_text_pages_ratio: float = 0.3\n    quality_alpha_ratio_threshold: float = 0.6\n    quality_suspicious_token_threshold: float = 0.25\n    quality_min_avg_chars_per_page: int = 80\n    quality_confidence_threshold: float = 0.5\n    quality_use_wordfreq: bool = True\n    quality_wordfreq_min_zipf: float = 3.0\n    quality_image_heavy_text_chars: int = 200\n    quality_image_heavy_min_images: int = 2\n    quality_image_heavy_ratio_threshold: float = 0.6\n    quality_image_heavy_penalty: float = 0.3\n    quality_image_page_ratio_threshold: float = 0.7\n    quality_classifier_enable: bool = True\n    quality_classifier_max_pages: int = 12\n    quality_classifier_min_samples: int = 6\n    quality_classifier_decision_ratio: float = 0.6\n    quality_classifier_image_coverage_threshold: float = 0.6\n    quality_classifier_invisible_text_ratio_threshold: float = 0.7\n    quality_classifier_min_text_ops: int = 4\n    column_detect_enable: bool = True\n    column_detect_dpi: int = 150\n    column_detect_max_pages: int = 3\n    column_detect_crop_top_ratio: float = 0.08\n    column_detect_crop_bottom_ratio: float = 0.08\n    column_detect_threshold_std_mult: float = 1.0\n    column_detect_threshold_min: int = 120\n    column_detect_threshold_max: int = 210\n    column_detect_text_percentile: float = 0.7\n    column_detect_min_text_density: float = 0.02\n    column_detect_gap_threshold_ratio: float = 0.2\n    column_detect_min_gap_density: float = 0.01\n    column_detect_min_gap_ratio: float = 0.03\n    column_detect_min_pages_ratio: float = 0.4\n    column_detect_smooth_window: int = 5\n    page_range_sample_tokens: int = 200\n    page_range_min_overlap: float = 0.02\n    page_range_min_hits: int = 5\n    page_range_top_k: int = 5\n    page_range_peak_ratio: float = 0.5\n    page_range_cluster_gap: int = 1\n    page_range_max_span_ratio: float = 0.7\n    max_chunk_chars: int = 3000\n    chunk_overlap_chars: int = 250\n    per_page_ocr_on_low_quality: bool = True\n    force_per_page_ocr: bool = False\n    force_ocr_on_low_quality_text: bool = False\n    enable_post_correction: bool = True\n    enable_dictionary_correction: bool = False\n    dictionary_path: Optional[str] = None\n    dictionary_words: Optional[Sequence[str]] = None\n    default_dictionary_name: str = \"ocr_wordlist.txt\"\n    enable_llm_correction: bool = False\n    llm_correct: Optional[Callable[[str], str]] = None\n    llm_cleanup_base_url: Optional[str] = None\n    llm_cleanup_api_key: Optional[str] = None\n    llm_cleanup_model: Optional[str] = None\n    llm_cleanup_temperature: float = 0.0\n    llm_cleanup_timeout_sec: int = 60\n    llm_correction_min_quality: float = 0.35\n    llm_correction_max_chars: int = 2000\n    enable_boilerplate_removal: bool = True\n    boilerplate_min_line_len: int = 8\n    boilerplate_repeat_ratio: float = 0.4\n    boilerplate_min_pages: int = 3\n    boilerplate_edge_lines: int = 3\n    boilerplate_ngram_size: int = 3\n    boilerplate_near_dup_threshold: float = 0.82\n    postprocess_markdown: bool = False\n    analysis_max_pages: int = 5\n    analysis_sample_strategy: str = \"middle\"\n    ocr_dpi: int = 300\n    ocr_overlay_dpi: int = 300\n    paddle_max_dpi: int = 300\n    paddle_target_max_side_px: int = 6000\n    paddle_use_doc_orientation_classify: bool = True\n    paddle_use_doc_unwarping: bool = False\n    paddle_use_textline_orientation: bool = True\n    paddle_use_structure_v3: bool = False\n    paddle_structure_version: str = \"PP-StructureV3\"\n    paddle_structure_header_ratio: float = 0.05\n    paddle_structure_footer_ratio: float = 0.05\n    # When true and PP-StructureV3 is used, re-run recognition on detected layout\n    # boxes using PaddleOCR recognizer to better follow layout boxes and reading order.\n    paddle_recognize_from_layout_boxes: bool = True\n    # PaddleX DocLayout extraction (mirrors paddle_ocr_smoke.py layout path).\n    paddle_use_paddlex_layout: bool = True\n    paddle_layout_model: str = \"PP-DocLayout-L\"\n    paddle_layout_threshold: float = 0.3\n    paddle_layout_img_size: Optional[int] = 6000\n    paddle_layout_merge: str = \"large\"\n    paddle_layout_unclip: float = 1.06\n    paddle_crop_padding: int = 60\n    paddle_crop_vbias: int = 6\n    paddle_layout_device: Optional[str] = None\n    paddle_layout_nms: bool = True\n    paddle_layout_keep_labels: str = (\n        \"text,paragraph_title,title,heading,caption,header,number,figure_title,\"\n        \"body,section,text_block,textbox,textline,paragraph\"\n    )\n    paddle_layout_recognize_boxes: bool = True\n    paddle_layout_fail_on_zero: bool = True\n    paddle_layout_save_crops: Optional[str] = None\n    paddle_dump: bool = False\n    paddle_layout_markdown_out: Optional[str] = None\n    # PaddleOCR-VL (optional, requires paddleocr[doc-parser])\n    paddle_use_vl: bool = False\n    paddle_vl_device: Optional[str] = None\n    paddle_vl_rec_backend: Optional[str] = None\n    paddle_vl_rec_server_url: Optional[str] = None\n    paddle_vl_rec_max_concurrency: Optional[int] = None\n    paddle_vl_rec_api_key: Optional[str] = None\n    paddle_vl_use_layout_detection: Optional[bool] = True\n    paddle_vl_use_chart_recognition: Optional[bool] = True\n    paddle_vl_format_block_content: Optional[bool] = True\n    paddle_vl_prompt_label: Optional[str] = \"ocr\"\n    paddle_vl_use_queues: Optional[bool] = False\n    paddle_vl_layout_threshold: Optional[float] = 0.3\n    paddle_vl_layout_nms: Optional[bool] = True\n    paddle_vl_layout_unclip: Optional[float] = 1.2\n    paddle_vl_layout_merge: Optional[str] = \"small\"\n    paddle_vl_api_disable: bool = False\n    paddle_vl_api_url: Optional[str] = None\n    paddle_vl_api_token: Optional[str] = None\n    paddle_vl_api_timeout_sec: int = 600\n    paddle_vl_api_max_pages: int = 100\n    paddle_vl_api_max_chunk_bytes: int = 3000000\n    paddle_vl_markdown_ignore_labels: Optional[Sequence[str]] = field(\n        default_factory=lambda: [\"header\",\"header_image\",\"footer\",\"footer_image\",\"number\",\"aside_text\"]\n    )\n    paddle_vl_repetition_penalty: Optional[float] = 1.0\n    paddle_vl_temperature: Optional[float] = 0.0\n    paddle_vl_top_p: Optional[float] = 1.0\n    paddle_vl_min_pixels: Optional[int] = 147384\n    paddle_vl_max_pixels: Optional[int] = 2822400\n    paddle_structure_api_disable: bool = False\n    paddle_structure_api_url: Optional[str] = None\n    paddle_structure_api_token: Optional[str] = None\n    paddle_structure_api_timeout_sec: int = 600\n    # Optional Hunspell integration\n    enable_hunspell: bool = True\n    hunspell_aff_path: Optional[str] = None\n    hunspell_dic_path: Optional[str] = None\n\n\n@dataclass\nclass OcrRouteDecision:\n    ocr_used: bool\n    ocr_engine: str\n    languages: str\n    route_reason: str\n    use_external_ocr: bool\n    per_page_ocr: bool\n    per_page_reason: str\n\n\n@dataclass\nclass TextQuality:\n    avg_chars_per_page: float\n    alpha_ratio: float\n    suspicious_token_ratio: float\n    confidence_proxy: float\n    dictionary_hit_ratio: Optional[float] = None\n    spellchecker_hit_ratio: Optional[float] = None\n    image_heavy_ratio: Optional[float] = None\n    image_page_ratio: Optional[float] = None\n    ocr_overlay_ratio: Optional[float] = None\n    digital_page_ratio: Optional[float] = None\n    layer_classification: Optional[str] = None\n    effective_confidence_proxy: Optional[float] = None\n\n@dataclass\nclass ColumnLayoutDetection:\n    detected: bool\n    page_ratio: float\n    reason: str\n\n@dataclass\nclass DoclingConversionResult:\n    markdown: str\n    pages: List[Dict[str, Any]]\n    metadata: Dict[str, Any]\n\n\n@dataclass\nclass BoilerplateCluster:\n    rep: str\n    shingles: Set[str]\n    count: int = 0\n\n\ndef normalize_text(text: str) -> str:\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n\ndef extract_alnum_tokens(text: str) -> List[str]:\n    tokens: List[str] = []\n    current: List[str] = []\n    for char in text:\n        if char.isalnum():\n            current.append(char)\n        elif current:\n            tokens.append(\"\".join(current))\n            current = []\n    if current:\n        tokens.append(\"\".join(current))\n    return tokens\n\n\ndef remove_image_placeholders(text: str) -> str:\n    return re.sub(r\"<!--\\s*image\\s*-->\", \"\", text, flags=re.IGNORECASE)\n\n\ndef clean_chunk_text(text: str, config: Optional[DoclingProcessingConfig]) -> str:\n    if not text:\n        return \"\"\n    return remove_image_placeholders(text)\n\n\ndef normalize_whitespace(text: str) -> str:\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()\n\ndef normalize_display_markdown(text: str) -> str:\n    if not text:\n        return \"\"\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    lines = [line.rstrip() for line in text.split(\"\\n\")]\n    text = \"\\n\".join(lines)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    return text.strip()\n\n_IMG_TAG_RE = re.compile(r\"<img[^>]*?>\", re.IGNORECASE)\n_DIV_IMG_TAG_RE = re.compile(r\"<div[^>]*>\\s*<img[^>]*?>\\s*</div>\", re.IGNORECASE | re.DOTALL)\n\n\ndef _extract_image_filename(src: str) -> Optional[str]:\n    if not src:\n        return None\n    if src.startswith(\"data:\"):\n        return None\n    path = src\n    if src.startswith((\"http://\", \"https://\")):\n        try:\n            path = urllib.parse.urlparse(src).path\n        except Exception:\n            path = src\n    filename = os.path.basename(path)\n    return filename or None\n\n\ndef _extract_img_attr(tag: str, attr: str) -> Optional[str]:\n    match = re.search(rf\"\\b{re.escape(attr)}=(['\\\"])(?P<val>[^'\\\"]*)\\1\", tag, re.IGNORECASE)\n    if match:\n        return match.group(\"val\")\n    return None\n\n\ndef _obsidian_image_link(\n    src: str,\n    alt_text: Optional[str] = None,\n    image_labels: Optional[Dict[str, str]] = None,\n) -> Optional[str]:\n    filename = _extract_image_filename(src)\n    if not filename:\n        return None\n    label = None\n    if image_labels:\n        label = image_labels.get(filename) or image_labels.get(src)\n    if not label and alt_text:\n        label = alt_text.strip() or None\n    if label:\n        return f\"![[{filename}|{label}]]\"\n    return f\"![[{filename}]]\"\n\n\ndef convert_html_images_to_obsidian(\n    markdown: str,\n    image_labels: Optional[Dict[str, str]] = None,\n) -> str:\n    if not markdown:\n        return \"\"\n\n    def replace_div(match: re.Match[str]) -> str:\n        tag = match.group(0)\n        src = _extract_img_attr(tag, \"src\")\n        alt_text = _extract_img_attr(tag, \"alt\")\n        if not src:\n            return tag\n        link = _obsidian_image_link(src, alt_text=alt_text, image_labels=image_labels)\n        return link if link else match.group(0)\n\n    def replace_img(match: re.Match[str]) -> str:\n        tag = match.group(0)\n        src = _extract_img_attr(tag, \"src\")\n        alt_text = _extract_img_attr(tag, \"alt\")\n        if not src:\n            return tag\n        link = _obsidian_image_link(src, alt_text=alt_text, image_labels=image_labels)\n        return link if link else match.group(0)\n\n    updated = _DIV_IMG_TAG_RE.sub(replace_div, markdown)\n    updated = _IMG_TAG_RE.sub(replace_img, updated)\n    return updated\n\n\ndef remap_layout_image_keys(layout_images: Dict[str, Any]) -> Dict[str, Any]:\n    remapped: Dict[str, Any] = {}\n    for key, value in layout_images.items():\n        new_key = key\n        if isinstance(key, str):\n            filename = _extract_image_filename(key)\n            if filename:\n                new_key = filename\n        if new_key in remapped:\n            LOGGER.warning(\"Duplicate layout image key after remap: %s\", new_key)\n            continue\n        remapped[new_key] = value\n    return remapped\n\n\ndef normalize_chunk_whitespace(text: str) -> str:\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \" \")\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    lines = text.split(\"\\n\")\n    out_lines: List[str] = []\n    buffer: List[str] = []\n\n    def flush() -> None:\n        if buffer:\n            out_lines.append(\" \".join(buffer).strip())\n            buffer.clear()\n\n    heading_re = re.compile(r\"^#{1,6}\\s+\")\n    list_re = re.compile(\n        r\"^(?:[-*+]\\s+|\\d+[.)]\\s+|[\\u2022\\u2023\\u25AA\\u2013\\u2014\\u00B7\\x81]\\s+)\"\n    )\n    table_sep_re = re.compile(r\"^\\s*\\|?\\s*:?-{2,}:?(?:\\s*\\|\\s*:?-{2,}:?)+\\s*\\|?\\s*$\")\n\n    def is_table_line(line: str) -> bool:\n        if table_sep_re.match(line):\n            return True\n        return line.count(\"|\") >= 2\n    for line in lines:\n        stripped = line.replace(\"\\ufeff\", \"\").strip()\n        if not stripped:\n            flush()\n            if out_lines and out_lines[-1] != \"\":\n                out_lines.append(\"\")\n            continue\n        if (\n            heading_re.match(stripped)\n            or list_re.match(stripped)\n            or is_table_line(stripped)\n        ):\n            flush()\n            out_lines.append(stripped)\n            continue\n        buffer.append(stripped)\n\n    flush()\n    result = \"\\n\".join(out_lines)\n    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n    return result.strip()\n\n\ndef reset_debug_directory(path: Optional[str]) -> None:\n    if not path:\n        return\n    try:\n        if os.path.isdir(path):\n            shutil.rmtree(path)\n        elif os.path.exists(path):\n            os.remove(path)\n    except Exception as exc:\n        LOGGER.warning(\"Failed to clear debug directory %s: %s\", path, exc)\n\n\ndef reflow_page_text(text: str) -> str:\n    if not text:\n        return \"\"\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \" \")\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    lines = text.split(\"\\n\")\n    out_lines: List[str] = []\n    buffer: List[str] = []\n\n    def flush() -> None:\n        if buffer:\n            out_lines.append(\" \".join(buffer).strip())\n            buffer.clear()\n\n    heading_re = re.compile(r\"^#{1,6}\\s+\")\n    list_bullet_re = re.compile(r\"^[-*+]\\s+(.+)\")\n    list_number_re = re.compile(r\"^(\\d+)[.)]\\s+(.+)\")\n    list_unicode_re = re.compile(r\"^[\\u2022\\u2023\\u25AA\\u2013\\u2014\\u00B7\\x81]\\s*(.+)\")\n    table_sep_re = re.compile(r\"^\\s*\\|?\\s*:?-{2,}:?(?:\\s*\\|\\s*:?-{2,}:?)+\\s*\\|?\\s*$\")\n    url_re = re.compile(r\"^(https?://|doi:)\", re.IGNORECASE)\n\n    def is_table_line(line: str) -> bool:\n        if table_sep_re.match(line):\n            return True\n        return line.count(\"|\") >= 2\n\n    list_active = False\n    list_prefix = \"\"\n    list_buffer: List[str] = []\n\n    def flush_list() -> None:\n        nonlocal list_active, list_prefix, list_buffer\n        if list_active and list_buffer:\n            out_lines.append(f\"{list_prefix}{' '.join(list_buffer).strip()}\")\n        list_active = False\n        list_prefix = \"\"\n        list_buffer = []\n\n    for line in lines:\n        stripped = line.replace(\"\\ufeff\", \"\").strip()\n        if not stripped:\n            flush_list()\n            flush()\n            if out_lines and out_lines[-1] != \"\":\n                out_lines.append(\"\")\n            continue\n        bullet_match = list_bullet_re.match(stripped)\n        number_match = list_number_re.match(stripped)\n        unicode_match = list_unicode_re.match(stripped)\n        if bullet_match or number_match or unicode_match:\n            flush()\n            flush_list()\n            if number_match:\n                list_prefix = f\"{number_match.group(1)}. \"\n                list_buffer = [number_match.group(2).strip()]\n            else:\n                list_prefix = \"- \"\n                list_buffer = [(bullet_match or unicode_match).group(1).strip()]\n            list_active = True\n            continue\n        if heading_re.match(stripped) or is_table_line(stripped):\n            flush()\n            flush_list()\n            out_lines.append(stripped)\n            continue\n        if list_active and url_re.match(stripped):\n            list_buffer.append(stripped)\n            continue\n        if url_re.match(stripped):\n            flush()\n            flush_list()\n            out_lines.append(stripped)\n            continue\n        if list_active:\n            list_buffer.append(stripped)\n            continue\n        buffer.append(stripped)\n\n    flush()\n    flush_list()\n    result = \"\\n\".join(out_lines)\n    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n    return result.strip()\n\n\n\ndef dehyphenate_text(text: str) -> str:\n    return re.sub(r\"(?<=\\w)-\\s*\\n\\s*(?=\\w)\", \"\", text)\n\n\ndef replace_ligatures(text: str) -> str:\n    return (\n        text.replace(\"\\ufb01\", \"fi\")\n        .replace(\"\\ufb02\", \"fl\")\n        .replace(\"\\ufb03\", \"ffi\")\n        .replace(\"\\ufb04\", \"ffl\")\n    )\n\n_BOILERPLATE_PATTERNS = [\n    re.compile(r\"(?i)^this content downloaded from\"),\n    re.compile(r\"(?i)content downloaded from\"),\n    re.compile(r\"(?i)^all use subject to\"),\n    re.compile(r\"(?i)about\\s*\\.?jstor\\.org/terms\"),\n    re.compile(r\"(?i)^jstor is a not-for-profit\"),\n    re.compile(r\"(?i)^your use of the jstor archive\"),\n    re.compile(r\"(?i)^for more information about jstor\"),\n    re.compile(r\"(?i)^state historical society\"),\n    re.compile(r\"(?i)\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b.*\\butc\\b\"),\n]\n_PAGE_NUMBER_RE = re.compile(r\"^[ivxlcdm]+$|^\\d{1,4}$\", re.IGNORECASE)\n_IP_RE = re.compile(r\"\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b\")\n_TIME_RE = re.compile(r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b\")\n_DATE_ISO_RE = re.compile(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\")\n_DATE_SLASH_RE = re.compile(r\"\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b\")\n_MONTH_RE = (\n    r\"(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|\"\n    r\"jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\"\n)\n_DATE_TEXT_RE = re.compile(rf\"\\b\\d{{1,2}}\\s+{_MONTH_RE}\\s+\\d{{2,4}}\\b\", re.IGNORECASE)\n_DATE_TEXT_REVERSE = re.compile(rf\"\\b{_MONTH_RE}\\s+\\d{{1,2}},?\\s+\\d{{4}}\\b\", re.IGNORECASE)\n_LONG_NUM_RE = re.compile(r\"\\b\\d{4,}\\b\")\n\n\ndef mask_boilerplate_tokens(text: str) -> str:\n    cleaned = text\n    cleaned = _IP_RE.sub(\"<ip>\", cleaned)\n    cleaned = _TIME_RE.sub(\"<time>\", cleaned)\n    cleaned = _DATE_ISO_RE.sub(\"<date>\", cleaned)\n    cleaned = _DATE_SLASH_RE.sub(\"<date>\", cleaned)\n    cleaned = _DATE_TEXT_RE.sub(\"<date>\", cleaned)\n    cleaned = _DATE_TEXT_REVERSE.sub(\"<date>\", cleaned)\n    cleaned = _LONG_NUM_RE.sub(\"<num>\", cleaned)\n    cleaned = re.sub(r\"\\d\", \"0\", cleaned)\n    return cleaned\n\n\ndef normalize_boilerplate_line(line: str) -> str:\n    cleaned = line.replace(\"\\u00a0\", \" \")\n    cleaned = cleaned.lower()\n    cleaned = mask_boilerplate_tokens(cleaned)\n    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n    return cleaned\n\n\ndef is_boilerplate_line(line: str) -> bool:\n    if not line:\n        return False\n    if _PAGE_NUMBER_RE.match(line):\n        return True\n    for pattern in _BOILERPLATE_PATTERNS:\n        if pattern.search(line):\n            return True\n    return False\n\n\ndef line_shingles(text: str, size: int) -> Set[str]:\n    cleaned = re.sub(r\"\\s+\", \"\", text)\n    if size <= 1:\n        return {cleaned} if cleaned else set()\n    if len(cleaned) <= size:\n        return {cleaned} if cleaned else set()\n    return {cleaned[i:i + size] for i in range(len(cleaned) - size + 1)}\n\n\ndef jaccard_similarity(a: Set[str], b: Set[str]) -> float:\n    if not a or not b:\n        return 0.0\n    union = a | b\n    if not union:\n        return 0.0\n    return len(a & b) / len(union)\n\n\ndef match_boilerplate_cluster(\n    shingles: Set[str],\n    clusters: Sequence[BoilerplateCluster],\n    threshold: float,\n) -> Optional[int]:\n    best_idx: Optional[int] = None\n    best_score = 0.0\n    for idx, cluster in enumerate(clusters):\n        score = jaccard_similarity(shingles, cluster.shingles)\n        if score >= threshold and score > best_score:\n            best_idx = idx\n            best_score = score\n    return best_idx\n\n\ndef get_edge_lines(lines: Sequence[str], edge_lines: int) -> List[str]:\n    if edge_lines <= 0:\n        return list(lines)\n    total = len(lines)\n    if total <= edge_lines * 2:\n        return list(lines)\n    return list(lines[:edge_lines]) + list(lines[-edge_lines:])\n\n\ndef is_edge_line_index(idx: int, total: int, edge_lines: int) -> bool:\n    if edge_lines <= 0:\n        return True\n    return idx < edge_lines or idx >= max(0, total - edge_lines)\n\n\ndef select_edge_texts_by_y(\n    lines: Sequence[Tuple[str, float]],\n    edge_lines: int,\n) -> List[str]:\n    if edge_lines <= 0:\n        return [text for text, _ in lines]\n    sorted_lines = sorted(lines, key=lambda item: item[1])\n    total = len(sorted_lines)\n    if total <= edge_lines * 2:\n        return [text for text, _ in sorted_lines]\n    top = sorted_lines[:edge_lines]\n    bottom = sorted_lines[-edge_lines:]\n    return [text for text, _ in top + bottom]\n\n\ndef edge_ids_by_y(\n    items: Sequence[Tuple[int, float]],\n    edge_lines: int,\n) -> Set[int]:\n    if edge_lines <= 0:\n        return {idx for idx, _ in items}\n    sorted_items = sorted(items, key=lambda item: item[1])\n    total = len(sorted_items)\n    if total <= edge_lines * 2:\n        return {idx for idx, _ in sorted_items}\n    top = sorted_items[:edge_lines]\n    bottom = sorted_items[-edge_lines:]\n    return {idx for idx, _ in top + bottom}\n\n\ndef detect_repeated_line_clusters(\n    page_lines: Sequence[Sequence[str]],\n    total_pages: int,\n    config: DoclingProcessingConfig,\n) -> Tuple[List[BoilerplateCluster], int]:\n    if total_pages < config.boilerplate_min_pages:\n        return [], 0\n    threshold = max(2, int(math.ceil(total_pages * config.boilerplate_repeat_ratio)))\n    clusters: List[BoilerplateCluster] = []\n    for lines in page_lines:\n        seen: Set[int] = set()\n        for line in lines:\n            normalized = normalize_boilerplate_line(line)\n            if not normalized or len(normalized) < config.boilerplate_min_line_len:\n                continue\n            shingles = line_shingles(normalized, config.boilerplate_ngram_size)\n            idx = match_boilerplate_cluster(\n                shingles,\n                clusters,\n                config.boilerplate_near_dup_threshold,\n            )\n            if idx is None:\n                clusters.append(BoilerplateCluster(rep=normalized, shingles=shingles, count=0))\n                idx = len(clusters) - 1\n            if idx not in seen:\n                clusters[idx].count += 1\n                seen.add(idx)\n    repeated = [cluster for cluster in clusters if cluster.count >= threshold]\n    return repeated, threshold\n\n\ndef matches_repeated_cluster(\n    line: str,\n    clusters: Sequence[BoilerplateCluster],\n    config: DoclingProcessingConfig,\n) -> bool:\n    if not clusters:\n        return False\n    normalized = normalize_boilerplate_line(line)\n    if not normalized:\n        return False\n    shingles = line_shingles(normalized, config.boilerplate_ngram_size)\n    return match_boilerplate_cluster(\n        shingles,\n        clusters,\n        config.boilerplate_near_dup_threshold,\n    ) is not None\n\n\ndef detect_repeated_lines(\n    pages: Sequence[Dict[str, Any]],\n    config: DoclingProcessingConfig,\n) -> Tuple[List[BoilerplateCluster], int]:\n    total_pages = len(pages)\n    if total_pages < config.boilerplate_min_pages:\n        return [], 0\n    page_lines: List[List[str]] = []\n    for page in pages:\n        lines = str(page.get(\"text\", \"\")).splitlines()\n        page_lines.append(get_edge_lines(lines, config.boilerplate_edge_lines))\n    clusters, threshold = detect_repeated_line_clusters(page_lines, total_pages, config)\n    return clusters, threshold\n\n\ndef remove_boilerplate_from_pages(\n    pages: List[Dict[str, Any]],\n    config: DoclingProcessingConfig,\n) -> Tuple[List[Dict[str, Any]], List[BoilerplateCluster], Dict[str, Any]]:\n    if not config.enable_boilerplate_removal or not pages:\n        return pages, [], {}\n    repeated_clusters, threshold = detect_repeated_lines(pages, config)\n    removed_total = 0\n    updated_pages: List[Dict[str, Any]] = []\n    for page in pages:\n        text = str(page.get(\"text\", \"\"))\n        if not text:\n            updated_pages.append(page)\n            continue\n        lines = text.splitlines()\n        kept_lines: List[str] = []\n        removed_page = 0\n        for idx, line in enumerate(lines):\n            normalized = normalize_boilerplate_line(line)\n            if not normalized:\n                kept_lines.append(\"\")\n                continue\n            is_edge = is_edge_line_index(idx, len(lines), config.boilerplate_edge_lines)\n            if is_edge and (\n                matches_repeated_cluster(line, repeated_clusters, config)\n                or is_boilerplate_line(normalized)\n            ):\n                removed_page += 1\n                continue\n            kept_lines.append(line)\n        removed_total += removed_page\n        new_page = dict(page)\n        new_page[\"text\"] = \"\\n\".join(kept_lines).strip()\n        updated_pages.append(new_page)\n    if removed_total:\n        LOGGER.info(\n            \"Boilerplate removal: removed %s lines (repeat_threshold=%s, repeated_lines=%s)\",\n            removed_total,\n            threshold,\n            len(repeated_clusters),\n        )\n    return updated_pages, repeated_clusters, {\n        \"removed_lines\": removed_total,\n        \"repeat_threshold\": threshold,\n        \"repeated_lines\": len(repeated_clusters),\n    }\n\n\ndef remove_boilerplate_from_markdown(\n    markdown: str,\n    repeated_clusters: Sequence[BoilerplateCluster],\n    config: DoclingProcessingConfig,\n) -> str:\n    if not config.enable_boilerplate_removal or not markdown:\n        return markdown\n    kept: List[str] = []\n    removed = 0\n    for line in markdown.splitlines():\n        normalized = normalize_boilerplate_line(line)\n        if not normalized:\n            kept.append(line)\n            continue\n        if matches_repeated_cluster(line, repeated_clusters, config) or is_boilerplate_line(normalized):\n            removed += 1\n            continue\n        kept.append(line)\n    if removed:\n        LOGGER.info(\"Boilerplate removal: stripped %s markdown lines\", removed)\n    return \"\\n\".join(kept).strip()\n\ndef split_paragraphs(text: str) -> List[str]:\n    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n    return [para.strip() for para in paragraphs if para.strip()]\n\n\ndef split_long_text(text: str, max_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n    if len(sentences) <= 1:\n        return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n    for sentence in sentences:\n        sent = sentence.strip()\n        if not sent:\n            continue\n        if current_len + len(sent) + 1 > max_chars and current:\n            chunks.append(\" \".join(current).strip())\n            current = [sent]\n            current_len = len(sent)\n        else:\n            current.append(sent)\n            current_len += len(sent) + 1\n    if current:\n        chunks.append(\" \".join(current).strip())\n    return chunks\n\n\ndef split_text_by_size(text: str, max_chars: int, overlap_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    paragraphs = split_paragraphs(text) or [text]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n\n    def flush() -> None:\n        nonlocal current, current_len\n        if not current:\n            return\n        chunk = \"\\n\\n\".join(current).strip()\n        chunks.append(chunk)\n        current = []\n        current_len = 0\n\n    for para in paragraphs:\n        for piece in split_long_text(para, max_chars):\n            piece_len = len(piece)\n            if current_len + piece_len + 2 > max_chars and current:\n                flush()\n            current.append(piece)\n            current_len += piece_len + 2\n\n    flush()\n\n    if overlap_chars <= 0 or len(chunks) <= 1:\n        return chunks\n\n    overlapped: List[str] = []\n    previous = \"\"\n    for chunk in chunks:\n        if previous:\n            overlap = previous[-overlap_chars:]\n            combined = f\"{overlap}\\n{chunk}\".strip()\n        else:\n            combined = chunk\n        overlapped.append(combined)\n        previous = chunk\n    return overlapped\n\n\ndef select_wordfreq_languages(languages: str) -> List[str]:\n    lang = (languages or \"\").lower()\n    selected: List[str] = []\n    if any(token in lang for token in (\"deu\", \"ger\", \"de\", \"german\", \"deutsch\")):\n        selected.append(\"de\")\n    if any(token in lang for token in (\"eng\", \"en\", \"english\")):\n        selected.append(\"en\")\n    if any(token in lang for token in (\"fra\", \"fr\", \"french\", \"francais\", \"français\")):\n        selected.append(\"fr\")\n    if any(token in lang for token in (\"spa\", \"es\", \"spanish\", \"espanol\", \"español\")):\n        selected.append(\"es\")\n    if any(token in lang for token in (\"ita\", \"it\", \"italian\", \"italiano\")):\n        selected.append(\"it\")\n    if any(token in lang for token in (\"pol\", \"pl\", \"polish\", \"polski\")):\n        selected.append(\"pl\")\n    if any(token in lang for token in (\"por\", \"pt\", \"portuguese\", \"português\", \"portugues\")):\n        selected.append(\"pt\")\n    if any(token in lang for token in (\"nld\", \"dut\", \"nl\", \"dutch\", \"nederlands\")):\n        selected.append(\"nl\")\n    if any(token in lang for token in (\"swe\", \"sv\", \"swedish\", \"svenska\")):\n        selected.append(\"sv\")\n    if any(token in lang for token in (\"nor\", \"no\", \"norsk\", \"bokmal\", \"bokmål\", \"nynorsk\")):\n        selected.append(\"no\")\n    if any(token in lang for token in (\"dan\", \"da\", \"danish\", \"dansk\")):\n        selected.append(\"da\")\n    if any(token in lang for token in (\"fin\", \"fi\", \"finnish\", \"suomi\")):\n        selected.append(\"fi\")\n    if any(token in lang for token in (\"rus\", \"ru\", \"russian\", \"рус\")):\n        selected.append(\"ru\")\n    if any(token in lang for token in (\"ces\", \"cze\", \"cs\", \"czech\", \"čeština\", \"cesky\", \"česky\")):\n        selected.append(\"cs\")\n    if any(token in lang for token in (\"ell\", \"el\", \"greek\", \"ελληνικά\")):\n        selected.append(\"el\")\n    if not selected:\n        selected.append(\"en\")\n    return selected\n\n\ndef compute_dictionary_hit_ratio(\n    tokens: Sequence[str],\n    languages: str,\n    min_zipf: float,\n) -> Optional[float]:\n    try:\n        from wordfreq import zipf_frequency\n    except Exception:\n        return None\n\n    if not tokens:\n        return None\n    lang_codes = select_wordfreq_languages(languages)\n    hits = 0\n    total = 0\n    for token in tokens:\n        lower = token.lower()\n        if len(lower) < 2:\n            continue\n        total += 1\n        if any(zipf_frequency(lower, lang) >= min_zipf for lang in lang_codes):\n            hits += 1\n    if not total:\n        return None\n    return hits / total\n\n\ndef compute_spellchecker_hit_ratio(\n    tokens: Sequence[str],\n    languages: str,\n    config: Optional[DoclingProcessingConfig],\n) -> Optional[float]:\n    if not config or not config.enable_hunspell or not languages:\n        return None\n    hs = build_spellchecker_for_languages(config, languages)\n    if hs is None:\n        return None\n    hits = 0\n    total = 0\n    for token in tokens:\n        if len(token) < 2:\n            continue\n        if not any(char.isalpha() for char in token):\n            continue\n        total += 1\n        try:\n            if hs.spell(token):\n                hits += 1\n        except Exception:\n            continue\n    if not total:\n        return None\n    return hits / total\n\n\ndef compute_image_heavy_ratio(\n    pages: Sequence[Dict[str, Any]],\n    config: DoclingProcessingConfig,\n) -> Optional[float]:\n    if not pages:\n        return None\n    heavy = 0\n    total = 0\n    for page in pages:\n        total += 1\n        text = str(page.get(\"text\", \"\"))\n        image_count = int(page.get(\"image_count\") or 0)\n        if len(text) < config.quality_image_heavy_text_chars and image_count >= config.quality_image_heavy_min_images:\n            heavy += 1\n    if not total:\n        return None\n    return heavy / total\n\n\ndef compute_image_page_ratio(pages: Sequence[Dict[str, Any]]) -> Optional[float]:\n    if not pages:\n        return None\n    total = 0\n    with_images = 0\n    for page in pages:\n        total += 1\n        image_count = int(page.get(\"image_count\") or 0)\n        if image_count > 0:\n            with_images += 1\n    if not total:\n        return None\n    return with_images / total\n\n\ndef _matrix_multiply(\n    left: Tuple[float, float, float, float, float, float],\n    right: Tuple[float, float, float, float, float, float],\n) -> Tuple[float, float, float, float, float, float]:\n    a1, b1, c1, d1, e1, f1 = left\n    a2, b2, c2, d2, e2, f2 = right\n    return (\n        a1 * a2 + b1 * c2,\n        a1 * b2 + b1 * d2,\n        c1 * a2 + d1 * c2,\n        c1 * b2 + d1 * d2,\n        e1 * a2 + f1 * c2 + e2,\n        e1 * b2 + f1 * d2 + f2,\n    )\n\n\ndef _operator_to_str(operator: Any) -> str:\n    if isinstance(operator, bytes):\n        try:\n            return operator.decode(\"latin-1\")\n        except Exception:\n            return str(operator)\n    return str(operator)\n\n\ndef _extract_xobjects_from_resources(resources: Any) -> Dict[str, Any]:\n    xobject_map: Dict[str, Any] = {}\n    try:\n        x_objects = resources.get(\"/XObject\") if resources else None\n        if x_objects:\n            x_objects = x_objects.get_object() if hasattr(x_objects, \"get_object\") else x_objects\n            for key, obj in x_objects.items():\n                key_name = str(key)\n                resolved = obj.get_object() if hasattr(obj, \"get_object\") else obj\n                xobject_map[key_name] = resolved\n    except Exception:\n        return {}\n    return xobject_map\n\n\ndef _extract_page_xobjects(page: Any) -> Dict[str, Any]:\n    try:\n        resources = page.get(\"/Resources\") or {}\n        return _extract_xobjects_from_resources(resources)\n    except Exception:\n        return {}\n\n\ndef _normalize_matrix(value: Any) -> Tuple[float, float, float, float, float, float]:\n    try:\n        if isinstance(value, (list, tuple)) and len(value) == 6:\n            return tuple(float(item) for item in value)\n    except Exception:\n        return (1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n    return (1.0, 0.0, 0.0, 1.0, 0.0, 0.0)\n\n\ndef _scan_content_stream(\n    content: Any,\n    metrics: Dict[str, Any],\n    ctm: Tuple[float, float, float, float, float, float],\n    resources: Any,\n    reader: Any,\n    text_render_mode: int,\n    visited: Set[Any],\n) -> None:\n    try:\n        from pypdf import ContentStream\n    except Exception:\n        return\n    try:\n        stream = ContentStream(content, reader)\n    except Exception:\n        return\n    local_ctm = ctm\n    ctm_stack: List[Tuple[float, float, float, float, float, float]] = []\n    render_mode = text_render_mode\n    render_stack: List[int] = []\n    xobject_map = _extract_xobjects_from_resources(resources)\n    for operands, operator in stream.operations:\n        op = _operator_to_str(operator)\n        if op == \"q\":\n            ctm_stack.append(local_ctm)\n            render_stack.append(render_mode)\n            continue\n        if op == \"Q\":\n            if ctm_stack:\n                local_ctm = ctm_stack.pop()\n            if render_stack:\n                render_mode = render_stack.pop()\n            continue\n        if op == \"cm\" and len(operands) == 6:\n            try:\n                cm = tuple(float(value) for value in operands)\n                local_ctm = _matrix_multiply(local_ctm, cm)  # type: ignore[arg-type]\n            except Exception:\n                pass\n            continue\n        if op == \"Tr\" and operands:\n            try:\n                render_mode = int(operands[0])\n            except Exception:\n                render_mode = 0\n            continue\n        if op in (\"Tj\", \"TJ\", \"'\", \"\\\"\"):\n            metrics[\"text_ops\"] += 1\n            if render_mode == 3:\n                metrics[\"invisible_text_ops\"] += 1\n            continue\n        if op == \"Do\" and operands:\n            name = str(operands[0])\n            xobj = xobject_map.get(name)\n            if not xobj:\n                continue\n            try:\n                subtype = xobj.get(\"/Subtype\")\n            except Exception:\n                subtype = None\n            if subtype == \"/Image\":\n                metrics[\"image_count\"] += 1\n                det = abs(local_ctm[0] * local_ctm[3] - local_ctm[1] * local_ctm[2])\n                metrics[\"image_area\"] += det\n                continue\n            if subtype == \"/Form\":\n                form_key = getattr(xobj, \"indirect_reference\", None) or id(xobj)\n                if form_key in visited:\n                    continue\n                visited.add(form_key)\n                form_matrix = _normalize_matrix(xobj.get(\"/Matrix\"))\n                form_ctm = _matrix_multiply(local_ctm, form_matrix)\n                form_resources = xobj.get(\"/Resources\") or resources\n                if hasattr(form_resources, \"get_object\"):\n                    try:\n                        form_resources = form_resources.get_object()\n                    except Exception:\n                        pass\n                _scan_content_stream(\n                    xobj,\n                    metrics,\n                    form_ctm,\n                    form_resources,\n                    reader,\n                    render_mode,\n                    visited,\n                )\n                visited.remove(form_key)\n\n\ndef _analyze_pdf_page_content(page: Any, reader: Any) -> Dict[str, Any]:\n    metrics = {\n        \"text_ops\": 0,\n        \"invisible_text_ops\": 0,\n        \"image_area\": 0.0,\n        \"image_count\": 0,\n        \"image_coverage\": 0.0,\n        \"invisible_text_ratio\": 0.0,\n    }\n    try:\n        page_width = float(page.mediabox.width)\n        page_height = float(page.mediabox.height)\n    except Exception:\n        page_width = 0.0\n        page_height = 0.0\n    page_area = page_width * page_height if page_width > 0 and page_height > 0 else 0.0\n    try:\n        contents = page.get_contents()\n        if not contents:\n            return metrics\n    except Exception:\n        return metrics\n    resources = page.get(\"/Resources\") or {}\n    visited: Set[Any] = set()\n    _scan_content_stream(\n        contents,\n        metrics,\n        (1.0, 0.0, 0.0, 1.0, 0.0, 0.0),\n        resources,\n        reader,\n        0,\n        visited,\n    )\n\n    if metrics[\"text_ops\"] > 0:\n        metrics[\"invisible_text_ratio\"] = metrics[\"invisible_text_ops\"] / metrics[\"text_ops\"]\n    if page_area > 0:\n        metrics[\"image_coverage\"] = min(1.0, metrics[\"image_area\"] / page_area)\n    return metrics\n\n\ndef classify_pdf_text_layer(\n    pdf_path: str,\n    config: DoclingProcessingConfig,\n) -> Optional[Dict[str, Any]]:\n    if not config.quality_classifier_enable:\n        return None\n    try:\n        from pypdf import PdfReader\n    except Exception:\n        return None\n    try:\n        reader = PdfReader(pdf_path)\n    except Exception as exc:\n        LOGGER.warning(\"Text-layer classifier failed to read PDF: %s\", exc)\n        return None\n    total_pages = len(reader.pages)\n    if total_pages <= 0:\n        return None\n\n    sample_max = max(1, int(config.quality_classifier_max_pages))\n    sample_count = min(total_pages, sample_max)\n    seed_payload = f\"{pdf_path}:{os.path.getsize(pdf_path)}\".encode(\"utf-8\")\n    seed_bytes = hashlib.sha1(seed_payload).digest()\n    rng = random.Random(int.from_bytes(seed_bytes[:8], \"big\"))\n    sample_indices = select_classifier_sample_indices(total_pages, sample_count, rng)\n\n    min_samples = max(1, min(int(config.quality_classifier_min_samples), sample_count))\n    decision_ratio = max(0.5, min(1.0, float(config.quality_classifier_decision_ratio)))\n    image_threshold = float(config.quality_classifier_image_coverage_threshold)\n    invisible_threshold = float(config.quality_classifier_invisible_text_ratio_threshold)\n    min_text_ops = max(1, int(config.quality_classifier_min_text_ops))\n    min_text_len = max(1, int(config.min_text_chars_per_page))\n\n    digital_count = 0\n    ocr_count = 0\n    mixed_count = 0\n    ocr_score_sum = 0.0\n    digital_score_sum = 0.0\n    seen = 0\n    decision = None\n    short_circuit = False\n\n    for idx in sample_indices:\n        page = reader.pages[idx]\n        try:\n            text = page.extract_text() or \"\"\n        except Exception:\n            text = \"\"\n        text_len = len(normalize_text(text))\n        metrics = _analyze_pdf_page_content(page, reader)\n        image_coverage = float(metrics.get(\"image_coverage\") or 0.0)\n        text_ops = int(metrics.get(\"text_ops\") or 0)\n        invisible_ratio = float(metrics.get(\"invisible_text_ratio\") or 0.0)\n\n        text_ops_factor = min(1.0, text_ops / max(1.0, float(min_text_ops)))\n        image_score = 0.0\n        if image_threshold < 1.0:\n            image_score = (image_coverage - image_threshold) / max(1e-6, 1.0 - image_threshold)\n            image_score = max(0.0, min(1.0, image_score))\n        invisible_score = 0.0\n        if invisible_threshold < 1.0:\n            invisible_score = (invisible_ratio - invisible_threshold) / max(1e-6, 1.0 - invisible_threshold)\n            invisible_score = max(0.0, min(1.0, invisible_score))\n        text_score = 1.0 - min(1.0, text_len / max(1.0, min_text_len * 2.0))\n        text_score = max(text_score, 1.0 - text_ops_factor)\n\n        ocr_score = max(image_score, invisible_score)\n        ocr_score = min(1.0, (ocr_score * 0.85) + (text_score * 0.15))\n        text_factor = min(1.0, text_len / max(1.0, min_text_len * 2.0))\n        text_factor *= text_ops_factor\n        digital_score = text_factor * (1.0 - image_score) * (1.0 - invisible_score)\n\n        ocr_score_sum += ocr_score\n        digital_score_sum += digital_score\n\n        if ocr_score >= decision_ratio and ocr_score >= digital_score:\n            label = \"ocr\"\n        elif digital_score >= decision_ratio and digital_score >= ocr_score:\n            label = \"digital\"\n        else:\n            label = \"mixed\"\n\n        seen += 1\n        if label == \"ocr\":\n            ocr_count += 1\n        elif label == \"digital\":\n            digital_count += 1\n        else:\n            mixed_count += 1\n\n        if seen >= min_samples:\n            avg_ocr = ocr_score_sum / seen\n            avg_digital = digital_score_sum / seen\n            if avg_ocr >= decision_ratio and avg_ocr >= avg_digital:\n                decision = \"ocr\"\n                short_circuit = True\n                break\n            if avg_digital >= decision_ratio and avg_digital >= avg_ocr:\n                decision = \"digital\"\n                short_circuit = True\n                break\n\n    if seen == 0:\n        return None\n    ocr_ratio = ocr_score_sum / seen\n    digital_ratio = digital_score_sum / seen\n    mixed_ratio = mixed_count / seen\n    if decision is None:\n        if ocr_ratio >= decision_ratio and ocr_ratio >= digital_ratio:\n            decision = \"ocr\"\n        elif digital_ratio >= decision_ratio and digital_ratio >= ocr_ratio:\n            decision = \"digital\"\n        else:\n            decision = \"mixed\"\n\n    return {\n        \"decision\": decision,\n        \"ocr_ratio\": ocr_ratio,\n        \"digital_ratio\": digital_ratio,\n        \"mixed_ratio\": mixed_ratio,\n        \"sampled_pages\": seen,\n        \"short_circuit\": short_circuit,\n    }\n\n\ndef compute_effective_confidence(\n    quality: TextQuality,\n    config: DoclingProcessingConfig,\n) -> float:\n    score = float(quality.confidence_proxy)\n    if quality.ocr_overlay_ratio is not None:\n        ocr_ratio = max(0.0, min(1.0, float(quality.ocr_overlay_ratio)))\n        score = (score * 0.6) + ((1.0 - ocr_ratio) * 0.4)\n    if quality.digital_page_ratio is not None:\n        digital_ratio = float(quality.digital_page_ratio)\n        digital_weight = 0.7\n        if quality.image_page_ratio is not None:\n            threshold = max(0.0, min(1.0, float(config.quality_image_page_ratio_threshold)))\n            if threshold < 1.0 and quality.image_page_ratio >= threshold:\n                guard = 1.0 - (float(quality.image_page_ratio) - threshold) / max(1e-6, 1.0 - threshold)\n                digital_weight *= max(0.0, min(1.0, guard))\n        boosted = (score * (1.0 - digital_weight)) + (digital_ratio * digital_weight)\n        score = max(score, boosted)\n    return max(0.0, min(1.0, score))\n\n\ndef apply_text_layer_classifier(\n    quality: TextQuality,\n    pdf_path: str,\n    config: DoclingProcessingConfig,\n) -> Tuple[TextQuality, Optional[Dict[str, Any]]]:\n    classifier = classify_pdf_text_layer(pdf_path, config)\n    if not classifier:\n        quality.effective_confidence_proxy = compute_effective_confidence(quality, config)\n        return quality, None\n    ocr_ratio = classifier.get(\"ocr_ratio\")\n    digital_ratio = classifier.get(\"digital_ratio\")\n    decision = classifier.get(\"decision\")\n    guardrail_applied = False\n    ocr_ratio_value = float(ocr_ratio) if ocr_ratio is not None else 0.0\n    digital_ratio_value = float(digital_ratio) if digital_ratio is not None else 0.0\n    if quality.image_page_ratio is not None:\n        threshold = max(0.0, min(1.0, float(config.quality_image_page_ratio_threshold)))\n        if threshold < 1.0 and quality.image_page_ratio >= threshold:\n            guard = 1.0 - (float(quality.image_page_ratio) - threshold) / max(1e-6, 1.0 - threshold)\n            digital_ratio_value *= max(0.0, min(1.0, guard))\n            ocr_ratio_value = max(ocr_ratio_value, float(quality.image_page_ratio))\n            guardrail_applied = True\n    if decision == \"digital\" and digital_ratio_value < config.quality_classifier_decision_ratio:\n        decision = \"mixed\"\n    if decision in (None, \"mixed\") and ocr_ratio_value >= config.quality_classifier_decision_ratio:\n        if digital_ratio_value < config.quality_classifier_decision_ratio:\n            decision = \"ocr\"\n    quality.ocr_overlay_ratio = ocr_ratio_value\n    quality.digital_page_ratio = digital_ratio_value\n    quality.layer_classification = decision\n    if guardrail_applied:\n        classifier[\"ocr_ratio\"] = ocr_ratio_value\n        classifier[\"digital_ratio\"] = digital_ratio_value\n        classifier[\"decision\"] = decision\n        classifier[\"guardrail_applied\"] = True\n        classifier[\"short_circuit\"] = False\n    quality.effective_confidence_proxy = compute_effective_confidence(quality, config)\n    return quality, classifier\n\n\ndef normalize_ocr_confidence(value: Any) -> Optional[float]:\n    if value is None:\n        return None\n    try:\n        conf = float(value)\n    except Exception:\n        return None\n    if conf < 0:\n        return None\n    if conf > 1.0:\n        conf = conf / 100.0\n    return max(0.0, min(1.0, conf))\n\n\ndef estimate_text_quality(\n    pages: Sequence[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    languages: Optional[str] = None,\n) -> TextQuality:\n    if not pages:\n        return TextQuality(0.0, 0.0, 1.0, 0.0, None)\n\n    texts = [str(page.get(\"text\", \"\")) for page in pages]\n    total_chars = sum(len(text) for text in texts)\n    alpha_chars = sum(sum(char.isalpha() for char in text) for text in texts)\n    alpha_ratio = alpha_chars / max(1, total_chars)\n\n    tokens = extract_alnum_tokens(\" \".join(texts))\n    suspicious_tokens = [\n        token for token in tokens\n        if (sum(char.isdigit() for char in token) / max(1, len(token))) > 0.5\n        or re.search(r\"(.)\\1\\1\", token)\n    ]\n    suspicious_ratio = len(suspicious_tokens) / max(1, len(tokens))\n\n    avg_chars = total_chars / max(1, len(pages))\n    dictionary_hit_ratio = None\n    spellchecker_hit_ratio = None\n    image_heavy_ratio = None\n    image_page_ratio = None\n    if config and config.quality_use_wordfreq and languages:\n        dictionary_hit_ratio = compute_dictionary_hit_ratio(\n            tokens,\n            languages,\n            config.quality_wordfreq_min_zipf,\n        )\n    if config and languages:\n        spellchecker_hit_ratio = compute_spellchecker_hit_ratio(tokens, languages, config)\n        image_heavy_ratio = compute_image_heavy_ratio(pages, config)\n    if config:\n        image_page_ratio = compute_image_page_ratio(pages)\n    lexicon_ratio = None\n    if dictionary_hit_ratio is not None and spellchecker_hit_ratio is not None:\n        lexicon_ratio = max(dictionary_hit_ratio, spellchecker_hit_ratio)\n    elif dictionary_hit_ratio is not None:\n        lexicon_ratio = dictionary_hit_ratio\n    elif spellchecker_hit_ratio is not None:\n        lexicon_ratio = spellchecker_hit_ratio\n    confidence = alpha_ratio * (1.0 - suspicious_ratio)\n    if lexicon_ratio is not None:\n        confidence *= 0.4 + (0.6 * lexicon_ratio)\n    if (\n        image_heavy_ratio is not None\n        and config\n        and image_heavy_ratio >= config.quality_image_heavy_ratio_threshold\n    ):\n        penalty = 1.0 - (config.quality_image_heavy_penalty * image_heavy_ratio)\n        confidence *= max(0.0, penalty)\n    confidence = max(0.0, min(1.0, confidence))\n    return TextQuality(\n        avg_chars,\n        alpha_ratio,\n        suspicious_ratio,\n        confidence,\n        dictionary_hit_ratio,\n        spellchecker_hit_ratio,\n        image_heavy_ratio,\n        image_page_ratio,\n    )\n\n\ndef detect_text_layer_from_pages(pages: Sequence[Dict[str, Any]], config: DoclingProcessingConfig) -> bool:\n    if not pages:\n        return False\n    pages_with_text = 0\n    for page in pages:\n        cleaned = normalize_text(str(page.get(\"text\", \"\")))\n        if len(cleaned) >= config.min_text_chars_per_page:\n            pages_with_text += 1\n    ratio = pages_with_text / max(1, len(pages))\n    return ratio >= config.min_text_pages_ratio\n\n\ndef is_low_quality(quality: TextQuality, config: DoclingProcessingConfig) -> bool:\n    effective_confidence = (\n        quality.effective_confidence_proxy\n        if quality.effective_confidence_proxy is not None\n        else quality.confidence_proxy\n    )\n    return effective_confidence < config.quality_confidence_threshold\n\n\ndef should_rasterize_text_layer(has_text_layer: bool, low_quality: bool, config: DoclingProcessingConfig) -> bool:\n    if config.ocr_mode == \"force\":\n        return True\n    return bool(has_text_layer and low_quality and config.force_ocr_on_low_quality_text)\n\n\ndef decide_per_page_ocr(\n    has_text_layer: bool,\n    quality: TextQuality,\n    config: DoclingProcessingConfig,\n) -> Tuple[bool, str]:\n    if config.force_per_page_ocr:\n        return True, \"Per-page OCR forced by config\"\n    if not config.per_page_ocr_on_low_quality:\n        return False, \"Per-page OCR disabled by config\"\n    if not has_text_layer and is_low_quality(quality, config):\n        return True, \"Low-quality scan detected\"\n    if quality.suspicious_token_ratio > config.quality_suspicious_token_threshold:\n        return True, \"High suspicious token ratio\"\n    if quality.avg_chars_per_page < config.quality_min_avg_chars_per_page:\n        return True, \"Low text density\"\n    return False, \"Quality metrics acceptable\"\n\n\ndef select_language_set(\n    language_hint: Optional[str],\n    filename: str,\n    config: DoclingProcessingConfig,\n) -> str:\n    hint = (language_hint or \"\").lower().strip()\n    name = os.path.basename(filename).lower()\n\n    # import langcodes\n\n    def normalize_hint(h: str) -> str:\n        if not h:\n            return \"\"\n        try:\n            lang = langcodes.find(h)\n            code = lang.to_alpha3()\n            if code == \"deu\":\n                return config.default_lang_german\n            if code == \"eng\":\n                return config.default_lang_english\n            if code == \"fra\":\n                return \"fra+eng\"  # French + English fallback\n            if code == \"pol\":\n                return \"pol+eng\"  # Polish + English fallback\n            if code == \"ita\":\n                return \"ita+eng\"  # Italian + English fallback\n            if code == \"spa\":\n                return \"spa+eng\"  # Spanish + English fallback\n            if code == \"por\":\n                return \"por+eng\"  # Portuguese + English fallback\n            if code == \"nld\" or code == \"dut\":\n                return \"nld+eng\"  # Dutch + English fallback\n            if code == \"swe\":\n                return \"swe+eng\"  # Swedish + English fallback\n            if code == \"nor\":\n                return \"nor+eng\"  # Norwegian + English fallback\n            if code == \"dan\":\n                return \"dan+eng\"  # Danish + English fallback\n            if code == \"fin\":\n                return \"fin+eng\"  # Finnish + English fallback\n            if code == \"rus\":\n                return \"rus+eng\"  # Russian + English fallback\n            if code == \"ces\" or code == \"cze\":\n                return \"ces+eng\"  # Czech + English fallback\n            if code == \"ell\" or code == \"gre\":\n                return \"ell+eng\"  # Greek + English fallback\n            # Add more as needed\n            return code\n        except Exception:\n            return h\n\n    if hint:\n        return normalize_hint(hint)\n\n    # Try to infer from filename using langcodes\n    for pattern, lang_code in [\n        (r\"(\\bde\\b|_de\\b|-de\\b|deu|german|deutsch)\", config.default_lang_german),\n        (r\"(\\bfr\\b|_fr\\b|-fr\\b|fra|french|francais|français)\", \"fra+eng\"),\n        (r\"(\\bit\\b|_it\\b|-it\\b|ita|italian|italiano)\", \"ita+eng\"),\n        (r\"(\\bes\\b|_es\\b|-es\\b|spa|spanish|espanol|español)\", \"spa+eng\"),\n        (r\"(\\bpl\\b|_pl\\b|-pl\\b|pol|polish|polski)\", \"pol+eng\"),\n        (r\"(\\bpt\\b|_pt\\b|-pt\\b|por|portuguese|português|portugues)\", \"por+eng\"),\n        (r\"(\\bnl\\b|_nl\\b|-nl\\b|nld|dut|dutch|nederlands)\", \"nld+eng\"),\n        (r\"(\\bsv\\b|_sv\\b|-sv\\b|swe|swedish|svenska)\", \"swe+eng\"),\n        (r\"(\\bno\\b|_no\\b|-no\\b|nor|norsk|bokmal|bokmål|nynorsk)\", \"nor+eng\"),\n        (r\"(\\bda\\b|_da\\b|-da\\b|dan|danish|dansk)\", \"dan+eng\"),\n        (r\"(\\bfi\\b|_fi\\b|-fi\\b|fin|finnish|suomi)\", \"fin+eng\"),\n        (r\"(\\bru\\b|_ru\\b|-ru\\b|rus|russian|рус)\", \"rus+eng\"),\n        (r\"(\\bcs\\b|_cs\\b|-cs\\b|ces|cze|czech|čeština|cesky|česky)\", \"ces+eng\"),\n        (r\"(\\bel\\b|_el\\b|-el\\b|ell|greek|ελληνικά)\", \"ell+eng\"),\n    ]:\n        if re.search(pattern, name):\n            return lang_code\n    return config.default_lang_english\n\n\ndef normalize_languages_for_engine(languages: str, engine: str) -> str:\n    lang = languages.lower()\n    if engine == \"paddle\":\n        # PaddleOCR expects ISO 639-1 or specific language names (e.g., 'german', 'french', etc.)\n        try:\n            # Use the first language if multiple are given\n            first_lang = lang.split('+')[0].strip()\n            code = langcodes.find(first_lang)\n            paddle_map = {\n                \"de\": \"german\",\n                \"deu\": \"german\",\n                \"fr\": \"french\",\n                \"fra\": \"french\",\n                \"en\": \"en\",\n                \"eng\": \"en\",\n                \"it\": \"italian\",\n                \"ita\": \"italian\",\n                \"es\": \"spanish\",\n                \"spa\": \"spanish\",\n                \"pl\": \"polish\",\n                \"pol\": \"polish\",\n                \"pt\": \"portuguese\",\n                \"por\": \"portuguese\",\n                \"ru\": \"russian\",\n                \"rus\": \"russian\",\n            }\n            alpha2 = code.to_alpha2()\n            alpha3 = code.to_alpha3()\n            if alpha2 in paddle_map:\n                return paddle_map[alpha2]\n            if alpha3 in paddle_map:\n                return paddle_map[alpha3]\n        except Exception:\n            return \"en\"\n        return \"en\"\n    return languages\n\n\ndef get_pdf_max_page_points(pdf_path: str, max_pages: int = 3) -> Optional[float]:\n    try:\n        from pypdf import PdfReader\n    except Exception:\n        return None\n    try:\n        reader = PdfReader(pdf_path)\n        max_side = 0.0\n        total_pages = len(reader.pages)\n        sample_count = min(max_pages, total_pages)\n        for idx in range(sample_count):\n            page = reader.pages[idx]\n            width = float(page.mediabox.width)\n            height = float(page.mediabox.height)\n            max_side = max(max_side, width, height)\n        return max_side or None\n    except Exception:\n        return None\n\n\ndef decide_ocr_route(\n    has_text_layer: bool,\n    quality: TextQuality,\n    available_engines: Sequence[str],\n    config: DoclingProcessingConfig,\n    languages: str,\n) -> OcrRouteDecision:\n    low_quality = is_low_quality(quality, config)\n    force_external_for_paddle_layout = bool(\n        config.prefer_ocr_engine == \"paddle\"\n        and (\n            getattr(config, \"paddle_use_paddlex_layout\", False)\n            or getattr(config, \"paddle_use_vl\", False)\n        )\n        and config.ocr_mode != \"off\"\n        and (config.ocr_mode == \"force\" or not has_text_layer or low_quality)\n    )\n    if config.ocr_mode == \"off\":\n        return OcrRouteDecision(\n            False,\n            \"none\",\n            languages,\n            \"OCR disabled by config\",\n            False,\n            False,\n            \"Per-page OCR disabled by config\",\n        )\n\n    if config.ocr_mode == \"force\":\n        ocr_used = True\n        route_reason = \"OCR forced by config\"\n    elif has_text_layer and not (config.force_ocr_on_low_quality_text and low_quality) and not force_external_for_paddle_layout:\n        return OcrRouteDecision(\n            False,\n            \"none\",\n            languages,\n            \"Text layer detected\",\n            False,\n            False,\n            \"Per-page OCR not applicable (text layer)\",\n        )\n    else:\n        ocr_used = True\n        if has_text_layer:\n            if force_external_for_paddle_layout:\n                route_reason = \"Text layer detected; external OCR forced for Paddle layout\"\n            else:\n                route_reason = \"Text layer detected but low quality\"\n        else:\n            route_reason = \"No usable text layer detected\"\n\n    engine = \"docling\"\n    use_external = False\n    if ocr_used:\n        if config.prefer_ocr_engine in available_engines:\n            engine = config.prefer_ocr_engine\n            use_external = True\n        elif config.fallback_ocr_engine in available_engines:\n            engine = config.fallback_ocr_engine\n            use_external = True\n\n    per_page = False\n    per_page_reason = \"Per-page OCR not applicable\"\n    if use_external:\n        per_page, per_page_reason = decide_per_page_ocr(has_text_layer, quality, config)\n    if low_quality and not has_text_layer:\n        route_reason = f\"{route_reason}; low-quality scan suspected\"\n\n    return OcrRouteDecision(ocr_used, engine, languages, route_reason, use_external, per_page, per_page_reason)\n\n\ndef detect_available_ocr_engines() -> List[str]:\n    available: List[str] = []\n    try:\n        import paddleocr  # noqa: F401\n        import paddle  # noqa: F401\n        from pdf2image import convert_from_path  # noqa: F401\n        available.append(\"paddle\")\n    except Exception:\n        pass\n    try:\n        import pytesseract  # noqa: F401\n        from pdf2image import convert_from_path  # noqa: F401\n        if find_tesseract_path():\n            available.append(\"tesseract\")\n    except Exception:\n        pass\n    return available\n\n\ndef load_default_wordlist(config: DoclingProcessingConfig) -> Sequence[str]:\n    path = config.dictionary_path\n    if not path:\n        path = os.path.join(os.path.dirname(__file__), config.default_dictionary_name)\n    if not path or not os.path.isfile(path):\n        return []\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as handle:\n            return [line.strip() for line in handle if line.strip() and not line.startswith(\"#\")]\n    except Exception as exc:\n        LOGGER.warning(\"Failed to load dictionary word list: %s\", exc)\n        return []\n\n\ndef prepare_dictionary_words(config: DoclingProcessingConfig) -> Sequence[str]:\n    if not config.enable_dictionary_correction:\n        return []\n    if config.dictionary_words:\n        return [word.strip() for word in config.dictionary_words if word and word.strip()]\n    words = load_default_wordlist(config)\n    if not words:\n        LOGGER.warning(\"Dictionary correction enabled but no wordlist was loaded.\")\n    return words\n\n\ndef build_spellchecker_for_languages(config: DoclingProcessingConfig, languages: str):\n    \"\"\"\n    Build a cross-platform spellchecker adapter with a .spell(word) method.\n    Tries:\n      1) hunspell (C binding) if available\n      2) spylls (pure Python) if available\n    Returns an object with .spell(str)->bool, or None if unavailable.\n    \"\"\"\n    if not config.enable_hunspell:\n        return None\n    cache_key = f\"{languages}|{config.hunspell_aff_path or ''}|{config.hunspell_dic_path or ''}\"\n    if cache_key in SPELLCHECKER_CACHE:\n        return SPELLCHECKER_CACHE[cache_key]\n\n    # Resolve aff/dic paths (explicit or auto in tools/hunspell)\n    def resolve_paths() -> List[Tuple[str, str]]:\n        pairs: List[Tuple[str, str]] = []\n        aff = config.hunspell_aff_path\n        dic = config.hunspell_dic_path\n        if aff and dic and os.path.isfile(aff) and os.path.isfile(dic):\n            pairs.append((aff, dic))\n            return pairs\n        base_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n        lang = (languages or \"\").lower()\n        try_codes: List[str] = []\n        if any(t in lang for t in (\"de\", \"deu\", \"german\", \"deutsch\")):\n            try_codes += [\"de_DE\", \"de_AT\", \"de_CH\"]\n        if any(t in lang for t in (\"en\", \"eng\", \"english\")):\n            try_codes += [\"en_US\", \"en_GB\"]\n        if not try_codes:\n            try_codes = [\"en_US\"]\n        # Exact matches first\n        for code in try_codes:\n            aff_path = os.path.join(base_dir, f\"{code}.aff\")\n            dic_path = os.path.join(base_dir, f\"{code}.dic\")\n            if os.path.isfile(aff_path) and os.path.isfile(dic_path):\n                pairs.append((aff_path, dic_path))\n        if pairs:\n            return pairs\n\n        # Flexible matching: accept stems like de_DE_frami.* or en_US-large.* when both files share the same stem\n        try:\n            names = os.listdir(base_dir)\n        except Exception:\n            names = []\n        stems_with_aff = {n[:-4] for n in names if n.endswith(\".aff\")}\n        stems_with_dic = {n[:-4] for n in names if n.endswith(\".dic\")}\n        common_stems = list(stems_with_aff & stems_with_dic)\n\n        def stem_priority(stem: str, code: str) -> int:\n            # Higher number = higher priority\n            if stem == code:\n                return 3\n            if stem.startswith(code + \"_\"):\n                return 2\n            if code in stem:\n                return 1\n            return 0\n\n        for code in try_codes:\n            candidates = sorted(\n                [s for s in common_stems if stem_priority(s, code) > 0],\n                key=lambda s: stem_priority(s, code),\n                reverse=True,\n            )\n            for stem in candidates:\n                aff_path = os.path.join(base_dir, f\"{stem}.aff\")\n                dic_path = os.path.join(base_dir, f\"{stem}.dic\")\n                if os.path.isfile(aff_path) and os.path.isfile(dic_path):\n                    pairs.append((aff_path, dic_path))\n                    break\n        return pairs\n\n\n    pairs = resolve_paths()\n    # If no pairs found, try to download on demand\n    if not pairs:\n        # Map special cases for repo structure\n        repo_map = {\n            \"de_DE\": (\"de\", \"de_DE_frami\"),\n            \"de_AT\": (\"de\", \"de_AT\"),\n            \"de_CH\": (\"de\", \"de_CH\"),\n            \"en_US\": (\"en\", \"en_US\"),\n            \"en_GB\": (\"en\", \"en_GB\"),\n            \"fr_FR\": (\"fr_FR\", \"fr\"),\n            \"es_ES\": (\"es\", \"es\"),\n            \"it_IT\": (\"it_IT\", \"it_IT\"),\n            \"pl_PL\": (\"pl_PL\", \"pl_PL\"),\n            \"pt_PT\": (\"pt_PT\", \"pt_PT\"),\n            \"pt_BR\": (\"pt_BR\", \"pt_BR\"),\n            \"nl_NL\": (\"nl_NL\", \"nl_NL\"),\n            \"sv_SE\": (\"sv_SE\", \"sv_SE\"),\n            \"da_DK\": (\"da_DK\", \"da_DK\"),\n            \"fi_FI\": (\"fi_FI\", \"fi_FI\"),\n            \"ru_RU\": (\"ru_RU\", \"ru_RU\"),\n            \"cs_CZ\": (\"cs_CZ\", \"cs_CZ\"),\n            \"el_GR\": (\"el_GR\", \"el_GR\"),\n        }\n        lang_code = None\n        lang = (languages or \"\").lower()\n        if any(t in lang for t in (\"de\", \"deu\", \"german\", \"deutsch\")):\n            lang_code = \"de_DE\"\n        elif any(t in lang for t in (\"en\", \"eng\", \"english\")):\n            lang_code = \"en_US\"\n        elif any(t in lang for t in (\"fr\", \"fra\", \"french\", \"francais\", \"français\")):\n            lang_code = \"fr_FR\"\n        elif any(t in lang for t in (\"es\", \"spa\", \"spanish\", \"espanol\", \"español\")):\n            lang_code = \"es_ES\"\n        elif any(t in lang for t in (\"it\", \"ita\", \"italian\", \"italiano\")):\n            lang_code = \"it_IT\"\n        elif any(t in lang for t in (\"pl\", \"pol\", \"polish\", \"polski\")):\n            lang_code = \"pl_PL\"\n        elif any(t in lang for t in (\"pt\", \"por\", \"portuguese\", \"português\", \"portugues\")):\n            lang_code = \"pt_PT\"\n        elif any(t in lang for t in (\"nl\", \"nld\", \"dut\", \"dutch\", \"nederlands\")):\n            lang_code = \"nl_NL\"\n        elif any(t in lang for t in (\"sv\", \"swe\", \"swedish\", \"svenska\")):\n            lang_code = \"sv_SE\"\n        elif any(t in lang for t in (\"da\", \"dan\", \"danish\", \"dansk\")):\n            lang_code = \"da_DK\"\n        elif any(t in lang for t in (\"fi\", \"fin\", \"finnish\", \"suomi\")):\n            lang_code = \"fi_FI\"\n        elif any(t in lang for t in (\"ru\", \"rus\", \"russian\", \"рус\")):\n            lang_code = \"ru_RU\"\n        elif any(t in lang for t in (\"cs\", \"ces\", \"cze\", \"czech\", \"čeština\", \"česky\", \"cesky\")):\n            lang_code = \"cs_CZ\"\n        elif any(t in lang for t in (\"el\", \"ell\", \"greek\", \"ελληνικά\")):\n            lang_code = \"el_GR\"\n        if not lang_code:\n            lang_code = \"en_US\"\n        folder, prefix = repo_map.get(lang_code, (lang_code, lang_code))\n        base_url = f\"https://raw.githubusercontent.com/LibreOffice/dictionaries/master/{folder}/\"\n        aff_name = f\"{prefix}.aff\"\n        dic_name = f\"{prefix}.dic\"\n        aff_url = base_url + aff_name\n        dic_url = base_url + dic_name\n        out_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n        os.makedirs(out_dir, exist_ok=True)\n        aff_path = os.path.join(out_dir, f\"{lang_code}.aff\")\n        dic_path = os.path.join(out_dir, f\"{lang_code}.dic\")\n        def download(url, out_path):\n            try:\n                import urllib.request\n                print(f\"Downloading {url} -> {out_path}\")\n                urllib.request.urlretrieve(url, out_path)\n                return True\n            except Exception as exc:\n                print(f\"Failed to download {url}: {exc}\")\n                return False\n        ok_aff = download(aff_url, aff_path)\n        ok_dic = download(dic_url, dic_path)\n        if ok_aff and ok_dic:\n            print(f\"Successfully downloaded Hunspell dictionary for {lang_code} to {out_dir}\")\n        # Try to resolve again\n        pairs = resolve_paths()\n\n    # Attempt hunspell binding first\n    try:\n        import hunspell  # type: ignore\n\n        for aff_path, dic_path in pairs:\n            try:\n                hs = hunspell.HunSpell(dic_path, aff_path)\n                LOGGER.info(\n                    \"Spellchecker: using hunspell binding (%s, %s)\",\n                    os.path.basename(dic_path),\n                    os.path.basename(aff_path),\n                )\n                try:\n                    # Record details for external visibility\n                    LAST_SPELLCHECKER_INFO.update({\n                        \"backend\": \"hunspell\",\n                        \"dic\": dic_path,\n                        \"aff\": aff_path,\n                    })\n                except Exception:\n                    pass\n                SPELLCHECKER_CACHE[cache_key] = hs\n                return hs\n            except Exception:\n                continue\n    except Exception:\n        pass\n\n    # Attempt spylls fallback (pure Python)\n    try:\n        from spylls.hunspell import Dictionary as SpyllsDictionary  # type: ignore\n\n        class SpyllsWrapper:\n            def __init__(self, d):\n                self.d = d\n\n            def spell(self, word: str) -> bool:\n                # Try common case variants to recognize lowercased nouns etc.\n                variants = [word, word.lower(), word.capitalize(), word.title(), word.upper()]\n                seen = set()\n                for v in variants:\n                    if v in seen:\n                        continue\n                    seen.add(v)\n                    try:\n                        if hasattr(self.d, \"lookup\") and self.d.lookup(v):\n                            return True\n                    except Exception:\n                        pass\n                    try:\n                        sugg = self.d.suggest(v)\n                        if isinstance(sugg, (list, tuple)) and v in sugg:\n                            return True\n                    except Exception:\n                        pass\n                return False\n\n        for aff_path, dic_path in pairs:\n            try:\n                d = None\n                errors: List[str] = []\n                # Variant A: (aff, dic)\n                try:\n                    d = SpyllsDictionary.from_files(aff_path, dic_path)\n                except Exception as eA:\n                    errors.append(f\"A(aff,dic): {eA}\")\n                # Variant B: directory containing both\n                if d is None:\n                    try:\n                        d = SpyllsDictionary.from_files(os.path.dirname(dic_path))\n                    except Exception as eB:\n                        errors.append(f\"B(dir): {eB}\")\n                # Variant C: stem without extension\n                if d is None:\n                    try:\n                        stem = os.path.splitext(dic_path)[0]\n                        d = SpyllsDictionary.from_files(stem)\n                    except Exception as eC:\n                        errors.append(f\"C(stem): {eC}\")\n                # Variant D: single-path dic\n                if d is None:\n                    try:\n                        d = SpyllsDictionary.from_files(dic_path)\n                    except Exception as eD:\n                        errors.append(f\"D(dic): {eD}\")\n                # Variant E: single-path aff\n                if d is None:\n                    try:\n                        d = SpyllsDictionary.from_files(aff_path)\n                    except Exception as eE:\n                        errors.append(f\"E(aff): {eE}\")\n\n                if d is None:\n                    raise RuntimeError(\"spylls load failed: \" + \"; \".join(errors))\n\n                LOGGER.info(\n                    \"Spellchecker: using spylls fallback (%s, %s)\",\n                    os.path.basename(dic_path),\n                    os.path.basename(aff_path),\n                )\n                try:\n                    LAST_SPELLCHECKER_INFO.update({\n                        \"backend\": \"spylls\",\n                        \"dic\": dic_path,\n                        \"aff\": aff_path,\n                    })\n                except Exception:\n                    pass\n                wrapper = SpyllsWrapper(d)\n                SPELLCHECKER_CACHE[cache_key] = wrapper\n                return wrapper\n            except Exception:\n                continue\n    except Exception:\n        pass\n\n    # Naive .dic fallback (no affix rules) when hunspell/spylls are unavailable\n    try:\n        class NaiveDicWrapper:\n            def __init__(self, words: Sequence[str]):\n                self.words = set(w.lower() for w in words if w)\n\n            def spell(self, word: str) -> bool:\n                variants = [word, word.lower(), word.capitalize(), word.title(), word.upper()]\n                for v in variants:\n                    if v.lower() in self.words:\n                        return True\n                return False\n\n        def load_naive_dic(path: str) -> Optional[NaiveDicWrapper]:\n            try:\n                entries: List[str] = []\n                with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n                    first = True\n                    for raw in fh:\n                        line = raw.strip().lstrip(\"\\ufeff\")\n                        if not line:\n                            continue\n                        if first and line.isdigit():\n                            first = False\n                            continue\n                        first = False\n                        base = line.split(\"/\")[0].strip()\n                        if base:\n                            entries.append(base)\n                if entries:\n                    LOGGER.info(\"Spellchecker: using naive .dic (%s) entries=%d\", os.path.basename(path), len(entries))\n                    return NaiveDicWrapper(entries)\n            except Exception as exc:\n                LOGGER.warning(\"Naive .dic load failed for %s: %s\", path, exc)\n            return None\n\n        # Prefer .dic paths discovered via resolve_paths(); otherwise scan tools/hunspell\n        dic_paths: List[str] = []\n        for _aff, _dic in pairs:\n            if os.path.isfile(_dic):\n                dic_paths.append(_dic)\n        if not dic_paths:\n            base_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n            try:\n                candidates = [os.path.join(base_dir, name) for name in os.listdir(base_dir) if name.endswith(\".dic\")]\n            except Exception:\n                candidates = []\n            lang = (languages or \"\").lower()\n            filtered: List[str] = []\n            for p in candidates:\n                name = os.path.basename(p).lower()\n                if (\"en\" in lang or \"eng\" in lang) and (name.startswith(\"en_\") or name.startswith(\"en\")):\n                    filtered.append(p)\n                if (\"de\" in lang or \"deu\" in lang or \"german\" in lang or \"deutsch\" in lang) and (name.startswith(\"de_\") or name.startswith(\"de\")):\n                    filtered.append(p)\n            dic_paths = filtered or candidates\n\n        for dic_path in dic_paths:\n            wrapper = load_naive_dic(dic_path)\n            if wrapper is not None:\n                SPELLCHECKER_CACHE[cache_key] = wrapper\n                return wrapper\n    except Exception:\n        pass\n\n    LOGGER.info(\"Spellchecker: no hunspell/spylls dictionary available\")\n    try:\n        LAST_SPELLCHECKER_INFO.update({\"backend\": \"none\"})\n    except Exception:\n        pass\n    return None\n\n\ndef apply_dictionary_correction(text: str, wordlist: Sequence[str], hs=None) -> str:\n    if not wordlist:\n        # If Hunspell available, do a minimal pass using it only\n        if hs is None:\n            return text\n        dictionary = set()\n    else:\n        dictionary = {word.lower() for word in wordlist}\n    token_re = re.compile(r\"[A-Za-z0-9]+\")\n\n    def match_case(candidate: str, original: str) -> str:\n        if original.isupper():\n            return candidate.upper()\n        if original[:1].isupper():\n            return candidate.capitalize()\n        return candidate\n\n    def generate_candidates(token: str) -> Iterable[str]:\n        candidates: List[str] = []\n        if any(char.isdigit() for char in token) and any(char.isalpha() for char in token):\n            candidates.append(token.replace(\"0\", \"o\"))\n            candidates.append(token.replace(\"1\", \"l\"))\n            candidates.append(token.replace(\"5\", \"s\"))\n        if \"rn\" in token:\n            candidates.append(token.replace(\"rn\", \"m\"))\n        return candidates\n\n    def replace_token(match: re.Match) -> str:\n        token = match.group(0)\n        lower = token.lower()\n        if lower in dictionary or (hs is not None and hs.spell(token)):\n            return token\n        for candidate in generate_candidates(token):\n            cand_lower = candidate.lower()\n            if cand_lower in dictionary or (hs is not None and hs.spell(candidate)):\n                replaced = match_case(candidate, token)\n                try:\n                    LOGGER.info(\"Dict correction: %s -> %s\", token, replaced)\n                except Exception:\n                    pass\n                return replaced\n        return token\n\n    return token_re.sub(replace_token, text)\n\n\ndef apply_umlaut_corrections(text: str, languages: str, wordlist: Sequence[str], hs=None) -> str:\n    \"\"\"\n    Convert ASCII digraphs ae/oe/ue to German umlauts ä/ö/ü more comprehensively.\n\n    Strategy:\n    - If a dictionary is provided, prefer candidates that appear in it.\n    - Otherwise, use word frequency (wordfreq.zipf_frequency) for German to\n      select candidates whose frequency noticeably exceeds the original.\n    - Preserve original casing (UPPER, Title, lower).\n    - Only operate when language is German.\n    - Keep conservative: if no strong signal, leave token unchanged.\n    \"\"\"\n    lang = (languages or \"\").lower()\n    if not any(token in lang for token in (\"de\", \"deu\", \"german\", \"deutsch\")):\n        return text\n\n    dictionary = {word.lower() for word in (wordlist or [])}\n\n    try:\n        from wordfreq import zipf_frequency as _zipf\n    except Exception:\n        _zipf = None  # wordfreq optional\n\n    ascii_to_umlaut = ((\"ae\", \"\\u00e4\"), (\"oe\", \"\\u00f6\"), (\"ue\", \"\\u00fc\"))\n\n    def case_match(candidate: str, original: str) -> str:\n        if original.isupper():\n            return candidate.upper()\n        if original[:1].isupper() and original[1:].islower():\n            return candidate.capitalize()\n        return candidate\n\n    def generate_variants(token_lower: str) -> List[str]:\n        # Generate all unique variants by replacing any subset of ae/oe/ue occurrences\n        indices: List[Tuple[int, str, str]] = []\n        for ascii_seq, uml in ascii_to_umlaut:\n            start = 0\n            while True:\n                idx = token_lower.find(ascii_seq, start)\n                if idx == -1:\n                    break\n                # Heuristic: avoid replacing \"ue\" when preceded by 'e' (e.g., \"neue\", \"Treue\")\n                if ascii_seq == \"ue\" and idx > 0 and token_lower[idx - 1] == \"e\":\n                    pass\n                else:\n                    indices.append((idx, ascii_seq, uml))\n                start = idx + 1 if idx != -1 else start\n\n        if not indices:\n            return []\n\n        # Build combinations\n        variants = {token_lower}\n        for idx, ascii_seq, uml in indices:\n            new_set = set()\n            for base in variants:\n                # Replace at the same position if still matching\n                if base[idx:idx + len(ascii_seq)] == ascii_seq:\n                    new_set.add(base[:idx] + uml + base[idx + len(ascii_seq):])\n                new_set.add(base)\n            variants = new_set\n        return [v for v in variants if v != token_lower]\n\n    def pick_best(token: str) -> str:\n        lower = token.lower()\n        # Quick path: if already contains umlaut, skip\n        if any(ch in lower for ch in (\"ä\", \"ö\", \"ü\")):\n            return token\n\n        # Generate candidate variants\n        candidates = generate_variants(lower)\n        if not candidates:\n            return token\n\n        # Score candidates\n        best = None\n        best_score = float(\"-inf\")\n        # Base frequency for original\n        base_freq = _zipf(lower, \"de\") if _zipf else 0.0\n        for cand in candidates:\n            score = 0.0\n            if cand in dictionary or (hs is not None and hs.spell(cand)):\n                score += 10.0  # strong signal from dictionary\n            if _zipf:\n                freq = _zipf(cand, \"de\")\n                # Prefer if notably more frequent than original\n                score += (freq - base_freq)\n            # Prefer shorter (umlaut variant shortens by 1 char per replacement)\n            score += (len(lower) - len(cand)) * 0.05\n            if score > best_score:\n                best = cand\n                best_score = score\n\n        # Acceptance threshold: either in dictionary or frequency improved by >= 0.5\n        accept = False\n        if best is not None:\n            if best in dictionary or (hs is not None and hs.spell(best)):\n                accept = True\n            elif _zipf:\n                if (_zipf(best, \"de\") - base_freq) >= 0.5:\n                    accept = True\n\n        if not accept or not best:\n            return token\n        replaced = case_match(best, token)\n        try:\n            LOGGER.info(\"Umlaut correction: %s -> %s\", token, replaced)\n        except Exception:\n            pass\n        return replaced\n\n    # Replace word tokens conservatively (length >= 4 to avoid short codes)\n    return re.sub(r\"[A-Za-zÄÖÜäöüß]{4,}\", lambda m: pick_best(m.group(0)), text)\n\n\ndef restore_missing_spaces(text: str, languages: str, hs=None) -> str:\n    \"\"\"\n    Conservatively insert spaces inside overlong tokens when a split yields two\n    valid words (by Hunspell/Splylls or by wordfreq Zipf >= 3.0 for target langs).\n\n    Heuristics:\n    - Consider tokens of length >= 12 with only letters (incl. German chars).\n    - Prefer camelCase boundaries (a…zA…Z) when both sides are valid.\n    - Otherwise, try a single split; accept only if BOTH parts look valid.\n    - Log accepted splits.\n    \"\"\"\n    try:\n        from wordfreq import zipf_frequency as _zipf\n    except Exception:\n        _zipf = None\n\n    lang_codes = select_wordfreq_languages(languages)\n\n    def score_word(w: str) -> Tuple[float, bool]:\n        spelled = False\n        try:\n            if hs is not None and (hs.spell(w) or hs.spell(w.lower())):\n                spelled = True\n        except Exception:\n            pass\n        if spelled:\n            return 4.0, True\n        if _zipf is None:\n            return 0.0, False\n        try:\n            z = max(_zipf(w.lower(), lc) for lc in lang_codes)\n        except Exception:\n            z = 0.0\n        return float(z), False\n\n    token_re = re.compile(r\"[A-Za-zÄÖÜäöüß]{12,}\")\n\n    def consider_split(tok: str) -> str:\n        base_score, base_dict = score_word(tok)\n        if base_dict or base_score >= 3.0:\n            return tok\n        best = None  # type: Optional[Tuple[str, float, bool, str, float, bool]]\n\n        # Try camelCase boundary first: a…zA…Z\n        for m in re.finditer(r\"([a-zäöüß])([A-ZÄÖÜ])\", tok):\n            i = m.start(2)\n            left, right = tok[:i], tok[i:]\n            if len(left) < 3 or len(right) < 3:\n                continue\n            s1, d1 = score_word(left)\n            s2, d2 = score_word(right)\n            if (d1 or s1 >= 3.0) and (d2 or s2 >= 3.0):\n                combined = s1 + s2\n                best = (left, s1, d1, right, s2, d2)\n                break\n\n        # Otherwise, try single split positions\n        if best is None:\n            n = len(tok)\n            for i in range(3, n - 2):\n                left, right = tok[:i], tok[i:]\n                if len(left) < 3 or len(right) < 3:\n                    continue\n                s1, d1 = score_word(left)\n                s2, d2 = score_word(right)\n                if (d1 or s1 >= 3.0) and (d2 or s2 >= 3.0):\n                    combined = s1 + s2\n                    if best is None or combined > (best[1] + best[4]):\n                        best = (left, s1, d1, right, s2, d2)\n\n        if best is None:\n            return tok\n\n        left, s1, d1, right, s2, d2 = best\n        replacement = f\"{left} {right}\"\n        try:\n            LOGGER.info(\n                \"Inserted space: %s -> %s (scores=%.2f/%.2f, dict=%s/%s)\",\n                tok,\n                replacement,\n                s1,\n                s2,\n                d1,\n                d2,\n            )\n        except Exception:\n            pass\n        return replacement\n\n    return token_re.sub(lambda m: consider_split(m.group(0)), text)\n\n\ndef should_apply_llm_correction(text: str, config: DoclingProcessingConfig) -> bool:\n    if not config.enable_llm_correction:\n        return False\n    if not config.llm_correct:\n        return False\n    if config.llm_correction_max_chars and len(text) > config.llm_correction_max_chars:\n        return False\n    languages = select_language_set(config.language_hint, \"\", config)\n    quality = estimate_text_quality([{\"text\": text}], config, languages)\n    return quality.confidence_proxy < config.llm_correction_min_quality\n\n\ndef build_llm_cleanup_callback(config: DoclingProcessingConfig) -> Optional[Callable[[str], str]]:\n    if not config.enable_llm_correction:\n        return None\n    if not config.llm_cleanup_base_url or not config.llm_cleanup_model:\n        LOGGER.warning(\"LLM cleanup enabled but base URL or model is missing.\")\n        return None\n\n    base_url = config.llm_cleanup_base_url.rstrip(\"/\")\n    endpoint = f\"{base_url}/chat/completions\"\n    api_key = (config.llm_cleanup_api_key or \"\").strip()\n\n    def _requires_default_temperature(model_name: str) -> bool:\n        name = (model_name or \"\").lower()\n        return \"gpt-5\" in name or name.startswith(\"gpt5\")\n\n    def _call(text: str) -> str:\n        try:\n            import requests\n        except Exception as exc:\n            LOGGER.warning(\"requests not available for LLM cleanup: %s\", exc)\n            return text\n\n        headers = {\"Content-Type\": \"application/json\"}\n        if api_key:\n            headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n        payload = {\n            \"model\": config.llm_cleanup_model,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are an OCR cleanup assistant. Fix OCR errors without changing meaning. \"\n                        \"Do not add content. Return corrected text only.\"\n                        \"Detect footnote references and definitions and format them in Markdown as [^n] and [^n]: (for the note text). Preserve special characters and formatting. Do not create new footnotes or content; only reformat existing footnote markers/lines.\"\n                    ),\n                },\n                {\"role\": \"user\", \"content\": text},\n            ],\n        }\n        if not _requires_default_temperature(config.llm_cleanup_model) or config.llm_cleanup_temperature == 1.0:\n            payload[\"temperature\"] = config.llm_cleanup_temperature\n        try:\n            response = requests.post(endpoint, headers=headers, json=payload, timeout=config.llm_cleanup_timeout_sec)\n            response.raise_for_status()\n            data = response.json()\n            content = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\")\n            if content:\n                return str(content).strip()\n        except Exception as exc:\n            LOGGER.warning(\"LLM cleanup failed: %s\", exc)\n        return text\n\n    return _call\n\n\ndef postprocess_text(\n    text: str,\n    config: DoclingProcessingConfig,\n    languages: str,\n    wordlist: Sequence[str],\n    allow_missing_space: bool = True,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_label: Optional[str] = None,\n) -> str:\n    if not text:\n        return text\n    cleaned = dehyphenate_text(text)\n    cleaned = replace_ligatures(cleaned)\n    cleaned = normalize_whitespace(cleaned)\n    hs = build_spellchecker_for_languages(config, languages) if config.enable_hunspell else None\n    try:\n        from wordfreq import zipf_frequency as _zipf\n    except Exception:\n        _zipf = None\n    lang_codes = select_wordfreq_languages(languages)\n\n    dictionary = {word.lower() for word in (wordlist or [])}\n\n    def is_valid_word(word: str) -> bool:\n        lower = word.lower()\n        if lower in dictionary:\n            return True\n        if hs is not None and (hs.spell(word) or hs.spell(lower)):\n            return True\n        if _zipf is not None:\n            try:\n                return max(_zipf(lower, lc) for lc in lang_codes) >= 3.0\n            except Exception:\n                return False\n        return False\n\n    def match_case(candidate: str, original: str) -> str:\n        if original.isupper():\n            return candidate.upper()\n        if original[:1].isupper():\n            return candidate.capitalize()\n        return candidate\n\n    def merge_broken_words(input_text: str) -> str:\n        token_re = re.compile(r\"\\b([A-Za-zÄÖÜäöüß]{2,})\\s+([A-Za-zÄÖÜäöüß]{2,})\\b\")\n\n        def repl(match: re.Match) -> str:\n            w1 = match.group(1)\n            w2 = match.group(2)\n            combined = w1 + w2\n            if len(combined) < 5:\n                return match.group(0)\n            if not is_valid_word(combined):\n                return match.group(0)\n            w1_ok = is_valid_word(w1)\n            w2_ok = is_valid_word(w2)\n            if w1_ok and w2_ok:\n                return match.group(0)\n            return match_case(combined, w1)\n\n        prev = input_text\n        for _ in range(2):\n            updated = token_re.sub(repl, prev)\n            if updated == prev:\n                break\n            prev = updated\n        return prev\n    # Attempt to restore missing spaces before word-level corrections\n    if allow_missing_space:\n        try:\n            restored = restore_missing_spaces(cleaned, languages, hs)\n            if restored != cleaned:\n                LOGGER.info(\"Applied missing-space restoration pass\")\n            cleaned = restored\n        except Exception as exc:\n            LOGGER.warning(\"Missing-space restoration failed: %s\", exc)\n\n    def split_concatenated_words(input_text: str) -> str:\n        token_re = re.compile(r\"[A-Za-zÄÖÜäöüß]{6,}\")\n        has_caps_re = re.compile(r\"[a-zäöüß][A-ZÄÖÜ]\")\n\n        def score_word(word: str) -> Tuple[float, bool]:\n            spelled = False\n            try:\n                if hs is not None and (hs.spell(word) or hs.spell(word.lower())):\n                    spelled = True\n            except Exception:\n                pass\n            if spelled:\n                return 5.0, True\n            if _zipf is None:\n                return 0.0, False\n            try:\n                z = max(_zipf(word.lower(), lc) for lc in lang_codes)\n            except Exception:\n                z = 0.0\n            return float(z), False\n\n        def is_strong_word(word: str) -> bool:\n            score, spelled = score_word(word)\n            return spelled or score >= 4.0\n\n        def repl(match: re.Match) -> str:\n            tok = match.group(0)\n            base_score, base_dict = score_word(tok)\n            if base_dict or base_score >= 3.0:\n                return tok\n            if len(tok) < 10 and not has_caps_re.search(tok):\n                return tok\n\n            best = None  # type: Optional[Tuple[str, float, str, float]]\n            for i in range(3, len(tok) - 2):\n                left, right = tok[:i], tok[i:]\n                if len(left) < 3 or len(right) < 3:\n                    continue\n                if not (is_strong_word(left) and is_strong_word(right)):\n                    continue\n                s1, _ = score_word(left)\n                s2, _ = score_word(right)\n                combined = s1 + s2\n                if best is None or combined > (best[1] + best[3]):\n                    best = (left, s1, right, s2)\n\n            if best is None:\n                return tok\n            left, s1, right, s2 = best\n            if _zipf is not None and (s1 + s2) - base_score < 3.0:\n                return tok\n            try:\n                LOGGER.info(\n                    \"Split concat: %s -> %s %s (scores=%.2f/%.2f base=%.2f)\",\n                    tok,\n                    left,\n                    right,\n                    s1,\n                    s2,\n                    base_score,\n                )\n            except Exception:\n                pass\n            return f\"{left} {right}\"\n\n        return token_re.sub(repl, input_text)\n\n    cleaned = split_concatenated_words(cleaned)\n    cleaned = merge_broken_words(cleaned)\n    if config.enable_dictionary_correction or hs is not None:\n        cleaned = apply_dictionary_correction(cleaned, wordlist, hs)\n    cleaned = apply_umlaut_corrections(cleaned, languages, wordlist, hs)\n    if should_apply_llm_correction(cleaned, config) and config.llm_correct:\n        if progress_cb:\n            label = f\"LLM cleanup ({progress_label})\" if progress_label else \"LLM cleanup...\"\n            progress_cb(100, \"llm_cleanup\", label)\n        cleaned = config.llm_correct(cleaned)\n    return cleaned\n\ndef postprocess_text_light(\n    text: str,\n    config: DoclingProcessingConfig,\n    languages: str,\n    wordlist: Sequence[str],\n    for_markdown: bool = False,\n) -> str:\n    if not text:\n        return text\n    cleaned = dehyphenate_text(text)\n    cleaned = replace_ligatures(cleaned)\n    cleaned = normalize_display_markdown(cleaned) if for_markdown else normalize_whitespace(cleaned)\n    hs = build_spellchecker_for_languages(config, languages) if config.enable_hunspell else None\n    if config.enable_dictionary_correction or hs is not None:\n        cleaned = apply_dictionary_correction(cleaned, wordlist, hs)\n    cleaned = apply_umlaut_corrections(cleaned, languages, wordlist, hs)\n    if should_apply_llm_correction(cleaned, config) and config.llm_correct:\n        LOGGER.info(\"LLM cleanup applied (light mode)\")\n        cleaned = config.llm_correct(cleaned)\n    return cleaned\n\ndef export_markdown(doc: Any) -> str:\n    for method_name in (\"export_to_markdown\", \"to_markdown\", \"export_to_md\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    for method_name in (\"export_to_text\", \"to_text\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    return str(doc)\n\n\ndef export_text(doc: Any) -> str:\n    for method_name in (\"export_to_text\", \"to_text\"):\n        method = getattr(doc, method_name, None)\n        if callable(method):\n            return method()\n    return str(doc)\n\n\ndef extract_pages(doc: Any) -> List[Dict[str, Any]]:\n    pages: List[Dict[str, Any]] = []\n    pages_attr = getattr(doc, \"pages\", None)\n    if pages_attr is not None and not isinstance(pages_attr, (str, bytes, dict)):\n        try:\n            pages_list = list(pages_attr)\n        except TypeError:\n            pages_list = []\n        if pages_list:\n            for idx, page in enumerate(pages_list, start=1):\n                page_num = getattr(page, \"page_number\", None) or getattr(page, \"number\", None) or idx\n                text = None\n                for attr in (\"markdown\", \"md\", \"text\", \"content\"):\n                    if hasattr(page, attr):\n                        value = getattr(page, attr)\n                        text = value() if callable(value) else value\n                        break\n                if text is None and hasattr(page, \"export_to_text\"):\n                    text = page.export_to_text()\n                if text is None:\n                    text = str(page)\n                pages.append({\"page_num\": int(page_num), \"text\": str(text)})\n            return pages\n\n    full_text = export_text(doc)\n    if full_text:\n        pages.append({\"page_num\": 1, \"text\": full_text})\n    return pages\n\n\ndef select_analysis_page_indices(\n    total_pages: int,\n    max_pages: Optional[int],\n    sample_strategy: str,\n) -> List[int]:\n    if total_pages <= 0:\n        return []\n    if not max_pages or max_pages <= 0 or total_pages <= max_pages:\n        return list(range(1, total_pages + 1))\n\n    strategy = (sample_strategy or \"first\").lower()\n    if strategy == \"middle\":\n        start = max(1, (total_pages - max_pages) // 2 + 1)\n        end = min(total_pages, start + max_pages - 1)\n        return list(range(start, end + 1))\n    return list(range(1, max_pages + 1))\n\n\ndef select_classifier_sample_indices(\n    total_pages: int,\n    sample_count: int,\n    rng: random.Random,\n) -> List[int]:\n    if total_pages <= 0 or sample_count <= 0:\n        return []\n    anchors: List[int] = [0]\n    if total_pages > 1:\n        anchors.append(total_pages - 1)\n    if total_pages > 2 and sample_count >= 3:\n        mid = (total_pages - 1) // 2\n        if mid not in anchors:\n            anchors.insert(1, mid)\n    if sample_count <= len(anchors):\n        return anchors[:sample_count]\n    selected = list(anchors)\n    remaining = [idx for idx in range(total_pages) if idx not in anchors]\n    needed = min(sample_count - len(selected), len(remaining))\n    if needed > 0:\n        selected.extend(rng.sample(remaining, needed))\n    rng.shuffle(selected)\n    return selected\n\n\ndef extract_pages_from_pdf(\n    pdf_path: str,\n    max_pages: Optional[int] = None,\n    sample_strategy: str = \"first\",\n) -> List[Dict[str, Any]]:\n    try:\n        from pypdf import PdfReader\n    except Exception as exc:\n        eprint(f\"pypdf is not available for fallback page extraction: {exc}\")\n        return []\n\n    pages: List[Dict[str, Any]] = []\n    try:\n        reader = PdfReader(pdf_path)\n        page_indices = select_analysis_page_indices(len(reader.pages), max_pages, sample_strategy)\n        for idx in page_indices:\n            page = reader.pages[idx - 1]\n            try:\n                text = page.extract_text() or \"\"\n            except Exception:\n                text = \"\"\n            image_count = 0\n            try:\n                resources = page.get(\"/Resources\") or {}\n                x_objects = resources.get(\"/XObject\")\n                if x_objects:\n                    x_objects = x_objects.get_object() if hasattr(x_objects, \"get_object\") else x_objects\n                    for obj in x_objects.values():\n                        try:\n                            resolved = obj.get_object() if hasattr(obj, \"get_object\") else obj\n                            if resolved.get(\"/Subtype\") == \"/Image\":\n                                image_count += 1\n                        except Exception:\n                            continue\n            except Exception:\n                image_count = 0\n            pages.append({\"page_num\": idx, \"text\": text, \"image_count\": image_count})\n    except Exception as exc:\n        eprint(f\"Failed to extract pages with pypdf: {exc}\")\n        return []\n\n    return pages\n\n\ndef split_markdown_sections(markdown: str) -> List[Dict[str, Any]]:\n    sections: List[Dict[str, Any]] = []\n    current_title = \"\"\n    current_heading = \"\"\n    current_lines: List[str] = []\n\n    def flush() -> None:\n        nonlocal current_title, current_heading, current_lines\n        if current_title or current_heading or current_lines:\n            sections.append({\n                \"title\": current_title.strip(),\n                \"heading\": current_heading.strip(),\n                \"text\": \"\\n\".join(current_lines).strip(),\n            })\n        current_title = \"\"\n        current_heading = \"\"\n        current_lines = []\n\n    for line in markdown.splitlines():\n        if line.startswith(\"#\"):\n            flush()\n            current_heading = line.rstrip()\n            current_title = line.lstrip(\"#\").strip()\n        else:\n            current_lines.append(line)\n\n    flush()\n    return sections\n\n\n_MARKDOWN_TABLE_SEP_RE = re.compile(r\"^\\s*\\|?\\s*:?-{2,}:?(?:\\s*\\|\\s*:?-{2,}:?)+\\s*\\|?\\s*$\")\n\n\ndef extract_markdown_table_blocks(markdown: str) -> List[str]:\n    if not markdown:\n        return []\n    lines = markdown.splitlines()\n    blocks: List[str] = []\n    idx = 0\n    while idx < len(lines) - 1:\n        line = lines[idx]\n        if line.count(\"|\") >= 2:\n            sep_idx: Optional[int] = None\n            if idx + 1 < len(lines) and _MARKDOWN_TABLE_SEP_RE.match(lines[idx + 1]):\n                sep_idx = idx + 1\n            elif (\n                idx + 2 < len(lines)\n                and not lines[idx + 1].strip()\n                and _MARKDOWN_TABLE_SEP_RE.match(lines[idx + 2])\n            ):\n                sep_idx = idx + 2\n\n            if sep_idx is not None:\n                block_lines = [line, lines[sep_idx]]\n                idx = sep_idx + 1\n                while idx < len(lines):\n                    row = lines[idx]\n                    if not row.strip():\n                        break\n                    if row.count(\"|\") < 2:\n                        break\n                    block_lines.append(row)\n                    idx += 1\n                blocks.append(\"\\n\".join(block_lines).strip())\n                continue\n\n            # Fallback: headerless pipe tables (3+ consecutive pipe rows)\n            if idx + 2 < len(lines):\n                pipe_run = [line]\n                scan = idx + 1\n                while scan < len(lines):\n                    row = lines[scan]\n                    if not row.strip() or row.count(\"|\") < 2:\n                        break\n                    pipe_run.append(row)\n                    scan += 1\n                if len(pipe_run) >= 3:\n                    blocks.append(\"\\n\".join(pipe_run).strip())\n                    idx = scan\n                    continue\n        idx += 1\n    return blocks\n\n\ndef build_page_table_map(\n    markdown: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n) -> Dict[int, List[str]]:\n    table_blocks = extract_markdown_table_blocks(markdown)\n    if not table_blocks:\n        return {}\n    table_map: Dict[int, List[str]] = {}\n    for block in table_blocks:\n        page_start, _ = find_page_range(block, pages, config)\n        if page_start <= 0:\n            continue\n        table_map.setdefault(int(page_start), []).append(block)\n    return table_map\n\n\ndef inject_markdown_tables(text: str, table_blocks: Sequence[str]) -> str:\n    if not text or not table_blocks:\n        return text\n    if any(line.count(\"|\") >= 2 for line in text.splitlines()):\n        return text\n    row_regexes: List[re.Pattern[str]] = []\n    for block in table_blocks:\n        for line in block.splitlines():\n            if _MARKDOWN_TABLE_SEP_RE.match(line):\n                continue\n            if line.count(\"|\") < 2:\n                continue\n            cells = [cell.strip() for cell in line.split(\"|\") if cell.strip()]\n            if len(cells) < 2:\n                continue\n            pattern = r\"\\b\" + r\"\\b.*\\b\".join(re.escape(cell) for cell in cells) + r\"\\b\"\n            row_regexes.append(re.compile(pattern, re.IGNORECASE))\n    if row_regexes:\n        kept_lines: List[str] = []\n        for line in text.splitlines():\n            if any(regex.search(line) for regex in row_regexes):\n                continue\n            kept_lines.append(line)\n        text = \"\\n\".join(kept_lines).strip()\n    for block in table_blocks:\n        if block and block not in text:\n            text = f\"{text}\\n\\n{block}\".strip()\n    return text\n\n\n_PAGE_RANGE_STOPWORDS_EN = {\n    \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"into\", \"over\",\n    \"under\", \"after\", \"before\", \"were\", \"was\", \"are\", \"is\", \"its\", \"their\",\n    \"then\", \"than\", \"than\", \"which\", \"when\", \"where\", \"have\", \"has\", \"had\",\n    \"into\", \"onto\", \"upon\", \"your\", \"yours\", \"they\", \"them\", \"these\", \"those\",\n    \"will\", \"would\", \"could\", \"should\", \"about\", \"there\", \"here\", \"while\",\n    \"what\", \"why\", \"how\", \"not\", \"but\", \"you\", \"your\", \"our\", \"ours\", \"his\",\n    \"her\", \"she\", \"him\", \"she\", \"him\", \"its\", \"also\", \"such\", \"been\", \"being\",\n    \"out\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n    \"nine\", \"ten\", \"more\", \"most\", \"some\", \"many\", \"few\", \"each\", \"per\",\n}\n\n_PAGE_RANGE_STOPWORDS_DE = {\n    \"der\", \"die\", \"das\", \"und\", \"oder\", \"aber\", \"nicht\", \"ist\", \"sind\",\n    \"war\", \"waren\", \"mit\", \"für\", \"von\", \"zu\", \"im\", \"in\", \"auf\", \"an\",\n    \"als\", \"auch\", \"wie\", \"dass\", \"dem\", \"den\", \"des\", \"ein\", \"eine\",\n    \"einer\", \"eines\", \"einem\", \"einen\", \"ich\", \"du\", \"er\", \"sie\", \"es\",\n    \"wir\", \"ihr\", \"ihnen\", \"sein\", \"haben\", \"hat\", \"hatte\", \"hatten\",\n    \"wird\", \"werden\", \"kann\", \"können\", \"soll\", \"sollen\", \"diese\",\n    \"dieser\", \"dieses\", \"jeder\", \"jede\", \"jedes\", \"mehr\", \"weniger\",\n}\n\n_PAGE_RANGE_STOPWORDS_FR = {\n    \"le\", \"la\", \"les\", \"de\", \"des\", \"du\", \"un\", \"une\", \"et\", \"ou\",\n    \"mais\", \"ne\", \"pas\", \"est\", \"sont\", \"été\", \"être\", \"avec\", \"pour\",\n    \"par\", \"sur\", \"dans\", \"ce\", \"ces\", \"cette\", \"son\", \"sa\", \"ses\",\n    \"leur\", \"leurs\", \"comme\", \"qui\", \"que\", \"quoi\", \"dont\", \"où\",\n    \"au\", \"aux\", \"plus\", \"moins\", \"se\", \"s\", \"il\", \"elle\", \"ils\",\n    \"elles\", \"nous\", \"vous\", \"je\", \"tu\",\n}\n\n_PAGE_RANGE_STOPWORDS_ES = {\n    \"el\", \"la\", \"los\", \"las\", \"de\", \"del\", \"y\", \"o\", \"pero\", \"no\",\n    \"es\", \"son\", \"fue\", \"fueron\", \"con\", \"para\", \"por\", \"en\", \"un\",\n    \"una\", \"unos\", \"unas\", \"su\", \"sus\", \"como\", \"que\", \"qué\", \"quien\",\n    \"quién\", \"donde\", \"dónde\", \"cuando\", \"cuándo\", \"más\", \"menos\",\n    \"al\", \"lo\", \"se\", \"si\", \"sí\", \"yo\", \"tú\", \"él\", \"ella\", \"ellos\",\n    \"ellas\", \"nosotros\", \"vosotros\", \"usted\", \"ustedes\",\n}\n\n_PAGE_RANGE_STOPWORDS_PL = {\n    \"i\", \"oraz\", \"a\", \"ale\", \"nie\", \"jest\", \"są\", \"był\", \"była\",\n    \"było\", \"byli\", \"były\", \"z\", \"ze\", \"do\", \"na\", \"w\", \"we\", \"o\",\n    \"od\", \"po\", \"przez\", \"dla\", \"u\", \"za\", \"pod\", \"nad\", \"między\",\n    \"się\", \"to\", \"ten\", \"ta\", \"te\", \"jego\", \"jej\", \"ich\", \"nas\",\n    \"was\", \"ja\", \"ty\", \"on\", \"ona\", \"oni\", \"one\", \"że\", \"jak\",\n    \"kiedy\", \"gdzie\", \"dlaczego\", \"który\", \"która\", \"które\", \"których\",\n    \"którym\", \"może\", \"można\", \"będzie\", \"będą\", \"być\", \"by\",\n}\n\n\ndef get_page_range_stopwords(languages: str) -> Set[str]:\n    stopwordsiso = None\n    try:\n        import stopwordsiso  # type: ignore\n    except Exception:\n        stopwordsiso = None\n\n    lang = (languages or \"\").lower()\n    selected: Set[str] = set()\n    tokens = [token for token in re.split(r\"[+,\\s]+\", lang) if token]\n\n    if stopwordsiso is not None:\n        available = None\n        for attr in (\"available_languages\", \"languages\", \"available\"):\n            getter = getattr(stopwordsiso, attr, None)\n            if callable(getter):\n                try:\n                    available = set(getter())\n                    break\n                except Exception:\n                    available = None\n        for token in tokens:\n            codes: List[str] = []\n            try:\n                parsed = langcodes.find(token)\n                alpha2 = parsed.to_alpha2()\n                alpha3 = parsed.to_alpha3()\n                if alpha2:\n                    codes.append(alpha2)\n                if alpha3:\n                    codes.append(alpha3)\n            except Exception:\n                codes.append(token)\n            for code in codes:\n                if available is not None and code not in available:\n                    continue\n                try:\n                    selected |= set(stopwordsiso.stopwords(code))\n                except Exception:\n                    continue\n        if not selected and (available is None or \"en\" in available):\n            try:\n                selected |= set(stopwordsiso.stopwords(\"en\"))\n            except Exception:\n                pass\n\n    if not selected:\n        if any(token in lang for token in (\"de\", \"deu\", \"german\", \"deutsch\")):\n            selected |= _PAGE_RANGE_STOPWORDS_DE\n        if any(token in lang for token in (\"fr\", \"fra\", \"french\", \"francais\", \"français\")):\n            selected |= _PAGE_RANGE_STOPWORDS_FR\n        if any(token in lang for token in (\"es\", \"spa\", \"spanish\", \"espanol\", \"español\")):\n            selected |= _PAGE_RANGE_STOPWORDS_ES\n        if any(token in lang for token in (\"pl\", \"pol\", \"polish\", \"polski\")):\n            selected |= _PAGE_RANGE_STOPWORDS_PL\n        if not selected or any(token in lang for token in (\"en\", \"eng\", \"english\")):\n            selected |= _PAGE_RANGE_STOPWORDS_EN\n\n    return selected\n\ndef tokenize_for_page_range(text: str, stopwords: Optional[Set[str]] = None) -> List[str]:\n    tokens = re.findall(r\"[A-Za-z0-9]{3,}\", text.lower())\n    if not stopwords:\n        stopwords = _PAGE_RANGE_STOPWORDS_EN\n    return [token for token in tokens if token not in stopwords]\n\n\ndef sample_tokens(tokens: Sequence[str], max_tokens: int) -> List[str]:\n    if max_tokens <= 0 or len(tokens) <= max_tokens:\n        return list(tokens)\n    step = max(1, len(tokens) // max_tokens)\n    return list(tokens[::step])\n\n\ndef compute_page_overlap(\n    section_text: str,\n    pages: List[Dict[str, Any]],\n    config: DoclingProcessingConfig,\n    languages: Optional[str] = None,\n) -> List[Tuple[float, int, int]]:\n    stopwords = get_page_range_stopwords(languages or \"\")\n    section_tokens = tokenize_for_page_range(section_text, stopwords)\n    if not section_tokens:\n        return []\n    sample = sample_tokens(section_tokens, config.page_range_sample_tokens)\n    sample_set = set(sample)\n    total = len(sample_set)\n    results: List[Tuple[float, int, int]] = []\n    for page in pages:\n        page_text = str(page.get(\"text\", \"\"))\n        page_tokens = set(tokenize_for_page_range(page_text, stopwords))\n        hits = len(sample_set & page_tokens)\n        ratio = hits / max(1, total)\n        results.append((ratio, hits, int(page.get(\"page_num\", 0))))\n    return results\n\n\ndef select_overlap_cluster(\n    overlap_scores: Sequence[Tuple[float, int, int]],\n    config: DoclingProcessingConfig,\n) -> List[int]:\n    if not overlap_scores:\n        return []\n    max_ratio = max(score[0] for score in overlap_scores)\n    max_hits = max(score[1] for score in overlap_scores)\n    ratio_cutoff = max(config.page_range_min_overlap, max_ratio * config.page_range_peak_ratio)\n    hits_cutoff = max(config.page_range_min_hits, int(max_hits * config.page_range_peak_ratio))\n    candidates = [\n        (ratio, hits, page_num)\n        for ratio, hits, page_num in overlap_scores\n        if ratio >= ratio_cutoff or hits >= hits_cutoff\n    ]\n    if not candidates:\n        candidates = sorted(overlap_scores, reverse=True)[: config.page_range_top_k]\n\n    candidates.sort(key=lambda item: item[2])\n    clusters: List[List[Tuple[float, int, int]]] = []\n    current: List[Tuple[float, int, int]] = []\n    for entry in candidates:\n        if not current:\n            current.append(entry)\n            continue\n        if entry[2] - current[-1][2] <= config.page_range_cluster_gap:\n            current.append(entry)\n        else:\n            clusters.append(current)\n            current = [entry]\n    if current:\n        clusters.append(current)\n\n    def cluster_score(cluster: Sequence[Tuple[float, int, int]]) -> Tuple[float, float]:\n        ratios = [item[0] for item in cluster]\n        return (sum(ratios), max(ratios))\n\n    best_cluster = max(clusters, key=cluster_score)\n    page_nums = [item[2] for item in best_cluster]\n    if len(page_nums) > 1:\n        span_ratio = (max(page_nums) - min(page_nums) + 1) / max(1, len(overlap_scores))\n        if span_ratio > config.page_range_max_span_ratio:\n            trimmed = sorted(best_cluster, reverse=True)[: config.page_range_top_k]\n            page_nums = [item[2] for item in trimmed]\n    return page_nums\n\n\ndef find_page_range(\n    section_text: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n) -> Tuple[int, int]:\n    if not pages:\n        return 0, 0\n\n    cleaned = normalize_text(section_text)\n    if not cleaned:\n        return 0, 0\n\n    snippet_start = cleaned[:200]\n    snippet_end = cleaned[-200:]\n\n    page_start = 0\n    page_end = 0\n\n    for page in pages:\n        page_clean = normalize_text(page.get(\"text\", \"\"))\n        if snippet_start and snippet_start in page_clean:\n            page_start = page.get(\"page_num\", 0)\n            break\n\n    for page in reversed(pages):\n        page_clean = normalize_text(page.get(\"text\", \"\"))\n        if snippet_end and snippet_end in page_clean:\n            page_end = page.get(\"page_num\", 0)\n            break\n\n    if page_start == 0 or page_end == 0:\n        config = config or DoclingProcessingConfig()\n        languages = select_language_set(config.language_hint, \"\", config)\n        overlap_scores = compute_page_overlap(cleaned, pages, config, languages)\n        page_nums = select_overlap_cluster(overlap_scores, config)\n        if page_nums:\n            if page_start == 0:\n                page_start = min(page_nums)\n            if page_end == 0:\n                page_end = max(page_nums)\n\n    if page_start == 0:\n        page_start = pages[0].get(\"page_num\", 0)\n    if page_end == 0:\n        page_end = pages[-1].get(\"page_num\", 0)\n\n    return int(page_start), int(page_end)\n\n\ndef slugify(text: str) -> str:\n    slug = re.sub(r\"[^a-z0-9]+\", \"-\", text.lower()).strip(\"-\")\n    return slug\n\n\ndef configure_layout_options(pipeline_options: Any) -> None:\n    if hasattr(pipeline_options, \"layout_mode\"):\n        pipeline_options.layout_mode = \"accurate\"\n    if hasattr(pipeline_options, \"detect_layout\"):\n        pipeline_options.detect_layout = True\n    if hasattr(pipeline_options, \"extract_tables\"):\n        pipeline_options.extract_tables = True\n    if hasattr(pipeline_options, \"table_structure\"):\n        pipeline_options.table_structure = True\n    layout_options = getattr(pipeline_options, \"layout_options\", None)\n    if layout_options is not None:\n        for name, value in (\n            (\"detect_columns\", True),\n            (\"detect_tables\", True),\n            (\"enable_table_structure\", True),\n            (\"max_columns\", 3),\n        ):\n            if hasattr(layout_options, name):\n                setattr(layout_options, name, value)\n\n\ndef build_converter(config: DoclingProcessingConfig, decision: OcrRouteDecision):\n    from docling.document_converter import DocumentConverter\n\n    try:\n        from docling.datamodel.base_models import InputFormat\n        from docling.datamodel.pipeline_options import PdfPipelineOptions, OCRMode\n        from docling.document_converter import PdfFormatOption\n    except Exception:\n        return DocumentConverter()\n\n    pipeline_options = PdfPipelineOptions()\n    if not decision.ocr_used:\n        if hasattr(pipeline_options, \"do_ocr\"):\n            pipeline_options.do_ocr = False\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.DISABLED\n    elif config.ocr_mode == \"force\":\n        if hasattr(pipeline_options, \"do_ocr\"):\n            pipeline_options.do_ocr = True\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.FORCE\n    else:\n        if hasattr(pipeline_options, \"ocr_mode\"):\n            pipeline_options.ocr_mode = OCRMode.AUTO\n\n    if decision.ocr_used:\n        if hasattr(pipeline_options, \"ocr_engine\"):\n            pipeline_options.ocr_engine = decision.ocr_engine\n        if hasattr(pipeline_options, \"ocr_languages\"):\n            pipeline_options.ocr_languages = decision.languages\n        if hasattr(pipeline_options, \"ocr_lang\"):\n            pipeline_options.ocr_lang = decision.languages\n\n    configure_layout_options(pipeline_options)\n\n    format_options = {InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n    return DocumentConverter(format_options=format_options)\n\n\ndef find_poppler_path() -> Optional[str]:\n    env_path = os.environ.get(\"POPPLER_PATH\")\n    if env_path and os.path.isfile(os.path.join(env_path, \"pdftoppm\")):\n        return env_path\n    pdftoppm = shutil.which(\"pdftoppm\")\n    if pdftoppm:\n        return os.path.dirname(pdftoppm)\n    for candidate in (\"/opt/homebrew/bin\", \"/usr/local/bin\", \"/usr/bin\"):\n        if os.path.isfile(os.path.join(candidate, \"pdftoppm\")):\n            return candidate\n    return None\n\n\nPOPPLER_LOGGED_ONCE = False\n\n\ndef render_pdf_pages(pdf_path: str, dpi: int) -> List[Any]:\n    from pdf2image import convert_from_path\n\n    poppler_path = find_poppler_path()\n    if poppler_path:\n        global POPPLER_LOGGED_ONCE\n        if shutil.which(\"pdftoppm\") is None and not POPPLER_LOGGED_ONCE:\n            LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n            POPPLER_LOGGED_ONCE = True\n        return convert_from_path(pdf_path, dpi=dpi, poppler_path=poppler_path)\n    return convert_from_path(pdf_path, dpi=dpi)\n\n\ndef render_pdf_pages_sample(pdf_path: str, dpi: int, max_pages: int) -> List[Any]:\n    from pdf2image import convert_from_path\n\n    if max_pages <= 0:\n        return []\n    poppler_path = find_poppler_path()\n    kwargs = {\"dpi\": dpi, \"first_page\": 1, \"last_page\": max_pages}\n    if poppler_path:\n        global POPPLER_LOGGED_ONCE\n        if shutil.which(\"pdftoppm\") is None and not POPPLER_LOGGED_ONCE:\n            LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n            POPPLER_LOGGED_ONCE = True\n        kwargs[\"poppler_path\"] = poppler_path\n    return convert_from_path(pdf_path, **kwargs)\n\n\ndef get_pdf_page_count(pdf_path: str) -> int:\n    \"\"\"Return total number of pages using pypdf (fast and light).\"\"\"\n    try:\n        from pypdf import PdfReader  # type: ignore\n        reader = PdfReader(pdf_path)\n        return int(len(reader.pages))\n    except Exception:\n        return 0\n\n\ndef select_column_sample_indices(total_pages: int, max_pages: int) -> List[int]:\n    \"\"\"Pick up to max_pages page indices spread across the document (1-based).\"\"\"\n    if total_pages <= 0:\n        return []\n    k = max(1, max_pages)\n    k = min(k, total_pages)\n    if k == 1:\n        return [max(1, (total_pages + 1) // 2)]\n    if k == 2:\n        return [1, total_pages]\n    # Spread evenly including first and last\n    step = (total_pages - 1) / (k - 1)\n    return [int(round(1 + i * step)) for i in range(k)]\n\n\ndef render_pdf_pages_at_indices(pdf_path: str, dpi: int, indices: Sequence[int]) -> List[Any]:\n    \"\"\"Render specific 1-based page indices to images. May call pdf2image multiple times.\"\"\"\n    from pdf2image import convert_from_path\n    images: List[Any] = []\n    if not indices:\n        return images\n    poppler_path = find_poppler_path()\n    for idx in indices:\n        kwargs = {\"dpi\": dpi, \"first_page\": int(idx), \"last_page\": int(idx)}\n        if poppler_path:\n            global POPPLER_LOGGED_ONCE\n            if shutil.which(\"pdftoppm\") is None and not POPPLER_LOGGED_ONCE:\n                LOGGER.info(\"Poppler not on PATH; using %s\", poppler_path)\n                POPPLER_LOGGED_ONCE = True\n            kwargs[\"poppler_path\"] = poppler_path\n        try:\n            imgs = convert_from_path(pdf_path, **kwargs)\n            if imgs:\n                images.append(imgs[0])\n        except Exception:\n            continue\n    return images\n\n\ndef compute_column_density(\n    image: Any,\n    config: DoclingProcessingConfig,\n    target_width: int = 300,\n) -> List[float]:\n    gray = image.convert(\"L\")\n    width, height = gray.size\n    if width > target_width:\n        scale = target_width / max(1, width)\n        gray = gray.resize((target_width, max(1, int(height * scale))))\n    width, height = gray.size\n    crop_top = int(height * config.column_detect_crop_top_ratio)\n    crop_bottom = int(height * config.column_detect_crop_bottom_ratio)\n    if crop_top + crop_bottom < height - 1:\n        gray = gray.crop((0, crop_top, width, height - crop_bottom))\n\n    try:\n        import numpy as np\n    except Exception:\n        pixels = list(gray.getdata())\n        w, h = gray.size\n        if w == 0 or h == 0:\n            return []\n        sorted_pixels = sorted(pixels)\n        median = sorted_pixels[len(sorted_pixels) // 2]\n        mean = sum(pixels) / max(1, len(pixels))\n        variance = sum((value - mean) ** 2 for value in pixels) / max(1, len(pixels))\n        std = variance ** 0.5\n        threshold = median - (std * config.column_detect_threshold_std_mult)\n        threshold = min(threshold, config.column_detect_threshold_max)\n        threshold = max(threshold, config.column_detect_threshold_min)\n        densities = [0] * w\n        for y in range(h):\n            row = pixels[y * w:(y + 1) * w]\n            for x, value in enumerate(row):\n                if value < threshold:\n                    densities[x] += 1\n        return [count / h for count in densities]\n\n    arr = np.asarray(gray)\n    if arr.size == 0:\n        return []\n    # Build a robust binarization threshold: combine median-std rule with Otsu\n    median = float(np.median(arr))\n    std = float(arr.std())\n    thr_a = median - (std * config.column_detect_threshold_std_mult)\n    thr_a = min(thr_a, float(config.column_detect_threshold_max))\n    thr_a = max(thr_a, float(config.column_detect_threshold_min))\n\n    # Otsu threshold (fast implementation without external deps)\n    try:\n        hist, _ = np.histogram(arr, bins=256, range=(0, 255))\n        hist = hist.astype(np.float64)\n        total = hist.sum()\n        if total > 0:\n            prob = hist / total\n            omega = np.cumsum(prob)\n            mu = np.cumsum(prob * np.arange(256))\n            mu_t = mu[-1]\n            sigma_b2 = (mu_t * omega - mu) ** 2 / np.maximum(omega * (1.0 - omega), 1e-9)\n            k = int(np.nanargmax(sigma_b2))\n            thr_b = float(k)\n        else:\n            thr_b = thr_a\n    except Exception:\n        thr_b = thr_a\n\n    threshold = 0.5 * (thr_a + thr_b)\n    mask = arr < threshold\n\n    # Focus on the vertical band with the most text-like pixels to avoid full-width pictures at top\n    h = mask.shape[0]\n    band_h = max(1, int(h * 0.6))  # use central 60% by default (adaptive below)\n    if band_h < h:\n        step = max(1, int(h * 0.04))\n        best_y = 0\n        best_score = -1.0\n        # Slide a window to find the densest text band\n        for y in range(0, h - band_h + 1, step):\n            score = mask[y : y + band_h, :].mean()\n            if score > best_score:\n                best_score = score\n                best_y = y\n        mask = mask[best_y : best_y + band_h, :]\n\n    return mask.mean(axis=0).tolist()\n\n\ndef smooth_density(density: Sequence[float], window: int) -> List[float]:\n    if window <= 1 or not density:\n        return list(density)\n    size = max(1, int(window))\n    half = size // 2\n    smoothed: List[float] = []\n    for idx in range(len(density)):\n        start = max(0, idx - half)\n        end = min(len(density), idx + half + 1)\n        smoothed.append(sum(density[start:end]) / max(1, end - start))\n    return smoothed\n\n\ndef density_percentile(density: Sequence[float], percentile: float) -> float:\n    if not density:\n        return 0.0\n    clamped = max(0.0, min(1.0, percentile))\n    sorted_vals = sorted(density)\n    idx = int(round(clamped * (len(sorted_vals) - 1)))\n    return sorted_vals[idx]\n\n\ndef find_column_gaps(\n    density: Sequence[float],\n    config: DoclingProcessingConfig,\n) -> List[Tuple[int, int]]:\n    if not density:\n        return []\n    total = len(density)\n    margin = max(1, int(total * 0.05))\n    start = margin\n    end = max(start + 1, total - margin)\n    core = density[start:end]\n    if not core:\n        return []\n    text_level = density_percentile(core, config.column_detect_text_percentile)\n    if text_level < config.column_detect_min_text_density:\n        return []\n    threshold = max(config.column_detect_min_gap_density, text_level * config.column_detect_gap_threshold_ratio)\n    min_gap = max(1, int(len(core) * config.column_detect_min_gap_ratio))\n\n    gaps: List[Tuple[int, int]] = []\n    idx = 0\n    while idx < len(core):\n        if core[idx] < threshold:\n            gap_start = idx\n            while idx < len(core) and core[idx] < threshold:\n                idx += 1\n            if idx - gap_start >= min_gap:\n                gaps.append((start + gap_start, start + idx))\n        else:\n            idx += 1\n    return gaps\n\n\ndef count_column_gaps(\n    density: Sequence[float],\n    config: DoclingProcessingConfig,\n) -> int:\n    return len(find_column_gaps(density, config))\n\n\ndef detect_multicolumn_layout(\n    images: Sequence[Any],\n    config: DoclingProcessingConfig,\n) -> ColumnLayoutDetection:\n    if not images:\n        return ColumnLayoutDetection(False, 0.0, \"No pages available\")\n    sample = list(images[: config.column_detect_max_pages])\n    if not sample:\n        return ColumnLayoutDetection(False, 0.0, \"No sample pages\")\n\n    hits = 0\n    for image in sample:\n        density = compute_column_density(image, config)\n        density = smooth_density(density, config.column_detect_smooth_window)\n        gaps = count_column_gaps(density, config)\n        if gaps >= 1:\n            hits += 1\n    ratio = hits / max(1, len(sample))\n    detected = ratio >= config.column_detect_min_pages_ratio\n    reason = f\"{hits}/{len(sample)} pages show column gutters\"\n    return ColumnLayoutDetection(detected, ratio, reason)\n\n\ndef rasterize_pdf_to_temp(pdf_path: str, dpi: int) -> str:\n    from tempfile import NamedTemporaryFile\n\n    images = render_pdf_pages(pdf_path, dpi)\n    if not images:\n        raise RuntimeError(\"Failed to render PDF pages for rasterization.\")\n\n    temp_file = NamedTemporaryFile(delete=False, suffix=\".pdf\")\n    temp_file.close()\n    first = images[0]\n    rest = images[1:]\n    first.save(temp_file.name, format=\"PDF\", save_all=True, append_images=rest)\n    return temp_file.name\n\n\ndef split_blocks_into_columns(\n    blocks: List[Dict[str, Any]], log_label: str = \"OCR\"\n) -> Tuple[List[List[Dict[str, Any]]], float, float]:\n    if not blocks:\n        return [], 0.0, 0.0\n    # Robust grouping by x-center: find one or two big gaps -> 2 or 3 columns\n    xs = sorted(b[\"xc\"] for b in blocks)\n    x_min, x_max = xs[0], xs[-1]\n    span = max(1.0, x_max - x_min)\n    widths = sorted((b[\"x1\"] - b[\"x0\"]) for b in blocks)\n    w_med = widths[len(widths) // 2] if widths else 1.0\n    # Lower threshold than before: helps separate three narrow columns\n    gap_thr = max(0.06 * span, 0.5 * w_med)\n\n    # Compute gaps between consecutive x-centers\n    diffs: List[Tuple[float, int]] = []\n    for i in range(1, len(xs)):\n        diffs.append((xs[i] - xs[i - 1], i))  # (gap, split_index)\n    gap_values = sorted(gap for gap, _ in diffs)\n    median_gap = gap_values[len(gap_values) // 2] if gap_values else 0.0\n    # Candidate split positions are those with large gaps\n    candidates = [idx for (gap, idx) in diffs if gap >= gap_thr]\n\n    # Build columns by splitting at up to two largest valid gaps ensuring min size per group\n    min_lines = max(3, len(blocks) // 20 or 1)\n    columns: List[List[Dict[str, Any]]] = []\n    blocks_sorted = sorted(blocks, key=lambda b: b[\"xc\"])  # align with xs order\n    used_splits: List[int] = []\n    if candidates:\n        # Prefer two-gap (3-column) split if possible\n        cands_sorted = sorted(\n            ((xs[i - 1], xs[i], i) for i in candidates), key=lambda t: t[1] - t[0], reverse=True\n        )\n        # Try all pairs of split indices to form 3 groups\n        tried = False\n        for _a in range(min(5, len(cands_sorted))):\n            for _b in range(_a + 1, min(6, len(cands_sorted))):\n                i1 = cands_sorted[_a][2]\n                i2 = cands_sorted[_b][2]\n                a, b = sorted([i1, i2])\n                if a < min_lines or (b - a) < min_lines or (len(blocks) - b) < min_lines:\n                    continue\n                used_splits = [a, b]\n                tried = True\n                break\n            if tried:\n                break\n        if not used_splits:\n            # Fall back to single split (2 columns)\n            # pick the largest valid gap that yields two groups of minimum size\n            for _, _, i in cands_sorted:\n                if i >= min_lines and (len(blocks) - i) >= min_lines:\n                    used_splits = [i]\n                    break\n\n    if used_splits:\n        used_splits = sorted(set(used_splits))\n        start = 0\n        for s in used_splits:\n            columns.append(blocks_sorted[start:s])\n            start = s\n        columns.append(blocks_sorted[start:])\n    else:\n        # Fallback threshold grouping\n        cur: List[Dict[str, Any]] = []\n        prev_xc: Optional[float] = None\n        for b in blocks_sorted:\n            if prev_xc is None or abs(b[\"xc\"] - prev_xc) <= gap_thr:\n                cur.append(b)\n            else:\n                if cur:\n                    columns.append(cur)\n                cur = [b]\n            prev_xc = b[\"xc\"]\n        if cur:\n            columns.append(cur)\n\n    def _kmeans_1d(points: List[float], k: int) -> Optional[Tuple[List[List[int]], List[float]]]:\n        if len(points) < k:\n            return None\n        sorted_points = sorted(points)\n        centers = []\n        for i in range(k):\n            pct = (i + 0.5) / k\n            idx = int(pct * (len(sorted_points) - 1))\n            centers.append(sorted_points[idx])\n        for _ in range(20):\n            clusters: List[List[int]] = [[] for _ in range(k)]\n            for idx, val in enumerate(points):\n                nearest = min(range(k), key=lambda c: abs(val - centers[c]))\n                clusters[nearest].append(idx)\n            new_centers = []\n            for c_idx in range(k):\n                if not clusters[c_idx]:\n                    return None\n                new_centers.append(sum(points[i] for i in clusters[c_idx]) / len(clusters[c_idx]))\n            if max(abs(new_centers[i] - centers[i]) for i in range(k)) < 0.5:\n                centers = new_centers\n                break\n            centers = new_centers\n        return clusters, centers\n\n    def _kmeans_improvement(points: List[float], clusters: List[List[int]], centers: List[float]) -> float:\n        mean = sum(points) / len(points)\n        total_var = sum((val - mean) ** 2 for val in points) / max(1, len(points))\n        if total_var <= 1e-6:\n            return 0.0\n        within = 0.0\n        for c_idx, cluster in enumerate(clusters):\n            center = centers[c_idx]\n            for i in cluster:\n                within += (points[i] - center) ** 2\n        within /= max(1, len(points))\n        return (total_var - within) / total_var\n\n    def _boundary_valley_ok(points: List[float], centers: List[float], span_points: float) -> bool:\n        ordered = sorted(centers)\n        if len(ordered) <= 1:\n            return False\n        band = max(0.04 * span_points, 1.5 * w_med)\n        band = min(band, 0.2 * span_points)\n        total = len(points)\n        for i in range(len(ordered) - 1):\n            boundary = 0.5 * (ordered[i] + ordered[i + 1])\n            count = sum(1 for val in points if abs(val - boundary) <= band / 2)\n            expected = max(1e-6, band / span_points * total)\n            if (count / expected) > 0.85:\n                return False\n        return True\n\n    if len(columns) <= 1 and len(blocks_sorted) >= 20:\n        min_lines = max(3, len(blocks_sorted) // 20 or 1)\n\n        def _try_kmeans(points: List[float], basis: str) -> Optional[Tuple[List[List[Dict[str, Any]]], float, int, str]]:\n            span_points = max(1.0, max(points) - min(points))\n            best_cols: Optional[List[List[Dict[str, Any]]]] = None\n            best_score = 0.0\n            best_k = 0\n            for k in (2, 3):\n                if len(blocks_sorted) < k * min_lines:\n                    continue\n                result = _kmeans_1d(points, k)\n                if not result:\n                    continue\n                clusters, centers = result\n                if min(len(c) for c in clusters) < min_lines:\n                    continue\n                improvement = _kmeans_improvement(points, clusters, centers)\n                if improvement < 0.6:\n                    continue\n                if not _boundary_valley_ok(points, centers, span_points):\n                    continue\n                ordered = sorted(range(k), key=lambda i: centers[i])\n                ordered_centers = [centers[i] for i in ordered]\n                min_gap = min(\n                    ordered_centers[i + 1] - ordered_centers[i]\n                    for i in range(len(ordered_centers) - 1)\n                )\n                if min_gap < 0.02 * span_points:\n                    continue\n                score = improvement + (min_gap / span_points)\n                if score > best_score:\n                    best_score = score\n                    best_k = k\n                    best_cols = [[blocks_sorted[i] for i in clusters[idx]] for idx in ordered]\n            if best_cols:\n                return best_cols, best_score, best_k, basis\n            return None\n\n        candidates = [\n            _try_kmeans([b[\"xc\"] for b in blocks_sorted], \"xc\"),\n            _try_kmeans([b[\"x0\"] for b in blocks_sorted], \"x0\"),\n        ]\n        best = None\n        for candidate in candidates:\n            if not candidate:\n                continue\n            if best is None or candidate[1] > best[1]:\n                best = candidate\n        if best:\n            columns, best_score, best_k, basis = best\n            try:\n                LOGGER.info(\n                    \"%s column grouping fallback (kmeans-%s): k=%d score=%.2f\",\n                    log_label,\n                    basis,\n                    best_k,\n                    best_score,\n                )\n            except Exception:\n                pass\n\n    # Sort columns left-to-right by median x center\n    def col_key(col: List[Dict[str, Any]]) -> float:\n        centers = sorted(b[\"xc\"] for b in col)\n        return centers[len(centers) // 2]\n\n    columns = [col for col in columns if col]\n    columns.sort(key=col_key)\n    try:\n        LOGGER.info(\"%s column grouping: k=%d (gap_thr=%.2f, span=%.1f)\", log_label, len(columns), gap_thr, span)\n    except Exception:\n        pass\n    return columns, gap_thr, span\n\n\ndef order_blocks_into_columns(\n    blocks: List[Dict[str, Any]],\n    log_label: str = \"OCR\",\n    preserve_single_column_order: bool = False,\n) -> str:\n    columns, _, _ = split_blocks_into_columns(blocks, log_label=log_label)\n    if not columns:\n        return \"\"\n    # Within each column, sort top-down and join\n    col_texts: List[str] = []\n    for col in columns:\n        if preserve_single_column_order and len(columns) == 1:\n            col_sorted = sorted(col, key=lambda b: b.get(\"line_id\", 0))\n        else:\n            col_sorted = sorted(col, key=lambda b: (b[\"y0\"], b[\"x0\"]))\n        lines: List[str] = []\n        for block in col_sorted:\n            raw = str(block.get(\"text\", \"\")).strip()\n            if not raw:\n                continue\n            lines.append(raw)\n        col_texts.append(\"\\n\".join(lines))\n    # Read columns left to right\n    return \"\\n\\n\".join(t for t in col_texts if t)\n\n\ndef ocr_pages_text_chars(pages: Sequence[Dict[str, Any]]) -> int:\n    return sum(len(str(page.get(\"text\", \"\")).strip()) for page in pages)\n\n\ndef has_output_text(markdown: str, pages: Sequence[Dict[str, Any]]) -> bool:\n    return bool(markdown.strip()) or ocr_pages_text_chars(pages) > 0\n\n\ndef _external_ocr_helpers() -> Dict[str, Any]:\n    return {\n        \"logger\": LOGGER,\n        \"ocr_pages_text_chars\": ocr_pages_text_chars,\n        \"detect_repeated_line_clusters\": detect_repeated_line_clusters,\n        \"normalize_boilerplate_line\": normalize_boilerplate_line,\n        \"matches_repeated_cluster\": matches_repeated_cluster,\n        \"is_boilerplate_line\": is_boilerplate_line,\n        \"edge_ids_by_y\": edge_ids_by_y,\n        \"select_edge_texts_by_y\": select_edge_texts_by_y,\n        \"order_blocks_into_columns\": order_blocks_into_columns,\n        \"split_blocks_into_columns\": split_blocks_into_columns,\n    }\n\n\ndef run_external_ocr_pages(\n    pdf_path: str,\n    engine: str,\n    languages: str,\n    config: DoclingProcessingConfig,\n    dpi: Optional[int] = None,\n    progress_cb: Optional[ProgressCallback] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    effective_dpi = dpi or config.ocr_dpi\n    helpers = _external_ocr_helpers()\n    helpers[\"ocr_source_path\"] = pdf_path\n    helpers[\"boilerplate_prepass_enabled\"] = bool(config.enable_boilerplate_removal)\n    if progress_cb and progress_span > 0:\n        label = \"Paddle OCR\" if engine == \"paddle\" else \"Tesseract OCR\"\n        # Use a neutral initializing message; inner routines will promptly override with page counters\n        progress_cb(progress_base, \"ocr\", f\"{label} initializing\")\n    def _paddle_vl_api_enabled() -> bool:\n        if bool(getattr(config, \"paddle_vl_api_disable\", False)):\n            return False\n        api_url = getattr(config, \"paddle_vl_api_url\", None) or os.getenv(\"PADDLE_VL_API_URL\")\n        api_token = getattr(config, \"paddle_vl_api_token\", None) or os.getenv(\"PADDLE_VL_API_TOKEN\")\n        return bool(api_url and api_token)\n    if engine == \"paddle\" and config.paddle_use_vl:\n        if _paddle_vl_api_enabled():\n            helpers[\"boilerplate_prepass_enabled\"] = False\n        LOGGER.info(\n            \"External OCR starting: engine=%s (PaddleOCR-VL), dpi=%d\",\n            engine,\n            effective_dpi,\n        )\n    elif engine == \"paddle\" and config.paddle_use_structure_v3:\n        LOGGER.info(\n            \"External OCR starting: engine=%s (PP-Structure), dpi=%d\",\n            engine,\n            effective_dpi,\n        )\n    else:\n        LOGGER.info(\n            \"External OCR starting: engine=%s, dpi=%d\",\n            engine,\n            effective_dpi,\n        )\n    if engine == \"paddle\":\n        max_side_points = get_pdf_max_page_points(pdf_path)\n        orig_effective_dpi = effective_dpi\n        if max_side_points and config.paddle_target_max_side_px > 0:\n            target_dpi = int(config.paddle_target_max_side_px * 72 / max_side_points)\n            if target_dpi > 0:\n                LOGGER.info(\n                    \"Paddle OCR target DPI: page max side=%.1f pts, limit=%d px -> %d DPI (requested=%d)\",\n                    max_side_points,\n                    config.paddle_target_max_side_px,\n                    target_dpi,\n                    orig_effective_dpi,\n                )\n            if target_dpi > 0 and target_dpi < effective_dpi:\n                LOGGER.info(\n                    \"Paddle OCR DPI adjusted for page size: %d -> %d\",\n                    effective_dpi,\n                    target_dpi,\n                )\n                effective_dpi = target_dpi\n        if config.paddle_max_dpi > 0 and effective_dpi > config.paddle_max_dpi:\n            LOGGER.info(\n                \"Paddle OCR DPI capped: %d -> %d\",\n                effective_dpi,\n                config.paddle_max_dpi,\n            )\n            effective_dpi = config.paddle_max_dpi\n    images = render_pdf_pages(pdf_path, effective_dpi)\n    LOGGER.info(\"External OCR rendered pages: %d\", len(images))\n    if images:\n        try:\n            sample_w, sample_h = images[0].size  # type: ignore[attr-defined]\n        except Exception:\n            sample_w = sample_h = 0\n        if sample_w and sample_h:\n            LOGGER.info(\n                \"External OCR sample page: %dx%d px @ %d DPI (engine=%s)\",\n                sample_w,\n                sample_h,\n                effective_dpi,\n                engine,\n            )\n    if engine == \"paddle\":\n        if config.paddle_use_vl:\n            try:\n                pages, stats = ocr_pages_with_paddle_vl(\n                    images,\n                    normalize_languages_for_engine(languages, engine),\n                    config,\n                    helpers,\n                    progress_cb,\n                    progress_base,\n                    progress_span,\n                )\n                if ocr_pages_text_chars(pages) == 0:\n                    LOGGER.warning(\n                        \"PaddleOCR-VL returned empty text; falling back to PaddleOCR.\"\n                    )\n                    helpers[\"boilerplate_prepass_enabled\"] = bool(config.enable_boilerplate_removal)\n                    return ocr_pages_with_paddle(\n                        images,\n                        normalize_languages_for_engine(languages, engine),\n                        config,\n                        helpers,\n                        progress_cb,\n                        progress_base,\n                        progress_span,\n                    )\n                return pages, stats\n            except Exception as exc:\n                LOGGER.warning(\"PaddleOCR-VL failed; falling back to PaddleOCR: %s\", exc)\n                helpers[\"boilerplate_prepass_enabled\"] = bool(config.enable_boilerplate_removal)\n        if config.paddle_use_structure_v3:\n            try:\n                pages, stats = ocr_pages_with_paddle_structure(\n                    images,\n                    normalize_languages_for_engine(languages, engine),\n                    config,\n                    helpers,\n                    progress_cb,\n                    progress_base,\n                    progress_span,\n                )\n                if ocr_pages_text_chars(pages) == 0:\n                    LOGGER.warning(\n                        \"PP-Structure returned empty text; falling back to PaddleOCR.\"\n                    )\n                    return ocr_pages_with_paddle(\n                        images,\n                        normalize_languages_for_engine(languages, engine),\n                        config,\n                        helpers,\n                        progress_cb,\n                        progress_base,\n                        progress_span,\n                    )\n                return pages, stats\n            except Exception as exc:\n                LOGGER.warning(\"PP-StructureV3 failed; falling back to PaddleOCR: %s\", exc)\n        return ocr_pages_with_paddle(\n            images,\n            normalize_languages_for_engine(languages, engine),\n            config,\n            helpers,\n            progress_cb,\n            progress_base,\n            progress_span,\n        )\n    if engine == \"tesseract\":\n        return ocr_pages_with_tesseract(\n            images,\n            normalize_languages_for_engine(languages, engine),\n            config,\n            helpers,\n            progress_cb,\n            progress_base,\n            progress_span,\n        )\n    return [], {}\n\n\ndef build_quality_report(pdf_path: str, config: DoclingProcessingConfig) -> Dict[str, Any]:\n    analysis_pages = extract_pages_from_pdf(\n        pdf_path,\n        max_pages=config.analysis_max_pages,\n        sample_strategy=config.analysis_sample_strategy,\n    )\n    has_text_layer = detect_text_layer_from_pages(analysis_pages, config)\n    languages = select_language_set(config.language_hint, pdf_path, config)\n    quality = estimate_text_quality(analysis_pages, config, languages)\n    quality, classifier_info = apply_text_layer_classifier(quality, pdf_path, config)\n    low_quality = is_low_quality(quality, config)\n    text_layer_overlay = bool(\n        has_text_layer\n        and (\n            (\n                quality.ocr_overlay_ratio is not None\n                and quality.ocr_overlay_ratio >= config.quality_classifier_decision_ratio\n            )\n            or (\n                quality.image_page_ratio is not None\n                and quality.image_page_ratio >= config.quality_image_page_ratio_threshold\n            )\n        )\n    )\n    if quality.image_page_ratio is not None:\n        LOGGER.info(\n            \"Text-layer overlay: %s (img_pages=%.2f, threshold=%.2f)\",\n            text_layer_overlay,\n            quality.image_page_ratio,\n            config.quality_image_page_ratio_threshold,\n        )\n    if classifier_info:\n        LOGGER.info(\n            \"Text-layer classifier: %s (ocr_ratio=%.2f, digital_ratio=%.2f, sampled=%d, short_circuit=%s, guardrail=%s)\",\n            quality.layer_classification or classifier_info.get(\"decision\"),\n            quality.ocr_overlay_ratio or 0.0,\n            quality.digital_page_ratio or 0.0,\n            classifier_info.get(\"sampled_pages\", 0),\n            classifier_info.get(\"short_circuit\", False),\n            classifier_info.get(\"guardrail_applied\", False),\n        )\n    return {\n        \"text_layer_detected\": has_text_layer,\n        \"text_layer_low_quality\": has_text_layer and low_quality,\n        \"text_layer_overlay\": text_layer_overlay,\n        \"avg_chars_per_page\": quality.avg_chars_per_page,\n        \"alpha_ratio\": quality.alpha_ratio,\n        \"suspicious_token_ratio\": quality.suspicious_token_ratio,\n        \"confidence_proxy\": quality.confidence_proxy,\n        \"effective_confidence_proxy\": quality.effective_confidence_proxy,\n        \"dictionary_hit_ratio\": quality.dictionary_hit_ratio,\n        \"spellchecker_hit_ratio\": quality.spellchecker_hit_ratio,\n        \"image_heavy_ratio\": quality.image_heavy_ratio,\n        \"image_page_ratio\": quality.image_page_ratio,\n        \"ocr_overlay_ratio\": quality.ocr_overlay_ratio,\n        \"digital_page_ratio\": quality.digital_page_ratio,\n        \"classifier_decision\": quality.layer_classification,\n        \"classifier_sampled_pages\": classifier_info.get(\"sampled_pages\") if classifier_info else None,\n        \"classifier_short_circuit\": classifier_info.get(\"short_circuit\") if classifier_info else None,\n    }\n\n\ndef convert_pdf_with_docling(\n    pdf_path: str,\n    config: DoclingProcessingConfig,\n    progress_cb: Optional[ProgressCallback] = None,\n) -> DoclingConversionResult:\n    emit = progress_cb or (lambda _p, _s, _m: None)\n    emit(5, \"analysis\", \"Analyzing text layer\")\n    analysis_pages = extract_pages_from_pdf(\n        pdf_path,\n        max_pages=config.analysis_max_pages,\n        sample_strategy=config.analysis_sample_strategy,\n    )\n    has_text_layer = detect_text_layer_from_pages(analysis_pages, config)\n    languages = select_language_set(config.language_hint, pdf_path, config)\n    quality = estimate_text_quality(analysis_pages, config, languages)\n    quality, classifier_info = apply_text_layer_classifier(quality, pdf_path, config)\n    low_quality = is_low_quality(quality, config)\n    text_layer_overlay = bool(\n        has_text_layer\n        and (\n            (\n                quality.ocr_overlay_ratio is not None\n                and quality.ocr_overlay_ratio >= config.quality_classifier_decision_ratio\n            )\n            or (\n                quality.image_page_ratio is not None\n                and quality.image_page_ratio >= config.quality_image_page_ratio_threshold\n            )\n        )\n    )\n    available_engines = detect_available_ocr_engines()\n    decision = decide_ocr_route(has_text_layer, quality, available_engines, config, languages)\n    emit(15, \"route\", \"Selecting OCR route\")\n    rasterized_source = False\n    rasterized_pdf_path = \"\"\n    rasterize_error: Optional[str] = None\n    column_layout: Optional[ColumnLayoutDetection] = None\n    if should_rasterize_text_layer(has_text_layer, low_quality, config):\n        try:\n            rasterized_pdf_path = rasterize_pdf_to_temp(pdf_path, config.ocr_dpi)\n            rasterized_source = True\n            emit(25, \"rasterize\", \"Rasterized PDF for OCR\")\n            LOGGER.info(\"Rasterized low-quality text layer for Docling OCR.\")\n        except Exception as exc:\n            rasterize_error = str(exc)\n            LOGGER.warning(\"Failed to rasterize PDF for OCR: %s\", exc)\n    if rasterized_source:\n        if not config.force_per_page_ocr:\n            decision.per_page_ocr = False\n            decision.per_page_reason = \"Rasterized PDF for Docling OCR\"\n\n    if config.column_detect_enable and decision.ocr_used and (rasterized_source or not has_text_layer):\n        try:\n            # Spread sampling across document to avoid false negatives on front-matter\n            total_pages = get_pdf_page_count(pdf_path)\n            sample_indices = select_column_sample_indices(total_pages, config.column_detect_max_pages)\n            if not sample_indices:\n                sample_indices = list(range(1, min(3, total_pages or 3) + 1))\n            LOGGER.info(\"Column layout sample pages: %s\", sample_indices)\n\n            sample_images = render_pdf_pages_at_indices(pdf_path, config.column_detect_dpi, sample_indices)\n            column_layout = detect_multicolumn_layout(sample_images, config)\n            # If not detected, retry at a higher DPI once\n            if not column_layout.detected and config.column_detect_dpi < 220:\n                hi_dpi = 300\n                hi_images = render_pdf_pages_at_indices(pdf_path, hi_dpi, sample_indices)\n                hi_layout = detect_multicolumn_layout(hi_images, config)\n                if hi_layout.detected:\n                    column_layout = hi_layout\n                    LOGGER.info(\"Column layout detection (hi-dpi %d): %s (%s)\", hi_dpi, column_layout.detected, column_layout.reason)\n            LOGGER.info(\n                \"Column layout detection: %s (%s)\",\n                column_layout.detected,\n                column_layout.reason,\n            )\n            emit(30, \"layout\", \"Checked column layout\")\n            if (\n                column_layout.detected\n                and decision.use_external_ocr\n                and decision.per_page_ocr\n                and not config.force_per_page_ocr\n            ):\n                decision.per_page_ocr = False\n                decision.per_page_reason = \"Columns detected; keep Docling layout\"\n        except Exception as exc:\n            LOGGER.warning(\"Column layout detection failed: %s\", exc)\n\n    dict_ratio = \"n/a\" if quality.dictionary_hit_ratio is None else f\"{quality.dictionary_hit_ratio:.2f}\"\n    spell_ratio = \"n/a\" if quality.spellchecker_hit_ratio is None else f\"{quality.spellchecker_hit_ratio:.2f}\"\n    img_ratio = \"n/a\" if quality.image_heavy_ratio is None else f\"{quality.image_heavy_ratio:.2f}\"\n    img_pages_ratio = \"n/a\" if quality.image_page_ratio is None else f\"{quality.image_page_ratio:.2f}\"\n    ocr_ratio = \"n/a\" if quality.ocr_overlay_ratio is None else f\"{quality.ocr_overlay_ratio:.2f}\"\n    digital_ratio = \"n/a\" if quality.digital_page_ratio is None else f\"{quality.digital_page_ratio:.2f}\"\n    LOGGER.info(\n        \"Text-layer check: %s (avg_chars=%.1f, alpha_ratio=%.2f, suspicious=%.2f, dict=%s, spell=%s, img=%s, img_pages=%s, ocr_ratio=%s, digital_ratio=%s)\",\n        has_text_layer,\n        quality.avg_chars_per_page,\n        quality.alpha_ratio,\n        quality.suspicious_token_ratio,\n        dict_ratio,\n        spell_ratio,\n        img_ratio,\n        img_pages_ratio,\n        ocr_ratio,\n        digital_ratio,\n    )\n    if classifier_info:\n        LOGGER.info(\n            \"Text-layer classifier: %s (ocr_ratio=%.2f, digital_ratio=%.2f, sampled=%d, short_circuit=%s, guardrail=%s)\",\n            quality.layer_classification or classifier_info.get(\"decision\"),\n            quality.ocr_overlay_ratio or 0.0,\n            quality.digital_page_ratio or 0.0,\n            classifier_info.get(\"sampled_pages\", 0),\n            classifier_info.get(\"short_circuit\", False),\n            classifier_info.get(\"guardrail_applied\", False),\n        )\n    if available_engines:\n        LOGGER.info(\"Available OCR engines: %s\", \", \".join(available_engines))\n    else:\n        LOGGER.info(\"Available OCR engines: none (external OCR disabled)\")\n\n    LOGGER.info(\n        \"Docling OCR route: %s (engine=%s, languages=%s)\",\n        decision.route_reason,\n        decision.ocr_engine,\n        decision.languages,\n    )\n    LOGGER.info(\"Per-page OCR: %s (%s)\", decision.per_page_ocr, decision.per_page_reason)\n    if decision.ocr_used and not decision.use_external_ocr:\n        LOGGER.info(\"External OCR unavailable; relying on Docling OCR.\")\n\n    converter = build_converter(config, decision)\n    docling_input = rasterized_pdf_path or pdf_path\n    emit(40, \"docling\", \"Docling conversion running\")\n    result = converter.convert(docling_input)\n    doc = result.document if hasattr(result, \"document\") else result\n    markdown = export_markdown(doc)\n    pages = extract_pages(doc)\n    if len(pages) <= 1:\n        fallback_pages = extract_pages_from_pdf(pdf_path)\n        if len(fallback_pages) > len(pages):\n            pages = fallback_pages\n    emit(70, \"docling\", \"Docling conversion complete\")\n\n    ocr_stats: Dict[str, Any] = {}\n    ocr_engine_used = decision.ocr_engine\n    external_ocr_used = False\n    # Always allow external OCR if selected, even when the PDF was rasterized for Docling,\n    # so we can prefer column-aware ordering from Paddle/Tesseract when desired.\n    if decision.ocr_used and decision.use_external_ocr:\n        ocr_dpi = config.ocr_overlay_dpi if text_layer_overlay else config.ocr_dpi\n        if ocr_dpi != config.ocr_dpi:\n            LOGGER.info(\"External OCR DPI bumped for overlay: %d -> %d\", config.ocr_dpi, ocr_dpi)\n        try:\n            ocr_pages, ocr_stats = run_external_ocr_pages(\n                pdf_path,\n                decision.ocr_engine,\n                languages,\n                config,\n                dpi=ocr_dpi,\n                progress_cb=emit,\n                progress_base=70,\n                progress_span=20,\n            )\n            if ocr_pages:\n                ocr_text_chars = ocr_pages_text_chars(ocr_pages)\n                if ocr_text_chars > 0:\n                    layout_used = ocr_stats.get(\"layout_used\")\n                    layout_model = ocr_stats.get(\"layout_model\")\n                    LOGGER.info(\n                        \"External OCR stats: engine=%s, layout_used=%s, layout_model=%s, text_chars=%d\",\n                        decision.ocr_engine,\n                        layout_used,\n                        layout_model,\n                        ocr_text_chars,\n                    )\n                    pages = ocr_pages\n                    external_ocr_used = True\n                    layout_markdown = ocr_stats.get(\"layout_markdown\")\n                    if isinstance(layout_markdown, str) and layout_markdown.strip():\n                        markdown = layout_markdown\n                    elif config.postprocess_markdown and not markdown.strip():\n                        markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in ocr_pages)\n                else:\n                    ocr_stats = {}\n                    LOGGER.warning(\n                        \"External OCR returned empty text (%s). Keeping Docling text.\",\n                        decision.ocr_engine,\n                    )\n            else:\n                ocr_stats = {}\n                LOGGER.warning(\n                    \"External OCR returned empty text (%s). Keeping Docling text.\",\n                    decision.ocr_engine,\n                )\n        except Exception as exc:\n            LOGGER.warning(\"External OCR failed (%s): %s\", decision.ocr_engine, exc)\n            if decision.ocr_engine != \"tesseract\" and \"tesseract\" in available_engines:\n                try:\n                    LOGGER.info(\"Retrying external OCR with tesseract.\")\n                    ocr_pages, ocr_stats = run_external_ocr_pages(\n                        pdf_path,\n                        \"tesseract\",\n                        languages,\n                        config,\n                        dpi=ocr_dpi,\n                        progress_cb=emit,\n                        progress_base=70,\n                        progress_span=20,\n                    )\n                    if ocr_pages:\n                        ocr_text_chars = ocr_pages_text_chars(ocr_pages)\n                        if ocr_text_chars > 0:\n                            layout_used = ocr_stats.get(\"layout_used\")\n                            layout_model = ocr_stats.get(\"layout_model\")\n                            LOGGER.info(\n                                \"External OCR stats: engine=%s, layout_used=%s, layout_model=%s, text_chars=%d\",\n                                \"tesseract\",\n                                layout_used,\n                                layout_model,\n                                ocr_text_chars,\n                            )\n                            pages = ocr_pages\n                            ocr_engine_used = \"tesseract\"\n                            external_ocr_used = True\n                            if config.postprocess_markdown and not markdown.strip():\n                                markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in ocr_pages)\n                        else:\n                            ocr_stats = {}\n                            LOGGER.warning(\n                                \"External OCR returned empty text (tesseract). Keeping Docling text.\"\n                            )\n                except Exception as exc2:\n                    LOGGER.warning(\"External OCR failed (tesseract): %s\", exc2)\n    if rasterized_source and rasterized_pdf_path:\n        try:\n            os.unlink(rasterized_pdf_path)\n        except Exception:\n            pass\n\n    fallback_engine: Optional[str] = None\n    if not has_output_text(markdown, pages):\n        LOGGER.warning(\"Docling output empty; attempting OCR fallback.\")\n        fallback_dpi = config.ocr_overlay_dpi if text_layer_overlay else config.ocr_dpi\n        fallback_engines: List[str] = []\n        if \"tesseract\" in available_engines and ocr_engine_used != \"tesseract\":\n            fallback_engines.append(\"tesseract\")\n        if \"paddle\" in available_engines and ocr_engine_used != \"paddle\":\n            fallback_engines.append(\"paddle\")\n        for engine in fallback_engines:\n            try:\n                fallback_pages, fallback_stats = run_external_ocr_pages(\n                    pdf_path,\n                    engine,\n                    languages,\n                    config,\n                    dpi=fallback_dpi,\n                )\n                if ocr_pages_text_chars(fallback_pages) > 0:\n                    pages = fallback_pages\n                    markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in pages)\n                    external_ocr_used = True\n                    ocr_engine_used = engine\n                    ocr_stats = fallback_stats\n                    fallback_engine = engine\n                    LOGGER.warning(\"External OCR fallback succeeded with %s.\", engine)\n                    break\n                LOGGER.warning(\"External OCR fallback returned empty text (%s).\", engine)\n            except Exception as exc:\n                LOGGER.warning(\"External OCR fallback failed (%s): %s\", engine, exc)\n        if not has_output_text(markdown, pages):\n            fallback_pages = extract_pages_from_pdf(pdf_path)\n            if ocr_pages_text_chars(fallback_pages) > 0:\n                pages = fallback_pages\n                markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in pages)\n                external_ocr_used = False\n                ocr_stats = dict(ocr_stats)\n                ocr_stats[\"text_layer_fallback\"] = True\n                fallback_engine = \"text_layer\"\n                LOGGER.warning(\"Text-layer fallback succeeded after empty output.\")\n\n    if external_ocr_used:\n        ocr_confidence = normalize_ocr_confidence(ocr_stats.get(\"ocr_confidence_avg\"))\n        if ocr_confidence is not None:\n            base_confidence = (\n                quality.effective_confidence_proxy\n                if quality.effective_confidence_proxy is not None\n                else quality.confidence_proxy\n            )\n            if not has_text_layer:\n                ocr_weight = 0.7\n            elif low_quality:\n                ocr_weight = 0.6\n            else:\n                ocr_weight = 0.4\n            blended = (base_confidence * (1.0 - ocr_weight)) + (ocr_confidence * ocr_weight)\n            quality.effective_confidence_proxy = max(0.0, min(1.0, blended))\n            ocr_stats = dict(ocr_stats)\n            ocr_stats[\"ocr_confidence_normalized\"] = ocr_confidence\n            ocr_stats[\"ocr_confidence_weight\"] = ocr_weight\n\n    emit(90, \"chunking\", \"Building chunks\")\n    metadata = {\n        \"ocr_used\": decision.ocr_used,\n        \"ocr_engine\": ocr_engine_used,\n        \"external_ocr_used\": external_ocr_used,\n        \"languages\": decision.languages,\n        \"route_reason\": decision.route_reason,\n        \"per_page_reason\": decision.per_page_reason,\n        \"text_layer_detected\": has_text_layer,\n        \"text_layer_low_quality\": has_text_layer and low_quality,\n        \"text_layer_overlay\": text_layer_overlay,\n        \"rasterized_source_pdf\": rasterized_source,\n        \"rasterize_failed\": bool(rasterize_error),\n        \"rasterize_error\": rasterize_error,\n        \"column_layout_detected\": column_layout.detected if column_layout else None,\n        \"column_layout_ratio\": column_layout.page_ratio if column_layout else None,\n        \"column_layout_reason\": column_layout.reason if column_layout else None,\n        \"avg_chars_per_page\": quality.avg_chars_per_page,\n        \"alpha_ratio\": quality.alpha_ratio,\n        \"suspicious_token_ratio\": quality.suspicious_token_ratio,\n        \"confidence_proxy\": quality.confidence_proxy,\n        \"effective_confidence_proxy\": quality.effective_confidence_proxy,\n        \"dictionary_hit_ratio\": quality.dictionary_hit_ratio,\n        \"spellchecker_hit_ratio\": quality.spellchecker_hit_ratio,\n        \"image_heavy_ratio\": quality.image_heavy_ratio,\n        \"image_page_ratio\": quality.image_page_ratio,\n        \"ocr_overlay_ratio\": quality.ocr_overlay_ratio,\n        \"digital_page_ratio\": quality.digital_page_ratio,\n        \"classifier_decision\": quality.layer_classification,\n        \"classifier_sampled_pages\": classifier_info.get(\"sampled_pages\") if classifier_info else None,\n        \"classifier_short_circuit\": classifier_info.get(\"short_circuit\") if classifier_info else None,\n        \"per_page_ocr\": decision.per_page_ocr,\n    }\n    if fallback_engine:\n        metadata[\"output_fallback\"] = fallback_engine\n    # Attach spellchecker backend info if available\n    if LAST_SPELLCHECKER_INFO:\n        try:\n            metadata.update({\n                \"spellchecker_backend\": LAST_SPELLCHECKER_INFO.get(\"backend\"),\n                \"spellchecker_dic\": LAST_SPELLCHECKER_INFO.get(\"dic\"),\n                \"spellchecker_aff\": LAST_SPELLCHECKER_INFO.get(\"aff\"),\n            })\n        except Exception:\n            pass\n    metadata.update(ocr_stats)\n    emit(100, \"done\", \"Extraction complete\")\n    return DoclingConversionResult(markdown=markdown, pages=pages, metadata=metadata)\n\n\ndef build_page_heading_map(\n    markdown: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n) -> Dict[int, List[str]]:\n    headings: Dict[int, List[str]] = {}\n    if not markdown or not pages:\n        return headings\n    sections = split_markdown_sections(markdown)\n    if not sections:\n        return headings\n    for section in sections:\n        title = str(section.get(\"title\") or \"\").strip()\n        text = str(section.get(\"text\") or \"\").strip()\n        if not title or not text:\n            continue\n        page_start, _ = find_page_range(text, pages, config)\n        if page_start <= 0:\n            continue\n        headings.setdefault(int(page_start), []).append(title)\n    return headings\n\n\ndef inject_headings_inline(text: str, titles: Sequence[str]) -> str:\n    if not text or not titles:\n        return text\n    updated = text\n    for title in titles:\n        clean_title = str(title or \"\").strip()\n        if not clean_title:\n            continue\n        pattern = re.escape(clean_title).replace(\"\\\\ \", r\"\\s+\")\n        heading_line = re.compile(rf\"^\\s*#+\\s*{pattern}\\s*$\", re.IGNORECASE | re.MULTILINE)\n        if heading_line.search(updated):\n            continue\n        title_re = re.compile(rf\"(?<!\\w){pattern}(?!\\w)\", re.IGNORECASE)\n        matches = list(title_re.finditer(updated))\n        if matches:\n            match = matches[-1]\n            start, end = match.span()\n            replacement = f\"\\n\\n## {clean_title}\\n\\n\"\n            updated = updated[:start] + replacement + updated[end:]\n    return updated\n\n\ndef build_chunks_page(\n    doc_id: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    postprocess: Optional[Callable[[str, Optional[str]], str]] = None,\n    heading_map: Optional[Dict[int, List[str]]] = None,\n    table_map: Optional[Dict[int, List[str]]] = None,\n    preserve_markdown: bool = False,\n) -> List[Dict[str, Any]]:\n    chunks: List[Dict[str, Any]] = []\n    total_pages = len(pages)\n    for page in pages:\n        raw_markdown = page.get(\"markdown\") if preserve_markdown else None\n        if isinstance(raw_markdown, str) and raw_markdown.strip():\n            raw_text = raw_markdown\n            apply_postprocess = False\n        else:\n            raw_text = str(page.get(\"text\", \"\"))\n            apply_postprocess = True\n        page_num = int(page.get(\"page_num\", 0))\n        if postprocess and apply_postprocess:\n            raw_text = postprocess(raw_text, f\"page {page_num}/{total_pages}\")\n        raw_text = clean_chunk_text(raw_text, config)\n        if apply_postprocess:\n            if table_map:\n                tables = table_map.get(page_num, [])\n                if tables:\n                    raw_text = inject_markdown_tables(raw_text, tables)\n            if heading_map:\n                titles = heading_map.get(page_num, [])\n                if titles:\n                    raw_text = inject_headings_inline(raw_text, titles)\n        cleaned = normalize_display_markdown(raw_text)\n        if apply_postprocess:\n            cleaned = reflow_page_text(cleaned)\n        if not cleaned:\n            continue\n        chunk_id = f\"p{page_num}\"\n        chunks.append({\n            \"chunk_id\": chunk_id,\n            \"text\": cleaned,\n            \"page_start\": page_num,\n            \"page_end\": page_num,\n            \"section\": \"\",\n            \"char_count\": len(cleaned),\n        })\n    return chunks\n\n\ndef build_chunks_section(\n    doc_id: str,\n    markdown: str,\n    pages: List[Dict[str, Any]],\n    config: Optional[DoclingProcessingConfig] = None,\n    postprocess: Optional[Callable[[str, Optional[str]], str]] = None,\n    preserve_markdown: bool = False,\n) -> List[Dict[str, Any]]:\n    sections = split_markdown_sections(markdown)\n    chunks: List[Dict[str, Any]] = []\n    seen_ids: Dict[str, int] = {}\n\n    if not sections:\n        return build_chunks_page(doc_id, pages, config=config, preserve_markdown=preserve_markdown)\n\n    total_sections = len(sections)\n    for idx, section in enumerate(sections, start=1):\n        title = section.get(\"title\", \"\")\n        heading_line = section.get(\"heading\", \"\")\n        text = section.get(\"text\", \"\")\n        if preserve_markdown and isinstance(heading_line, str) and heading_line.strip():\n            display_text = f\"{heading_line}\\n\\n{text}\".strip() if text else heading_line.strip()\n            apply_postprocess = False\n        else:\n            display_text = text\n            apply_postprocess = True\n        if postprocess and apply_postprocess:\n            display_text = postprocess(display_text, f\"section {idx}/{total_sections}\")\n        display_text = clean_chunk_text(display_text, config)\n        if not display_text.strip():\n            continue\n        base_id = slugify(title) or f\"section-{idx}\"\n        if base_id in seen_ids:\n            seen_ids[base_id] += 1\n            base_id = f\"{base_id}-{seen_ids[base_id]}\"\n        else:\n            seen_ids[base_id] = 1\n        max_chars = config.max_chunk_chars if config else 0\n        overlap_chars = config.chunk_overlap_chars if config else 0\n        segments = split_text_by_size(display_text, max_chars, overlap_chars)\n        for seg_idx, segment in enumerate(segments, start=1):\n            cleaned = normalize_display_markdown(segment)\n            if not cleaned:\n                continue\n            page_start, page_end = find_page_range(cleaned, pages, config)\n            chunk_id = base_id if seg_idx == 1 else f\"{base_id}-{seg_idx}\"\n            chunks.append({\n                \"chunk_id\": chunk_id,\n                \"text\": cleaned,\n                \"page_start\": page_start,\n                \"page_end\": page_end,\n                \"section\": title,\n                \"char_count\": len(cleaned),\n            })\n    return chunks\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Extract PDF content with Docling and produce chunks.\")\n    parser.add_argument(\"--download-hunspell\", metavar=\"LANG_CODE\", type=str, help=\"Download Hunspell dictionary for given language code (e.g. de_DE, en_US, fr_FR)\")\n    parser.add_argument(\"--pdf\", required=False, help=\"Path to PDF\")\n    parser.add_argument(\"--doc-id\", help=\"Document identifier\")\n    parser.add_argument(\"--out-json\", help=\"Output JSON path\")\n    parser.add_argument(\"--out-md\", help=\"Output markdown path\")\n    parser.add_argument(\n        \"--image-output-dir\",\n        help=\"Directory to write extracted images (defaults to markdown output dir)\",\n    )\n    parser.add_argument(\"--config-json\", help=\"Optional path to a JSON config file (default: docling_config.json under the cache root)\")\n    parser.add_argument(\"--log-file\", help=\"Optional path to write a detailed log file\")\n    parser.add_argument(\"--spellchecker-info-out\", help=\"Optional path to write spellchecker backend info JSON\")\n    parser.add_argument(\"--chunking\", choices=[\"page\", \"section\"], default=\"page\")\n    parser.add_argument(\"--ocr\", choices=[\"auto\", \"force\", \"off\"], default=\"auto\")\n    parser.add_argument(\"--language-hint\", help=\"Language hint for OCR/quality (e.g., eng, deu, deu+eng)\")\n    parser.add_argument(\n        \"--prefer-ocr-engine\",\n        choices=[\"paddle\", \"tesseract\"],\n        help=\"Preferred external OCR engine when available.\",\n    )\n    parser.add_argument(\n        \"--fallback-ocr-engine\",\n        choices=[\"paddle\", \"tesseract\"],\n        help=\"Fallback external OCR engine when the preferred engine is unavailable.\",\n    )\n    parser.add_argument(\n        \"--paddle-structure-v3\",\n        dest=\"paddle_structure_v3\",\n        action=\"store_true\",\n        default=None,\n        help=\"Use PP-StructureV3 layout parsing for Paddle OCR\",\n    )\n    parser.add_argument(\n        \"--no-paddle-structure-v3\",\n        dest=\"paddle_structure_v3\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PP-StructureV3 layout parsing for Paddle OCR\",\n    )\n    parser.add_argument(\n        \"--paddle-structure-version\",\n        help=\"Override Paddle PP-Structure version (e.g., PP-StructureV3)\",\n    )\n    parser.add_argument(\n        \"--paddle-structure-api\",\n        dest=\"paddle_structure_api_disable\",\n        action=\"store_false\",\n        default=None,\n        help=\"Enable PP-StructureV3 API (overrides local Paddle structure).\",\n    )\n    parser.add_argument(\n        \"--no-paddle-structure-api\",\n        dest=\"paddle_structure_api_disable\",\n        action=\"store_true\",\n        default=None,\n        help=\"Disable PP-StructureV3 API.\",\n    )\n    parser.add_argument(\n        \"--paddle-structure-api-url\",\n        help=\"PP-StructureV3 API URL.\",\n    )\n    parser.add_argument(\n        \"--paddle-structure-api-token\",\n        help=\"PP-StructureV3 API token.\",\n    )\n    parser.add_argument(\n        \"--paddle-structure-api-timeout\",\n        type=int,\n        help=\"PP-StructureV3 API timeout in seconds.\",\n    )\n    parser.add_argument(\n        \"--paddle-max-dpi\",\n        type=int,\n        help=\"Max DPI for Paddle OCR rendering (overrides default cap).\",\n    )\n    parser.add_argument(\n        \"--paddle-target-max-side\",\n        dest=\"paddle_target_max_side_px\",\n        type=int,\n        help=\"Target max side length (px) for Paddle OCR rendering.\",\n    )\n    parser.add_argument(\n        \"--paddle-use-doc-orientation-classify\",\n        dest=\"paddle_use_doc_orientation_classify\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable Paddle doc orientation classify (affects PaddleOCR-VL payload).\",\n    )\n    parser.add_argument(\n        \"--no-paddle-use-doc-orientation-classify\",\n        dest=\"paddle_use_doc_orientation_classify\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable Paddle doc orientation classify.\",\n    )\n    parser.add_argument(\n        \"--paddle-use-doc-unwarping\",\n        dest=\"paddle_use_doc_unwarping\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable Paddle doc unwarping (affects PaddleOCR-VL payload).\",\n    )\n    parser.add_argument(\n        \"--no-paddle-use-doc-unwarping\",\n        dest=\"paddle_use_doc_unwarping\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable Paddle doc unwarping.\",\n    )\n    parser.add_argument(\n        \"--paddle-use-paddlex-layout\",\n        dest=\"paddle_use_paddlex_layout\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable PaddleX DocLayout path for Paddle OCR.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-use-paddlex-layout\",\n        dest=\"paddle_use_paddlex_layout\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PaddleX DocLayout path for Paddle OCR.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl\",\n        dest=\"paddle_use_vl\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable PaddleOCR-VL pipeline for Paddle OCR.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl\",\n        dest=\"paddle_use_vl\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PaddleOCR-VL pipeline for Paddle OCR.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-device\",\n        help=\"PaddleOCR-VL device (e.g., cpu, gpu:0).\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-rec-backend\",\n        help=\"PaddleOCR-VL recognition backend (e.g., vllm-server).\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-rec-server-url\",\n        help=\"PaddleOCR-VL recognition server URL.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-rec-max-concurrency\",\n        type=int,\n        help=\"PaddleOCR-VL max concurrency for recognition server.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-rec-api-key\",\n        help=\"PaddleOCR-VL recognition server API key.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-use-layout-detection\",\n        dest=\"paddle_vl_use_layout_detection\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable layout detection in PaddleOCR-VL.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl-use-layout-detection\",\n        dest=\"paddle_vl_use_layout_detection\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable layout detection in PaddleOCR-VL.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-use-chart-recognition\",\n        dest=\"paddle_vl_use_chart_recognition\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable chart recognition in PaddleOCR-VL.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl-use-chart-recognition\",\n        dest=\"paddle_vl_use_chart_recognition\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable chart recognition in PaddleOCR-VL.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-format-block-content\",\n        dest=\"paddle_vl_format_block_content\",\n        action=\"store_true\",\n        default=None,\n        help=\"Format PaddleOCR-VL block content as markdown.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl-format-block-content\",\n        dest=\"paddle_vl_format_block_content\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PaddleOCR-VL markdown formatting.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-prompt-label\",\n        help=\"PaddleOCR-VL prompt label (ocr, formula, table, chart).\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-use-queues\",\n        dest=\"paddle_vl_use_queues\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable PaddleOCR-VL internal queues for large inputs.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl-use-queues\",\n        dest=\"paddle_vl_use_queues\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PaddleOCR-VL internal queues.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-layout-threshold\",\n        type=float,\n        help=\"Layout score threshold for PaddleOCR-VL.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-layout-unclip\",\n        type=float,\n        help=\"Layout unclip ratio for PaddleOCR-VL.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-layout-merge\",\n        help=\"Layout merge mode for PaddleOCR-VL (small, large, union).\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-layout-nms\",\n        dest=\"paddle_vl_layout_nms\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable PaddleOCR-VL layout NMS.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl-layout-nms\",\n        dest=\"paddle_vl_layout_nms\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PaddleOCR-VL layout NMS.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-api\",\n        dest=\"paddle_vl_api_disable\",\n        action=\"store_false\",\n        default=None,\n        help=\"Enable PaddleOCR-VL API.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-vl-api\",\n        dest=\"paddle_vl_api_disable\",\n        action=\"store_true\",\n        default=None,\n        help=\"Disable PaddleOCR-VL API.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-api-url\",\n        help=\"PaddleOCR-VL API URL (overrides local PaddleOCR-VL).\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-api-token\",\n        help=\"PaddleOCR-VL API token.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-api-timeout\",\n        type=int,\n        help=\"PaddleOCR-VL API timeout in seconds.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-markdown-ignore-labels\",\n        help=\"Comma-separated list of layout labels to ignore in API markdown output.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-repetition-penalty\",\n        type=float,\n        help=\"PaddleOCR-VL API repetition penalty.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-temperature\",\n        type=float,\n        help=\"PaddleOCR-VL API temperature.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-top-p\",\n        type=float,\n        help=\"PaddleOCR-VL API top-p.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-min-pixels\",\n        type=int,\n        help=\"PaddleOCR-VL API min pixels.\",\n    )\n    parser.add_argument(\n        \"--paddle-vl-max-pixels\",\n        type=int,\n        help=\"PaddleOCR-VL API max pixels.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-model\",\n        help=\"PaddleX layout model (e.g., PP-DocLayout-L).\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-threshold\",\n        type=float,\n        help=\"Confidence threshold for PaddleX layout detections.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-img-size\",\n        type=int,\n        help=\"Input image size for PaddleX layout model.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-merge\",\n        help=\"PaddleX layout merge mode (e.g., small, large, union).\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-unclip\",\n        type=float,\n        help=\"PaddleX layout unclip ratio.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-device\",\n        help=\"PaddleX layout device (e.g., cpu, gpu:0).\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-keep-labels\",\n        help=\"Comma-separated list of PaddleX layout labels to OCR.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-save-crops\",\n        help=\"Directory to write Paddle layout crop images for debugging.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-md-out\",\n        help=\"Path to write raw Paddle layout markdown output for debugging.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-recognize-boxes\",\n        dest=\"paddle_layout_recognize_boxes\",\n        action=\"store_true\",\n        default=None,\n        help=\"Recognize text inside PaddleX layout boxes.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-layout-recognize-boxes\",\n        dest=\"paddle_layout_recognize_boxes\",\n        action=\"store_false\",\n        default=None,\n        help=\"Skip OCR inside PaddleX layout boxes.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-nms\",\n        dest=\"paddle_layout_nms\",\n        action=\"store_true\",\n        default=None,\n        help=\"Enable PaddleX layout NMS.\",\n    )\n    parser.add_argument(\n        \"--no-paddle-layout-nms\",\n        dest=\"paddle_layout_nms\",\n        action=\"store_false\",\n        default=None,\n        help=\"Disable PaddleX layout NMS.\",\n    )\n    parser.add_argument(\n        \"--paddle-layout-fail-on-zero\",\n        action=\"store_true\",\n        help=\"Fail if PaddleX layout detects zero boxes.\",\n    )\n    parser.add_argument(\n        \"--paddle-dump\",\n        action=\"store_true\",\n        help=\"Enable verbose Paddle layout diagnostics (similar to smoke test).\",\n    )\n    parser.add_argument(\n        \"--max-chunk-chars\",\n        type=int,\n        help=\"Max chars for section chunks before splitting (section mode only).\",\n    )\n    parser.add_argument(\n        \"--chunk-overlap-chars\",\n        type=int,\n        help=\"Overlap chars when splitting large section chunks.\",\n    )\n    parser.add_argument(\n        \"--force-ocr-low-quality\",\n        action=\"store_true\",\n        help=\"Force OCR when text layer appears low quality\",\n    )\n    parser.add_argument(\n        \"--force-per-page-ocr\",\n        action=\"store_true\",\n        help=\"Force per-page OCR and bypass layout heuristics\",\n    )\n    parser.add_argument(\n        \"--quality-threshold\",\n        type=float,\n        help=\"Confidence threshold for treating text as low quality (0-1)\",\n    )\n    parser.add_argument(\"--quality-only\", action=\"store_true\", help=\"Output text-layer quality JSON and exit\")\n    parser.add_argument(\"--enable-llm-cleanup\", action=\"store_true\", help=\"Enable LLM cleanup for low-quality chunks\")\n    parser.add_argument(\"--llm-cleanup-base-url\", help=\"OpenAI-compatible base URL for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-api-key\", help=\"API key for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-model\", help=\"Model name for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-temperature\", type=float, help=\"Temperature for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-max-chars\", type=int, help=\"Max chars per chunk for LLM cleanup\")\n    parser.add_argument(\"--llm-cleanup-min-quality\", type=float, help=\"Min quality threshold for LLM cleanup\")\n    parser.add_argument(\"--progress\", action=\"store_true\", help=\"Emit JSON progress events to stdout\")\n    parser.add_argument(\"--enable-dictionary-correction\", action=\"store_true\", help=\"Enable dictionary-based OCR corrections\")\n    parser.add_argument(\"--dictionary-path\", help=\"Path to dictionary wordlist (one word per line)\")\n    parser.add_argument(\"--enable-hunspell\", action=\"store_true\", help=\"Enable Hunspell dictionary support if available\")\n    parser.add_argument(\"--hunspell-aff\", help=\"Path to Hunspell .aff file\")\n    parser.add_argument(\"--hunspell-dic\", help=\"Path to Hunspell .dic file\")\n\n    # Parse only known args to allow --download-hunspell to work standalone\n    args, _ = parser.parse_known_args()\n\n    if args.download_hunspell:\n        lang_code = args.download_hunspell\n        # Map special cases for repo structure\n        repo_map = {\n            \"de_DE\": (\"de\", \"de_DE_frami\"),\n            \"de_AT\": (\"de\", \"de_AT\"),\n            \"de_CH\": (\"de\", \"de_CH\"),\n            \"en_US\": (\"en\", \"en_US\"),\n            \"en_GB\": (\"en\", \"en_GB\"),\n            \"fr_FR\": (\"fr_FR\", \"fr\"),\n        }\n        # Default: folder and file prefix are lang_code\n        folder, prefix = repo_map.get(lang_code, (lang_code, lang_code))\n        base_url = f\"https://raw.githubusercontent.com/LibreOffice/dictionaries/master/{folder}/\"\n        aff_name = f\"{prefix}.aff\"\n        dic_name = f\"{prefix}.dic\"\n        aff_url = base_url + aff_name\n        dic_url = base_url + dic_name\n        out_dir = os.path.join(os.path.dirname(__file__), \"hunspell\")\n        os.makedirs(out_dir, exist_ok=True)\n        aff_path = os.path.join(out_dir, f\"{lang_code}.aff\")\n        dic_path = os.path.join(out_dir, f\"{lang_code}.dic\")\n        def download(url, out_path):\n            try:\n                import urllib.request\n                urllib.request.urlretrieve(url, out_path)\n                return True\n            except Exception as exc:\n                print(f\"Failed to download {url}: {exc}\")\n                return False\n        print(f\"Downloading {aff_url} -> {aff_path}\")\n        ok_aff = download(aff_url, aff_path)\n        print(f\"Downloading {dic_url} -> {dic_path}\")\n        ok_dic = download(dic_url, dic_path)\n        if ok_aff and ok_dic:\n            print(f\"Successfully downloaded Hunspell dictionary for {lang_code} to {out_dir}\")\n            return 0\n        else:\n            print(f\"Failed to download Hunspell dictionary for {lang_code}. Check the language code or try manually.\")\n            return 1\n\n    # Require --pdf for normal operation\n    if not args.pdf:\n        parser.print_help()\n        return 2\n\n    doc_label = os.path.basename(args.pdf) if args.pdf else \"-\"\n    record_factory = logging.getLogRecordFactory()\n    def _record_factory(*factory_args, **factory_kwargs):\n        record = record_factory(*factory_args, **factory_kwargs)\n        if not hasattr(record, \"doc_name\"):\n            record.doc_name = doc_label\n        return record\n    logging.setLogRecordFactory(_record_factory)\n    log_format = \"%(asctime)sZ %(levelname)s [pid=%(process)d doc=%(doc_name)s] %(name)s: %(message)s\"\n    logging.basicConfig(level=logging.INFO, format=log_format)\n    # Ensure Docling's internal modules emit INFO logs so the CLI log file captures\n    # each pipeline stage (external OCR, layout, etc.).\n    for logger_name in [\n        \"docling\",\n        \"docling.backend\",\n        \"docling.models\",\n        \"docling.pipeline\",\n        \"docling.pipeline.standard_pdf_pipeline\",\n        \"docling_extract\",\n        \"ocr_paddle\",\n    ]:\n        logging.getLogger(logger_name).setLevel(logging.INFO)\n    class _PypdfCmapWarningFilter(logging.Filter):\n        def __init__(self) -> None:\n            super().__init__()\n            self.suppressed = 0\n\n        def filter(self, record: logging.LogRecord) -> bool:\n            if record.name == \"pypdf._cmap\":\n                message = record.getMessage()\n                if \"Skipping broken line\" in message and \"Odd-length string\" in message:\n                    self.suppressed += 1\n                    return False\n            return True\n\n    cmap_filter = _PypdfCmapWarningFilter()\n    logging.getLogger(\"pypdf._cmap\").addFilter(cmap_filter)\n\n    def _log_cmap_summary() -> None:\n        if cmap_filter.suppressed:\n            LOGGER.warning(\n                \"Suppressed %d pypdf CMap warnings (Odd-length string).\",\n                cmap_filter.suppressed,\n            )\n\n    atexit.register(_log_cmap_summary)\n    # If a log file was requested, add a file handler\n    if args.log_file:\n        try:\n            fh = logging.FileHandler(args.log_file, encoding=\"utf-8\")\n            fh.setLevel(logging.INFO)\n            formatter = logging.Formatter(log_format)\n            formatter.converter = time.gmtime\n            fh.setFormatter(formatter)\n            logging.getLogger().addHandler(fh)\n            LOGGER.info(\"Logging to file: %s\", args.log_file)\n        except Exception as exc:\n            eprint(f\"Failed to set up log file {args.log_file}: {exc}\")\n    root_logger = logging.getLogger()\n    for handler in root_logger.handlers:\n        formatter = handler.formatter\n        if formatter is None:\n            formatter = logging.Formatter(log_format)\n            handler.setFormatter(formatter)\n        formatter.converter = time.gmtime\n\n    def _resolve_config_path() -> Optional[str]:\n        if args.config_json:\n            return args.config_json\n        try:\n            if args.out_json:\n                out_dir = os.path.abspath(os.path.dirname(args.out_json))\n                root_dir = os.path.abspath(os.path.join(out_dir, os.pardir))\n                return os.path.join(root_dir, \"docling_config.json\")\n        except Exception:\n            return None\n        return None\n\n    def _load_config_overrides(path: Optional[str]) -> Dict[str, Any]:\n        if not path or not os.path.isfile(path):\n            return {}\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as fh:\n                data = json.load(fh)\n            if isinstance(data, dict):\n                return data\n        except Exception as exc:\n            LOGGER.warning(\"Failed to read config file %s: %s\", path, exc)\n        return {}\n\n    _CONFIG_FIELDS = {f.name for f in fields(DoclingProcessingConfig)}\n\n    def _maybe_write_default_config(path: Optional[str]) -> None:\n        if not path:\n            return\n        if os.path.isfile(path):\n            return\n        try:\n            default_cfg = DoclingProcessingConfig()\n            os.makedirs(os.path.dirname(path), exist_ok=True)\n            with open(path, \"w\", encoding=\"utf-8\") as fh:\n                json.dump(asdict(default_cfg), fh, indent=2)\n            LOGGER.info(\"Wrote default Docling config to %s\", path)\n        except Exception as exc:\n            LOGGER.warning(\"Failed to write default config file %s: %s\", path, exc)\n\n    def _apply_config_overrides(cfg: DoclingProcessingConfig, overrides: Dict[str, Any], source: Optional[str]) -> None:\n        if not overrides:\n            return\n        applied: List[str] = []\n        for key, val in overrides.items():\n            if key in _CONFIG_FIELDS:\n                setattr(cfg, key, val)\n                applied.append(key)\n        if applied:\n            label = source or \"config file\"\n            LOGGER.info(\n                \"Applied %d config override(s) from %s: %s\",\n                len(applied),\n                label,\n                \", \".join(sorted(applied)),\n            )\n\n\n    config_path = _resolve_config_path()\n    _maybe_write_default_config(config_path)\n    config_overrides = _load_config_overrides(config_path)\n\n    if not os.path.isfile(args.pdf):\n        eprint(f\"PDF not found: {args.pdf}\")\n        return 2\n\n    if args.quality_only:\n        config = DoclingProcessingConfig(ocr_mode=args.ocr)\n        _apply_config_overrides(config, config_overrides, config_path)\n        if args.force_ocr_low_quality:\n            config.force_ocr_on_low_quality_text = True\n        if args.force_per_page_ocr:\n            config.force_per_page_ocr = True\n        if args.quality_threshold is not None:\n            config.quality_confidence_threshold = args.quality_threshold\n        report = build_quality_report(args.pdf, config)\n        print(json.dumps(report))\n        return 0\n\n    if not args.doc_id or not args.out_json or not args.out_md:\n        eprint(\"Missing required arguments: --doc-id, --out-json, --out-md\")\n        return 2\n\n    try:\n        out_json_dir = os.path.dirname(args.out_json)\n        out_md_dir = os.path.dirname(args.out_md)\n        if out_json_dir:\n            os.makedirs(out_json_dir, exist_ok=True)\n        if out_md_dir:\n            os.makedirs(out_md_dir, exist_ok=True)\n    except Exception as exc:\n        eprint(f\"Failed to create output directories: {exc}\")\n        return 2\n\n    config = DoclingProcessingConfig(ocr_mode=args.ocr)\n    _apply_config_overrides(config, config_overrides, config_path)\n    if args.force_ocr_low_quality:\n        config.force_ocr_on_low_quality_text = True\n    if args.force_per_page_ocr:\n        config.force_per_page_ocr = True\n    if args.quality_threshold is not None:\n        config.quality_confidence_threshold = args.quality_threshold\n    if args.language_hint:\n        config.language_hint = args.language_hint\n    if args.prefer_ocr_engine:\n        config.prefer_ocr_engine = args.prefer_ocr_engine\n    if args.fallback_ocr_engine:\n        config.fallback_ocr_engine = args.fallback_ocr_engine\n    if args.paddle_structure_v3 is not None:\n        config.paddle_use_structure_v3 = args.paddle_structure_v3\n    if args.paddle_structure_version:\n        config.paddle_structure_version = args.paddle_structure_version\n    if args.paddle_structure_api_disable is not None:\n        config.paddle_structure_api_disable = args.paddle_structure_api_disable\n    if args.paddle_structure_api_url:\n        config.paddle_structure_api_url = args.paddle_structure_api_url\n    if args.paddle_structure_api_token:\n        config.paddle_structure_api_token = args.paddle_structure_api_token\n    if args.paddle_structure_api_timeout is not None:\n        config.paddle_structure_api_timeout_sec = args.paddle_structure_api_timeout\n    if args.paddle_max_dpi is not None:\n        config.paddle_max_dpi = args.paddle_max_dpi\n    if args.paddle_target_max_side_px is not None:\n        config.paddle_target_max_side_px = args.paddle_target_max_side_px\n    if args.paddle_use_doc_orientation_classify is not None:\n        config.paddle_use_doc_orientation_classify = args.paddle_use_doc_orientation_classify\n    if args.paddle_use_doc_unwarping is not None:\n        config.paddle_use_doc_unwarping = args.paddle_use_doc_unwarping\n    if args.paddle_use_paddlex_layout is not None:\n        config.paddle_use_paddlex_layout = args.paddle_use_paddlex_layout\n    if args.paddle_use_vl is not None:\n        config.paddle_use_vl = args.paddle_use_vl\n    if args.paddle_vl_device:\n        config.paddle_vl_device = args.paddle_vl_device\n    if args.paddle_vl_rec_backend:\n        config.paddle_vl_rec_backend = args.paddle_vl_rec_backend\n    if args.paddle_vl_rec_server_url:\n        config.paddle_vl_rec_server_url = args.paddle_vl_rec_server_url\n    if args.paddle_vl_rec_max_concurrency is not None:\n        config.paddle_vl_rec_max_concurrency = args.paddle_vl_rec_max_concurrency\n    if args.paddle_vl_rec_api_key:\n        config.paddle_vl_rec_api_key = args.paddle_vl_rec_api_key\n    if args.paddle_vl_use_layout_detection is not None:\n        config.paddle_vl_use_layout_detection = args.paddle_vl_use_layout_detection\n    if args.paddle_vl_use_chart_recognition is not None:\n        config.paddle_vl_use_chart_recognition = args.paddle_vl_use_chart_recognition\n    if args.paddle_vl_format_block_content is not None:\n        config.paddle_vl_format_block_content = args.paddle_vl_format_block_content\n    if args.paddle_vl_prompt_label:\n        config.paddle_vl_prompt_label = args.paddle_vl_prompt_label\n    if args.paddle_vl_use_queues is not None:\n        config.paddle_vl_use_queues = args.paddle_vl_use_queues\n    if args.paddle_vl_layout_threshold is not None:\n        config.paddle_vl_layout_threshold = args.paddle_vl_layout_threshold\n    if args.paddle_vl_layout_unclip is not None:\n        config.paddle_vl_layout_unclip = args.paddle_vl_layout_unclip\n    if args.paddle_vl_layout_merge:\n        config.paddle_vl_layout_merge = args.paddle_vl_layout_merge\n    if args.paddle_vl_layout_nms is not None:\n        config.paddle_vl_layout_nms = args.paddle_vl_layout_nms\n    if args.paddle_vl_api_disable is not None:\n        config.paddle_vl_api_disable = args.paddle_vl_api_disable\n    if args.paddle_vl_api_url:\n        config.paddle_vl_api_url = args.paddle_vl_api_url\n    if args.paddle_vl_api_token:\n        config.paddle_vl_api_token = args.paddle_vl_api_token\n    if args.paddle_vl_api_timeout is not None:\n        config.paddle_vl_api_timeout_sec = args.paddle_vl_api_timeout\n    if args.paddle_vl_markdown_ignore_labels:\n        config.paddle_vl_markdown_ignore_labels = args.paddle_vl_markdown_ignore_labels\n    if args.paddle_vl_repetition_penalty is not None:\n        config.paddle_vl_repetition_penalty = args.paddle_vl_repetition_penalty\n    if args.paddle_vl_temperature is not None:\n        config.paddle_vl_temperature = args.paddle_vl_temperature\n    if args.paddle_vl_top_p is not None:\n        config.paddle_vl_top_p = args.paddle_vl_top_p\n    if args.paddle_vl_min_pixels is not None:\n        config.paddle_vl_min_pixels = args.paddle_vl_min_pixels\n    if args.paddle_vl_max_pixels is not None:\n        config.paddle_vl_max_pixels = args.paddle_vl_max_pixels\n    if args.paddle_layout_model:\n        config.paddle_layout_model = args.paddle_layout_model\n    if args.paddle_layout_threshold is not None:\n        config.paddle_layout_threshold = args.paddle_layout_threshold\n    if args.paddle_layout_img_size is not None:\n        config.paddle_layout_img_size = args.paddle_layout_img_size\n    if args.paddle_layout_merge:\n        config.paddle_layout_merge = args.paddle_layout_merge\n    if args.paddle_layout_unclip is not None:\n        config.paddle_layout_unclip = args.paddle_layout_unclip\n    if args.paddle_layout_device:\n        config.paddle_layout_device = args.paddle_layout_device\n    if args.paddle_layout_keep_labels:\n        config.paddle_layout_keep_labels = args.paddle_layout_keep_labels\n    paddle_layout_md_out = args.paddle_layout_md_out or os.environ.get(\"ZRR_PADDLE_LAYOUT_MD_OUT\")\n    if paddle_layout_md_out:\n        config.paddle_layout_markdown_out = paddle_layout_md_out\n    if args.paddle_layout_recognize_boxes is not None:\n        config.paddle_layout_recognize_boxes = args.paddle_layout_recognize_boxes\n    if args.paddle_layout_nms is not None:\n        config.paddle_layout_nms = args.paddle_layout_nms\n    if args.paddle_layout_fail_on_zero:\n        config.paddle_layout_fail_on_zero = True\n    paddle_save_crops = args.paddle_layout_save_crops or os.environ.get(\"ZRR_PADDLE_SAVE_CROPS\")\n    if paddle_save_crops:\n        config.paddle_layout_save_crops = paddle_save_crops\n    env_paddle_dump = os.environ.get(\"ZRR_PADDLE_DUMP\")\n    if args.paddle_dump or (env_paddle_dump and env_paddle_dump.strip().lower() not in {\"\", \"0\", \"false\", \"no\"}):\n        config.paddle_dump = True\n    if args.max_chunk_chars is not None:\n        config.max_chunk_chars = args.max_chunk_chars\n    if args.chunk_overlap_chars is not None:\n        config.chunk_overlap_chars = args.chunk_overlap_chars\n    if args.enable_llm_cleanup:\n        config.enable_llm_correction = True\n    if args.enable_dictionary_correction:\n        config.enable_dictionary_correction = True\n    if args.dictionary_path:\n        config.dictionary_path = args.dictionary_path\n    if args.enable_hunspell:\n        config.enable_hunspell = True\n    if args.hunspell_aff:\n        config.hunspell_aff_path = args.hunspell_aff\n    if args.hunspell_dic:\n        config.hunspell_dic_path = args.hunspell_dic\n    if args.llm_cleanup_base_url:\n        config.llm_cleanup_base_url = args.llm_cleanup_base_url\n    if args.llm_cleanup_api_key:\n        config.llm_cleanup_api_key = args.llm_cleanup_api_key\n    if args.llm_cleanup_model:\n        config.llm_cleanup_model = args.llm_cleanup_model\n    if args.llm_cleanup_temperature is not None:\n        config.llm_cleanup_temperature = args.llm_cleanup_temperature\n    if args.llm_cleanup_max_chars is not None:\n        config.llm_correction_max_chars = args.llm_cleanup_max_chars\n    if args.llm_cleanup_min_quality is not None:\n        config.llm_correction_min_quality = args.llm_cleanup_min_quality\n\n    config.llm_correct = build_llm_cleanup_callback(config)\n\n    if paddle_save_crops:\n        reset_debug_directory(config.paddle_layout_save_crops)\n\n    # Proactively build spellchecker once to record backend info; will be reused lazily later\n    spell_langs = select_language_set(config.language_hint, args.pdf, config)\n    if config.enable_hunspell:\n        try:\n            _ = build_spellchecker_for_languages(config, spell_langs)\n        except Exception:\n            pass\n\n    # Optionally write spellchecker backend info to a file\n    if args.spellchecker_info_out:\n        try:\n            info = dict(LAST_SPELLCHECKER_INFO)\n            info[\"languages\"] = spell_langs\n            out_dir = os.path.dirname(args.spellchecker_info_out)\n            if out_dir:\n                os.makedirs(out_dir, exist_ok=True)\n            with open(args.spellchecker_info_out, \"w\", encoding=\"utf-8\") as fh:\n                json.dump(info, fh, indent=2)\n            LOGGER.info(\"Wrote spellchecker info to %s\", args.spellchecker_info_out)\n        except Exception as exc:\n            LOGGER.warning(\"Failed to write spellchecker info file: %s\", exc)\n\n    progress_cb = make_progress_emitter(bool(args.progress))\n\n    try:\n        conversion = convert_pdf_with_docling(args.pdf, config, progress_cb=progress_cb)\n    except Exception as exc:\n        eprint(f\"Docling conversion failed: {exc}\")\n        return 2\n\n    try:\n        pages = conversion.pages\n        original_pages = pages\n        languages = conversion.metadata.get(\"languages\", config.default_lang_english)\n        layout_markdown_value = conversion.metadata.get(\"layout_markdown\")\n        external_ocr_used = bool(conversion.metadata.get(\"external_ocr_used\"))\n        layout_markdown_available = isinstance(layout_markdown_value, str) and bool(layout_markdown_value.strip())\n        if layout_markdown_available and config.paddle_layout_markdown_out:\n            layout_md_path = config.paddle_layout_markdown_out\n            try:\n                out_dir = os.path.dirname(layout_md_path)\n                if out_dir:\n                    os.makedirs(out_dir, exist_ok=True)\n                with open(layout_md_path, \"w\", encoding=\"utf-8\") as fh:\n                    fh.write(str(layout_markdown_value))\n                LOGGER.info(\"Wrote Paddle layout markdown to %s\", layout_md_path)\n            except Exception as exc:\n                LOGGER.warning(\"Failed to write Paddle layout markdown to %s: %s\", layout_md_path, exc)\n        prefer_layout_markdown = external_ocr_used and layout_markdown_available\n        layout_engine_used = bool(conversion.metadata.get(\"layout_used\")) or bool(conversion.metadata.get(\"layout_model\"))\n        layout_engine_configured = bool(\n            external_ocr_used\n            and (\n                getattr(config, \"paddle_use_vl\", False)\n                or getattr(config, \"paddle_use_structure_v3\", False)\n                or getattr(config, \"paddle_use_paddlex_layout\", False)\n            )\n        )\n        layout_engine_active = layout_markdown_available or layout_engine_used or layout_engine_configured\n        postprocess_fn: Optional[Callable[[str, Optional[str]], str]] = None\n        ocr_used = bool(conversion.metadata.get(\"ocr_used\"))\n        postprocess_mode = \"none\"\n        if config.enable_post_correction:\n            if layout_engine_active:\n                postprocess_mode = \"light\"\n            elif not prefer_layout_markdown:\n                postprocess_mode = \"full\"\n        if config.enable_post_correction and postprocess_mode == \"none\":\n            LOGGER.info(\"Skipping OCR post-processing to preserve Paddle layout output.\")\n        elif postprocess_mode == \"light\":\n            LOGGER.info(\"Using light OCR post-processing for layout output.\")\n        if postprocess_mode in (\"full\", \"light\"):\n            wordlist = prepare_dictionary_words(config)\n        if postprocess_mode == \"full\":\n            allow_missing_space = ocr_used\n            def safe_postprocess(text: str, label: Optional[str]) -> str:\n                processed = postprocess_text(\n                    text,\n                    config,\n                    languages,\n                    wordlist,\n                    allow_missing_space=allow_missing_space,\n                    progress_cb=progress_cb,\n                    progress_label=label,\n                )\n                if text.strip() and not processed.strip():\n                    LOGGER.warning(\"Postprocess removed all text for %s; keeping original.\", label or \"text\")\n                    return text\n                return processed\n\n            postprocess_fn = lambda text, label=None: safe_postprocess(text, label)\n        elif postprocess_mode == \"light\":\n            postprocess_fn = lambda text, label=None: postprocess_text_light(\n                text,\n                config,\n                languages,\n                wordlist,\n                for_markdown=False,\n            )\n\n        if postprocess_fn:\n            total_pages = len(pages)\n            updated_pages: List[Dict[str, Any]] = []\n            for idx, page in enumerate(pages, start=1):\n                label = f\"page {idx}/{total_pages}\"\n                updated_page = {\n                    \"page_num\": page.get(\"page_num\", idx),\n                    \"text\": postprocess_fn(str(page.get(\"text\", \"\")), label),\n                }\n                if isinstance(page, dict) and \"markdown\" in page:\n                    updated_page[\"markdown\"] = page.get(\"markdown\")\n                updated_pages.append(updated_page)\n            pages = updated_pages\n            if ocr_pages_text_chars(pages) == 0 and ocr_pages_text_chars(original_pages) > 0:\n                LOGGER.warning(\"Postprocess removed all page text; keeping original pages.\")\n                pages = original_pages\n\n        markdown = conversion.markdown\n        if external_ocr_used:\n            if layout_markdown_available:\n                markdown = layout_markdown_value\n            else:\n                markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in pages)\n        original_markdown = markdown\n        if config.enable_post_correction and config.postprocess_markdown and postprocess_mode in (\"full\", \"light\"):\n            if progress_cb:\n                progress_cb(100, \"postprocess_markdown\", \"Postprocess markdown...\")\n            if postprocess_mode == \"full\":\n                allow_missing_space = ocr_used\n                processed_markdown = postprocess_text(\n                    markdown,\n                    config,\n                    languages,\n                    wordlist,\n                    allow_missing_space=allow_missing_space,\n                    progress_cb=progress_cb,\n                    progress_label=\"markdown\",\n                )\n            else:\n                processed_markdown = postprocess_text_light(\n                    markdown,\n                    config,\n                    languages,\n                    wordlist,\n                    for_markdown=True,\n                )\n            if original_markdown.strip() and not processed_markdown.strip():\n                LOGGER.warning(\"Postprocess removed all markdown; keeping original.\")\n                markdown = original_markdown\n            else:\n                markdown = processed_markdown\n\n        repeated_clusters: List[BoilerplateCluster] = []\n        if config.enable_boilerplate_removal and not external_ocr_used:\n            pre_boilerplate_pages = pages\n            pre_boilerplate_markdown = markdown\n            pages, repeated_clusters, _ = remove_boilerplate_from_pages(pages, config)\n            markdown = remove_boilerplate_from_markdown(markdown, repeated_clusters, config)\n            if not has_output_text(markdown, pages) and has_output_text(pre_boilerplate_markdown, pre_boilerplate_pages):\n                LOGGER.warning(\"Boilerplate removal removed all text; keeping original.\")\n                pages = pre_boilerplate_pages\n                markdown = pre_boilerplate_markdown\n\n        if external_ocr_used and not layout_markdown_available:\n            markdown = \"\\n\\n\".join(page.get(\"text\", \"\") for page in pages)\n\n        if prefer_layout_markdown:\n            image_labels = conversion.metadata.get(\"layout_markdown_image_labels\")\n            if not isinstance(image_labels, dict):\n                image_labels = None\n            markdown = convert_html_images_to_obsidian(markdown, image_labels=image_labels)\n            if isinstance(pages, list):\n                for page in pages:\n                    if isinstance(page, dict) and isinstance(page.get(\"markdown\"), str):\n                        page[\"markdown\"] = convert_html_images_to_obsidian(\n                            page[\"markdown\"],\n                            image_labels=image_labels,\n                        )\n            layout_images = conversion.metadata.get(\"layout_markdown_images\")\n            if isinstance(layout_images, dict):\n                remapped_images = remap_layout_image_keys(layout_images)\n                conversion.metadata[\"layout_markdown_images\"] = remapped_images\n                if isinstance(image_labels, dict):\n                    remapped_labels: Dict[str, str] = {}\n                    for key, label in image_labels.items():\n                        if not isinstance(key, str) or not isinstance(label, str):\n                            continue\n                        filename = _extract_image_filename(key)\n                        if filename:\n                            remapped_labels.setdefault(filename, label)\n                        else:\n                            remapped_labels.setdefault(key, label)\n                    conversion.metadata[\"layout_markdown_image_labels\"] = remapped_labels\n\n        if not markdown.strip():\n            LOGGER.warning(\"Markdown empty; rebuilding from %d pages\", len(pages))\n            markdown = \"\\n\\n\".join(str(page.get(\"text\", \"\")) for page in pages)\n\n        if not has_output_text(markdown, pages):\n            eprint(\"Extraction produced empty output after fallback attempts.\")\n            return 2\n\n        LOGGER.info(\"Docling output: pages=%d, markdown_chars=%d\", len(pages), len(markdown))\n\n        layout_images = conversion.metadata.get(\"layout_markdown_images\")\n        if isinstance(layout_images, dict):\n            conversion.metadata[\"layout_markdown_image_paths\"] = sorted(\n                path for path in layout_images.keys() if isinstance(path, str) and path\n            )\n            conversion.metadata.pop(\"layout_markdown_images\", None)\n            image_output_dir = args.image_output_dir\n            if image_output_dir:\n                image_output_dir = image_output_dir.strip()\n                if image_output_dir and not os.path.isabs(image_output_dir):\n                    if args.out_md:\n                        image_output_dir = os.path.join(os.path.dirname(args.out_md), image_output_dir)\n                    else:\n                        image_output_dir = os.path.abspath(image_output_dir)\n            out_md_dir = os.path.dirname(args.out_md) if args.out_md else \"\"\n            for rel_path, image_obj in layout_images.items():\n                if not isinstance(rel_path, str) or not rel_path:\n                    continue\n                target_path = rel_path\n                if not os.path.isabs(rel_path):\n                    if image_output_dir:\n                        target_path = os.path.join(image_output_dir, rel_path)\n                    elif out_md_dir:\n                        target_path = os.path.join(out_md_dir, rel_path)\n                    else:\n                        continue\n                    try:\n                        target_dir = os.path.dirname(target_path)\n                        if target_dir:\n                            os.makedirs(target_dir, exist_ok=True)\n                        if hasattr(image_obj, \"save\"):\n                            image_obj.save(target_path)\n                            continue\n                        if isinstance(image_obj, str):\n                            image_ref = image_obj.strip()\n                            if image_ref.startswith(\"data:\") and \"base64,\" in image_ref:\n                                try:\n                                    _, encoded = image_ref.split(\"base64,\", 1)\n                                    data = base64.b64decode(encoded)\n                                    with open(target_path, \"wb\") as handle:\n                                        handle.write(data)\n                                    continue\n                                except Exception as exc:\n                                    LOGGER.warning(\"Failed to decode data URI for %s: %s\", rel_path, exc)\n                            if image_ref.startswith((\"http://\", \"https://\")):\n                                try:\n                                    with urllib.request.urlopen(image_ref, timeout=30) as resp:\n                                        content = resp.read()\n                                    with open(target_path, \"wb\") as handle:\n                                        handle.write(content)\n                                    continue\n                                except (urllib.error.URLError, ValueError) as exc:\n                                    LOGGER.warning(\"Failed to download layout image %s: %s\", rel_path, exc)\n                        try:\n                            import numpy as _np\n                            from PIL import Image as _PILImage\n\n                            if isinstance(image_obj, _np.ndarray):\n                                _PILImage.fromarray(image_obj).save(target_path)\n                        except Exception:\n                            continue\n                    except Exception as exc:\n                        LOGGER.warning(\"Failed to save layout image %s: %s\", rel_path, exc)\n\n        try:\n            with open(args.out_md, \"w\", encoding=\"utf-8\") as handle:\n                handle.write(markdown)\n        except Exception as exc:\n            eprint(f\"Failed to write markdown: {exc}\")\n            return 2\n\n        preserve_markdown_chunks = prefer_layout_markdown\n        if args.chunking == \"section\":\n            chunks = build_chunks_section(\n                args.doc_id,\n                markdown,\n                pages,\n                config=config,\n                postprocess=postprocess_fn,\n                preserve_markdown=preserve_markdown_chunks,\n            )\n        else:\n            heading_map = build_page_heading_map(markdown, pages, config)\n            table_map = build_page_table_map(markdown, pages, config)\n            chunks = build_chunks_page(\n                args.doc_id,\n                pages,\n                config=config,\n                postprocess=postprocess_fn,\n                heading_map=heading_map,\n                table_map=table_map,\n                preserve_markdown=preserve_markdown_chunks,\n            )\n    except Exception as exc:\n        eprint(f\"Failed to build chunks: {exc}\")\n        return 2\n\n    chunks = [chunk for chunk in chunks if chunk.get(\"text\")]\n\n    payload = {\n        \"doc_id\": args.doc_id,\n        \"source_pdf\": args.pdf,\n        \"chunks\": chunks,\n        \"metadata\": conversion.metadata,\n    }\n\n    try:\n        with open(args.out_json, \"w\", encoding=\"utf-8\") as handle:\n            json.dump(payload, handle, indent=2)\n    except Exception as exc:\n        eprint(f\"Failed to write JSON: {exc}\")\n        return 2\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "ocr_paddle.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport numbers\nimport os\nimport re\nimport tempfile\nimport time\nimport warnings\nfrom collections import Counter\nfrom typing import Any, Dict, List, Optional, Sequence, Set, Tuple\n\nLOGGER = logging.getLogger(\"docling_extract\")\n\n_INLINE_MATH_RE = re.compile(r\"(?<!\\$)\\$(?!\\$)([^$\\n]+?)\\$(?!\\$)\")\n_CURRENCY_THOUSANDS_RE = re.compile(r\"^[+-]?\\d{1,3}(?:[.,]\\d{3})+(?:[.,]\\d+)?%?$\")\n_CURRENCY_DECIMAL_RE = re.compile(r\"^[+-]?\\d+[.,]\\d+%?$\")\n_CURRENCY_CODES = {\n    \"USD\",\n    \"EUR\",\n    \"GBP\",\n    \"JPY\",\n    \"CNY\",\n    \"RMB\",\n    \"AUD\",\n    \"CAD\",\n    \"CHF\",\n    \"HKD\",\n    \"NZD\",\n    \"SEK\",\n    \"NOK\",\n    \"DKK\",\n    \"INR\",\n    \"KRW\",\n    \"BRL\",\n    \"MXN\",\n}\n_FRACTION_MAP: Dict[Tuple[int, int], str] = {\n    (1, 2): \"½\",\n    (1, 3): \"⅓\",\n    (2, 3): \"⅔\",\n    (1, 4): \"¼\",\n    (3, 4): \"¾\",\n    (1, 5): \"⅕\",\n    (2, 5): \"⅖\",\n    (3, 5): \"⅗\",\n    (4, 5): \"⅘\",\n    (1, 6): \"⅙\",\n    (5, 6): \"⅚\",\n    (1, 8): \"⅛\",\n    (3, 8): \"⅜\",\n    (5, 8): \"⅝\",\n    (7, 8): \"⅞\",\n}\n\n\ndef _extract_footnote_marker(value: str) -> Optional[str]:\n    match = re.fullmatch(r\"\\^\\s*\\{?\\s*([^\\s{}]+)\\s*\\}?\\s*\", value)\n    if not match:\n        return None\n    marker = match.group(1).strip()\n    if marker.startswith(\"\\\\\") and len(marker) == 2:\n        marker = marker[1:]\n    return marker or None\n\n\ndef _replace_simple_fraction(value: str) -> Optional[str]:\n    match = re.fullmatch(r\"(\\d+)\\s*/\\s*(\\d+)\", value)\n    if match:\n        return _FRACTION_MAP.get((int(match.group(1)), int(match.group(2))))\n    match = re.fullmatch(r\"\\\\frac\\{\\s*(\\d+)\\s*\\}\\{\\s*(\\d+)\\s*\\}\", value)\n    if match:\n        return _FRACTION_MAP.get((int(match.group(1)), int(match.group(2))))\n    return None\n\n\ndef _find_footnotes_section(lines: List[str]) -> Optional[Tuple[int, int]]:\n    for idx, line in enumerate(lines):\n        if re.match(r\"^#{1,6}\\s+footnotes\\s*$\", line.strip(), re.IGNORECASE):\n            end = len(lines)\n            for jdx in range(idx + 1, len(lines)):\n                if re.match(r\"^#{1,6}\\s+\", lines[jdx]):\n                    end = jdx\n                    break\n            return idx, end\n    return None\n\n\ndef _normalize_footnote_definition_line(line: str) -> Optional[str]:\n    stripped = line.strip()\n    if not stripped.startswith(\"[^\"):\n        return None\n    match = re.match(r\"^\\[\\^\\s*([^\\]\\s]+)\\s*\\]\\s*:?\\s*(.*)$\", stripped)\n    if not match:\n        return None\n    marker = match.group(1).strip()\n    rest = match.group(2).strip()\n    if rest:\n        return f\"[^{marker}]: {rest}\"\n    return f\"[^{marker}]:\"\n\n\ndef _looks_like_currency(value: str) -> bool:\n    stripped = value.strip()\n    if not stripped:\n        return False\n    if re.search(r\"[\\\\^_{}=<>\\[\\]]\", stripped):\n        return False\n    if re.search(r\"[*/]\", stripped):\n        return False\n    if re.search(r\"[+\\-]\", stripped) and not re.fullmatch(r\"[+-]?\\d+(?:[.,]\\d+)?%?\", stripped):\n        return False\n    letters = re.findall(r\"[A-Za-z]+\", stripped)\n    if letters:\n        if re.search(r\"\\d\", stripped):\n            codes = {letter.upper() for letter in letters}\n            return all(code in _CURRENCY_CODES for code in codes)\n        return False\n    return bool(_CURRENCY_THOUSANDS_RE.fullmatch(stripped) or _CURRENCY_DECIMAL_RE.fullmatch(stripped))\n\n\ndef _normalize_inline_math_for_obsidian(markdown: str, add_footnote_defs: bool = False) -> str:\n    if not markdown:\n        return markdown\n    footnotes: List[str] = []\n\n    def _replace(match: re.Match[str]) -> str:\n        content = match.group(1)\n        normalized = content.strip()\n        if not normalized:\n            return match.group(0)\n        marker = _extract_footnote_marker(normalized)\n        if marker:\n            footnotes.append(marker)\n            return f\"[^{marker}]\"\n        fraction = _replace_simple_fraction(normalized)\n        if fraction:\n            return fraction\n        if _looks_like_currency(normalized):\n            return f\"\\\\${normalized}\\\\$\"\n        return f\"${normalized}$\"\n\n    updated = _INLINE_MATH_RE.sub(_replace, markdown)\n    lines = updated.splitlines()\n    normalized_any = False\n    for idx, line in enumerate(lines):\n        normalized = _normalize_footnote_definition_line(line)\n        if normalized is not None:\n            lines[idx] = normalized\n            normalized_any = True\n    if normalized_any:\n        updated = \"\\n\".join(lines)\n    if not add_footnote_defs or not footnotes:\n        return updated\n\n    footnote_ids = list(dict.fromkeys(footnotes))\n    lines = updated.splitlines()\n    section = _find_footnotes_section(lines)\n    if section:\n        start, end = section\n        for idx in range(start + 1, end):\n            normalized = _normalize_footnote_definition_line(lines[idx])\n            if normalized is not None:\n                lines[idx] = normalized\n            else:\n                lines[idx] = lines[idx].rstrip()\n        updated = \"\\n\".join(lines)\n\n    missing = [\n        marker\n        for marker in footnote_ids\n        if not re.search(rf\"(?m)^\\[\\^{re.escape(marker)}\\]:\", updated)\n    ]\n    if not missing:\n        return updated\n\n    if section:\n        insert_at = section[1]\n        insertion: List[str] = []\n        if insert_at > 0 and lines[insert_at - 1].strip():\n            insertion.append(\"\")\n        insertion.extend([f\"[^{marker}]:\" for marker in missing])\n        lines[insert_at:insert_at] = insertion\n        return \"\\n\".join(lines)\n\n    suffix_lines = [\"\", \"## Footnotes\", \"\"]\n    suffix_lines.extend([f\"[^{marker}]:\" for marker in missing])\n    if updated and not updated.endswith(\"\\n\"):\n        updated += \"\\n\"\n    return updated + \"\\n\".join(suffix_lines)\n\n\ndef _paddlex_layout_ocr_pages(\n    images: Sequence[Any],\n    languages: str,\n    config: Any,\n    helpers: Dict[str, Any],\n    progress_cb: Optional[Any] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    global LOGGER\n    LOGGER = helpers.get(\"logger\", LOGGER)\n    ocr_pages_text_chars = helpers[\"ocr_pages_text_chars\"]\n\n    try:\n        import numpy as np\n        from paddleocr import PaddleOCR\n        from PIL import Image as _PILImage, ImageOps, ImageFilter\n    except Exception as exc:\n        raise RuntimeError(f\"PaddleX layout OCR dependencies missing: {exc}\") from exc\n\n    layout_model = str(getattr(config, \"paddle_layout_model\", \"PP-DocLayout-L\"))\n    layout_threshold = getattr(config, \"paddle_layout_threshold\", 0.5)\n    layout_img_size = getattr(config, \"paddle_layout_img_size\", None)\n    layout_merge = getattr(config, \"paddle_layout_merge\", \"small\")\n    layout_unclip = getattr(config, \"paddle_layout_unclip\", 1.05)\n    crop_padding = int(getattr(config, \"paddle_crop_padding\", 0))\n    crop_vbias = int(getattr(config, \"paddle_crop_vbias\", 0))\n    layout_device = getattr(config, \"paddle_layout_device\", None)\n    layout_nms = bool(getattr(config, \"paddle_layout_nms\", True))\n    layout_keep_labels = str(\n        getattr(\n            config,\n            \"paddle_layout_keep_labels\",\n            \"text,paragraph_title,title,heading,caption,header,number,figure_title,body,section,text_block,textbox,textline,paragraph\",\n        )\n    )\n    layout_recognize_boxes = bool(getattr(config, \"paddle_layout_recognize_boxes\", True))\n    fail_on_zero_layout = bool(getattr(config, \"paddle_layout_fail_on_zero\", False))\n    max_side_px = int(getattr(config, \"paddle_target_max_side_px\", 0) or 0)\n    use_file_path = bool(getattr(config, \"paddle_layout_use_file_path\", True))\n    save_crops_dir = getattr(config, \"paddle_layout_save_crops\", None)\n    dump = bool(getattr(config, \"paddle_dump\", False))\n    if save_crops_dir:\n        LOGGER.info(\"Paddle layout crop debugging enabled: %s\", save_crops_dir)\n\n    def _dump_log(message: str, *args: Any) -> None:\n        if not dump:\n            return\n        LOGGER.info(\"Paddle dump: \" + message, *args)\n\n    _PILImage.MAX_IMAGE_PIXELS = None  # type: ignore[attr-defined]\n    if hasattr(_PILImage, \"DecompressionBombWarning\"):\n        warnings.filterwarnings(\"ignore\", category=_PILImage.DecompressionBombWarning)  # type: ignore[attr-defined]\n\n    ocr_kwargs: Dict[str, Any] = {\"lang\": languages}\n    if max_side_px > 0:\n        ocr_kwargs[\"text_det_limit_side_len\"] = max_side_px\n        ocr_kwargs[\"text_det_limit_type\"] = \"max\"\n    if getattr(config, \"paddle_use_doc_orientation_classify\", False):\n        ocr_kwargs[\"use_doc_orientation_classify\"] = True\n    if getattr(config, \"paddle_use_doc_unwarping\", False):\n        ocr_kwargs[\"use_doc_unwarping\"] = True\n    if getattr(config, \"paddle_use_textline_orientation\", None) is not None:\n        ocr_kwargs[\"use_textline_orientation\"] = bool(config.paddle_use_textline_orientation)\n\n    def _create_ocr_direct(kwargs: Dict[str, Any]) -> PaddleOCR:\n        return PaddleOCR(**kwargs)\n\n    def _try_create_direct(kwargs: Dict[str, Any]) -> Optional[PaddleOCR]:\n        try:\n            return _create_ocr_direct(kwargs)\n        except TypeError:\n            return None\n        except Exception:\n            return None\n\n    reduced_kwargs = dict(ocr_kwargs)\n    reduced_kwargs.pop(\"use_doc_orientation_classify\", None)\n    reduced_kwargs.pop(\"use_doc_unwarping\", None)\n\n    ctor_candidates: List[Dict[str, Any]] = []\n    use_tlo = bool(getattr(config, \"paddle_use_textline_orientation\", True))\n    ctor_candidates.append({**ocr_kwargs, \"use_textline_orientation\": use_tlo})\n    ctor_candidates.append({**reduced_kwargs, \"use_textline_orientation\": use_tlo})\n    ctor_candidates.append({**ocr_kwargs})\n    ctor_candidates.append({**reduced_kwargs})\n    ctor_candidates.append({**ocr_kwargs, \"use_angle_cls\": use_tlo})\n    ctor_candidates.append({**reduced_kwargs, \"use_angle_cls\": use_tlo})\n\n    ocr: Optional[PaddleOCR] = None\n    for kw in ctor_candidates:\n        ocr = _try_create_direct(kw)\n        if ocr is not None:\n            break\n\n    def _paddle_obj_to_dict(obj: Any) -> Optional[Dict[str, Any]]:\n        if obj is None:\n            return None\n        if isinstance(obj, dict):\n            return obj\n        to_dict = getattr(obj, \"to_dict\", None)\n        if callable(to_dict):\n            try:\n                converted = to_dict()\n                if isinstance(converted, dict):\n                    return converted\n            except Exception:\n                return None\n        rec_texts = getattr(obj, \"rec_texts\", None)\n        dt_polys = getattr(obj, \"dt_polys\", None)\n        if rec_texts is not None or dt_polys is not None:\n            return {\"rec_texts\": rec_texts, \"dt_polys\": dt_polys, \"rec_scores\": getattr(obj, \"rec_scores\", None)}\n        return None\n\n    def _extract_from_dict(res: Dict[str, Any]) -> List[str]:\n        texts = res.get(\"rec_texts\") or res.get(\"texts\") or res.get(\"rec_text\")\n        if not isinstance(texts, list):\n            return []\n        return [str(text or \"\").strip() for text in texts if str(text or \"\").strip()]\n\n    def _extract_texts(res: Any) -> List[str]:\n        if isinstance(res, dict):\n            return _extract_from_dict(res)\n        if isinstance(res, list):\n            entries = res\n            if len(res) == 1:\n                maybe_dict = _paddle_obj_to_dict(res[0])\n                if maybe_dict is not None:\n                    return _extract_from_dict(maybe_dict)\n                if isinstance(res[0], (list, tuple, dict)):\n                    entries = res[0]\n            if isinstance(entries, dict):\n                return _extract_from_dict(entries)\n            if isinstance(entries, list) and entries and isinstance(entries[0], dict):\n                combined: List[str] = []\n                for entry in entries:\n                    if isinstance(entry, dict):\n                        combined.extend(_extract_from_dict(entry))\n                    else:\n                        maybe_dict = _paddle_obj_to_dict(entry)\n                        if maybe_dict is not None:\n                            combined.extend(_extract_from_dict(maybe_dict))\n                return combined\n            texts: List[str] = []\n            for entry in entries:\n                if not entry or not isinstance(entry, (list, tuple)) or len(entry) < 2:\n                    continue\n                text_part = entry[1]\n                if isinstance(text_part, (list, tuple)) and text_part:\n                    text_val = str(text_part[0] or \"\").strip()\n                else:\n                    text_val = str(text_part or \"\").strip()\n                if text_val:\n                    texts.append(text_val)\n            return texts\n        return []\n\n    def _ocr_predict(image: Any, det: Optional[bool] = None, rec: Optional[bool] = None, cls: Optional[bool] = None) -> Any:\n        if ocr is None or not hasattr(ocr, \"predict\"):\n            return None\n        try:\n            if det is None and rec is None and cls is None:\n                return ocr.predict(image)  # type: ignore[attr-defined]\n            return ocr.predict(image, det=det, rec=rec, cls=cls)  # type: ignore[attr-defined]\n        except TypeError:\n            try:\n                return ocr.predict(image)  # type: ignore[attr-defined]\n            except Exception:\n                return None\n        except Exception:\n            return None\n\n    def _ocr_legacy(image: Any, **kwargs: Any) -> Any:\n        if ocr is None or not hasattr(ocr, \"ocr\"):\n            return None\n        try:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\",\n                    message=\"Please use `predict` instead.\",\n                    category=DeprecationWarning,\n                )\n                return ocr.ocr(image, **kwargs)  # type: ignore[attr-defined]\n        except TypeError:\n            return None\n        except Exception:\n            return None\n\n    def _strip_html(text: str) -> str:\n        return re.sub(r\"<[^>]+>\", \" \", text)\n\n    def _block_to_dict(block: Any) -> Dict[str, Any]:\n        if isinstance(block, dict):\n            return block\n        to_dict = getattr(block, \"to_dict\", None)\n        if callable(to_dict):\n            try:\n                converted = to_dict()\n                if isinstance(converted, dict):\n                    return converted\n            except Exception:\n                return {}\n        return {}\n\n    def _extract_block_lines(block: Dict[str, Any]) -> List[str]:\n        res = block.get(\"res\") or block.get(\"text\") or block.get(\"content\")\n        if isinstance(res, str):\n            cleaned = _strip_html(res).strip()\n            return [cleaned] if cleaned else []\n        if isinstance(res, dict):\n            text_val = res.get(\"text\")\n            if isinstance(text_val, str):\n                cleaned = text_val.strip()\n                return [cleaned] if cleaned else []\n            html_val = res.get(\"html\")\n            if isinstance(html_val, str):\n                cleaned = _strip_html(html_val).strip()\n                return [cleaned] if cleaned else []\n        if isinstance(res, list):\n            lines: List[str] = []\n            for item in res:\n                if isinstance(item, str):\n                    s = item.strip()\n                    if s:\n                        lines.append(s)\n                elif isinstance(item, dict):\n                    tv = item.get(\"text\")\n                    if isinstance(tv, str):\n                        s = tv.strip()\n                        if s:\n                            lines.append(s)\n            return lines\n        return []\n\n    crop_seq = 0\n\n    def _paddlex_structure_extract_texts(\n        image_obj: Any,\n        lang: str,\n        src_path: Optional[str] = None,\n        page_num: Optional[int] = None,\n    ) -> Tuple[List[str], bool, List[List[Dict[str, Any]]], int, int]:\n        try:\n            from paddlex import create_model\n        except Exception as exc:\n            LOGGER.warning(\"PaddleX create_model import failed: %s\", exc)\n            return [], False, [], 0, 0\n\n        cm_kwargs: Dict[str, Any] = {\"model_name\": layout_model}\n        if layout_device:\n            cm_kwargs[\"device\"] = layout_device\n        img_size = layout_img_size\n        try:\n            if img_size:\n                model = create_model(**{**cm_kwargs, \"img_size\": img_size})\n            else:\n                model = create_model(**cm_kwargs)\n        except Exception as exc:\n            msg = str(exc)\n            LOGGER.warning(\"PaddleX create_model('%s') failed: %s\", layout_model, msg)\n            if img_size is not None and (\"not supported set input shape\" in msg.lower() or \"not supported\" in msg.lower()):\n                LOGGER.info(\"PaddleX model does not support overriding img_size; retrying with default config.\")\n                try:\n                    model = create_model(**cm_kwargs)\n                except Exception as exc2:\n                    LOGGER.warning(\"PaddleX create_model retry without img_size failed: %s\", exc2)\n                    return [], False, [], 0, 0\n            else:\n                return [], False, [], 0, 0\n        try:\n            predict_kwargs: Dict[str, Any] = {\"batch_size\": 1}\n            if layout_threshold is not None:\n                predict_kwargs[\"threshold\"] = layout_threshold\n            predict_kwargs[\"layout_nms\"] = bool(layout_nms)\n            if layout_unclip is not None:\n                predict_kwargs[\"layout_unclip_ratio\"] = layout_unclip\n            if layout_merge is not None:\n                predict_kwargs[\"layout_merge_bboxes_mode\"] = layout_merge\n            if src_path and isinstance(src_path, str):\n                out_gen = model.predict(src_path, **predict_kwargs)\n            else:\n                out_gen = model.predict(np.array(image_obj), **predict_kwargs)\n            output = list(out_gen)\n        except Exception as exc:\n            LOGGER.warning(\"PaddleX layout predict failed: %s\", exc)\n            return [], False, [], 0, 0\n\n        layout_has_boxes = False\n        total = 0\n        try:\n            if isinstance(output, (list, tuple)):\n                for res in output:\n                    try:\n                        maybe = _paddle_obj_to_dict(res)\n                    except Exception:\n                        maybe = None\n                    if isinstance(maybe, dict):\n                        dets = (\n                            maybe.get(\"boxes\")\n                            or maybe.get(\"layout\")\n                            or maybe.get(\"result\")\n                            or maybe.get(\"dt_polys\")\n                            or maybe.get(\"predictions\")\n                            or []\n                        )\n                        if isinstance(dets, (list, tuple)):\n                            total += len(dets)\n                            continue\n                    res_json = getattr(res, \"json\", None)\n                    if res_json is None and isinstance(res, dict):\n                        res_json = res\n                    if isinstance(res_json, dict):\n                        dets = res_json.get(\"boxes\") or res_json.get(\"layout\") or res_json.get(\"result\") or []\n                        total += len(dets) if isinstance(dets, (list, tuple)) else 0\n            layout_has_boxes = total > 0\n            _dump_log(\"PaddleX layout detections: %d\", total)\n            if dump:\n                try:\n                    _dump_log(\"PaddleX raw output length: %d\", len(output))\n                    if output:\n                        first = output[0]\n                        _dump_log(\"PaddleX first output type: %s\", type(first))\n                        try:\n                            first_repr = repr(first)\n                            if first_repr:\n                                _dump_log(\"PaddleX first output repr: %s\", first_repr[:200])\n                        except Exception:\n                            pass\n                        try:\n                            maybe = _paddle_obj_to_dict(first)\n                        except Exception:\n                            maybe = None\n                        if isinstance(maybe, dict):\n                            try:\n                                _dump_log(\"PaddleX first output dict keys: %s\", sorted(maybe.keys()))\n                            except Exception:\n                                _dump_log(\"PaddleX first output dict keys: %s\", list(maybe.keys()))\n                            for field in (\"boxes\", \"dt_polys\", \"rec_texts\", \"predictions\"):\n                                if field in maybe:\n                                    try:\n                                        _dump_log(\"  %s length: %d\", field, len(maybe[field]))\n                                    except Exception:\n                                        pass\n                except Exception:\n                    pass\n        except Exception:\n            pass\n        if total == 0 and fail_on_zero_layout:\n            raise RuntimeError(\"PaddleX layout detected 0 boxes and fail_on_zero_layout is enabled.\")\n\n        if not layout_recognize_boxes:\n            return [], layout_has_boxes, [], total, 0\n\n        boxes: List[Any] = []\n        try:\n            first = output[0] if isinstance(output, (list, tuple)) and output else None\n            maybe = _paddle_obj_to_dict(first)\n            if isinstance(maybe, dict):\n                raw_boxes = maybe.get(\"boxes\") or []\n                if isinstance(raw_boxes, (list, tuple)):\n                    boxes = list(raw_boxes)\n        except Exception:\n            boxes = []\n\n        if not boxes:\n            return [], layout_has_boxes, [], total, 0\n\n        layout_has_boxes = True\n\n        def _iter_ocr_entries(res: Any) -> List[Tuple[Any, str]]:\n            out: List[Tuple[Any, str]] = []\n            try:\n                maybe = _paddle_obj_to_dict(res)\n                if isinstance(maybe, dict):\n                    texts = maybe.get(\"rec_texts\") or maybe.get(\"texts\") or maybe.get(\"rec_text\")\n                    box_list = (\n                        maybe.get(\"dt_polys\")\n                        or maybe.get(\"det_polys\")\n                        or maybe.get(\"dt_boxes\")\n                        or maybe.get(\"boxes\")\n                    )\n                    if isinstance(texts, list):\n                        for i, tv in enumerate(texts):\n                            s = str(tv or \"\").strip()\n                            if not s:\n                                continue\n                            quad = None\n                            if isinstance(box_list, list) and i < len(box_list):\n                                quad = box_list[i]\n                            out.append((quad, s))\n                        return out\n            except Exception:\n                pass\n            if isinstance(res, dict):\n                texts = res.get(\"rec_texts\") or res.get(\"texts\") or res.get(\"rec_text\")\n                box_list = (\n                    res.get(\"dt_polys\")\n                    or res.get(\"det_polys\")\n                    or res.get(\"dt_boxes\")\n                    or res.get(\"boxes\")\n                )\n                if isinstance(texts, list):\n                    for i, tv in enumerate(texts):\n                        s = str(tv or \"\").strip()\n                        if not s:\n                            continue\n                        quad = None\n                        if isinstance(box_list, list) and i < len(box_list):\n                            quad = box_list[i]\n                        out.append((quad, s))\n                return out\n            if isinstance(res, list):\n                entries = res\n                if len(res) == 1:\n                    maybe = _paddle_obj_to_dict(res[0])\n                    if isinstance(maybe, dict):\n                        return _iter_ocr_entries(maybe)\n                    if isinstance(res[0], (list, tuple, dict)):\n                        entries = res[0]\n                if isinstance(entries, dict):\n                    return _iter_ocr_entries(entries)\n                for entry in entries:\n                    if isinstance(entry, str):\n                        s = entry.strip()\n                        if s:\n                            out.append((None, s))\n                        continue\n                    if not isinstance(entry, (list, tuple)):\n                        continue\n                    if entry and isinstance(entry[0], str):\n                        s = str(entry[0] or \"\").strip()\n                        if s:\n                            out.append((None, s))\n                        continue\n                    quad = entry[0] if len(entry) > 0 else None\n                    text_part = entry[1] if len(entry) > 1 else None\n                    if isinstance(text_part, (list, tuple)) and text_part:\n                        s = str(text_part[0] or \"\").strip()\n                    else:\n                        s = str(text_part or \"\").strip()\n                    if s:\n                        out.append((quad, s))\n                return out\n            return out\n\n        def _bbox_from_quad(quad: Any) -> Optional[Tuple[float, float, float, float, float]]:\n            try:\n                if isinstance(quad, (list, tuple)) and quad and isinstance(quad[0], (list, tuple)):\n                    xs = [float(p[0]) for p in quad]\n                    ys = [float(p[1]) for p in quad]\n                    x0, y0, x1, y1 = min(xs), min(ys), max(xs), max(ys)\n                    return x0, y0, x1, y1, 0.5 * (x0 + x1)\n            except Exception:\n                return None\n            return None\n\n        def _order_blocks_into_columns(blocks: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:\n            if not blocks:\n                return []\n\n            def _center_y(block: Dict[str, Any]) -> float:\n                try:\n                    return 0.5 * (float(block.get(\"y0\", 0.0)) + float(block.get(\"y1\", 0.0)))\n                except Exception:\n                    return 0.0\n\n            def _is_full_width(block: Dict[str, Any]) -> bool:\n                page_width = max(1.0, float(w or 1))\n                try:\n                    width = float(block.get(\"x1\", 0.0)) - float(block.get(\"x0\", 0.0))\n                except Exception:\n                    width = 0.0\n                if width <= 0.0:\n                    return False\n                ratio = width / page_width\n                label = str(block.get(\"label\", \"\")).strip().lower()\n                full_labels = {\"title\", \"heading\", \"header\", \"paragraph_title\", \"figure_title\", \"caption\"}\n                if ratio >= 0.85:\n                    return True\n                if label in full_labels and ratio >= 0.6:\n                    return True\n                return False\n\n            def _order_columns(col_blocks: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:\n                if not col_blocks:\n                    return []\n                xs = sorted(b[\"xc\"] for b in col_blocks)\n                span = max(1.0, xs[-1] - xs[0]) if xs else 1.0\n                widths = sorted((b[\"x1\"] - b[\"x0\"]) for b in col_blocks)\n                w_med = widths[len(widths) // 2] if widths else 1.0\n                gap_thr = max(0.06 * span, 0.5 * w_med)\n\n                diffs: List[Tuple[float, int]] = []\n                for i in range(1, len(xs)):\n                    diffs.append((xs[i] - xs[i - 1], i))\n                candidates = [idx for (gap, idx) in diffs if gap >= gap_thr]\n\n                blocks_sorted = sorted(col_blocks, key=lambda b: b[\"xc\"])\n                columns: List[List[Dict[str, Any]]] = []\n                used_splits: List[int] = []\n                min_lines = max(3, len(col_blocks) // 20 or 1)\n\n                if candidates:\n                    cands_sorted = sorted(candidates, reverse=True)\n                    tried = False\n                    for a_idx in range(min(5, len(cands_sorted))):\n                        for b_idx in range(a_idx + 1, min(6, len(cands_sorted))):\n                            a = cands_sorted[a_idx]\n                            b = cands_sorted[b_idx]\n                            lo, hi = min(a, b), max(a, b)\n                            if lo < min_lines or (hi - lo) < min_lines or (len(col_blocks) - hi) < min_lines:\n                                continue\n                            used_splits = [lo, hi]\n                            tried = True\n                            break\n                        if tried:\n                            break\n                    if not used_splits:\n                        for _, i in sorted(diffs, key=lambda t: t[0], reverse=True):\n                            if i >= min_lines and (len(col_blocks) - i) >= min_lines:\n                                used_splits = [i]\n                                break\n\n                if used_splits:\n                    used_splits = sorted(set(used_splits))\n                    start = 0\n                    for s in used_splits:\n                        columns.append(blocks_sorted[start:s])\n                        start = s\n                    columns.append(blocks_sorted[start:])\n                else:\n                    cur: List[Dict[str, Any]] = []\n                    prev_xc: Optional[float] = None\n                    for b in blocks_sorted:\n                        if prev_xc is None or abs(b[\"xc\"] - prev_xc) <= gap_thr:\n                            cur.append(b)\n                        else:\n                            if cur:\n                                columns.append(cur)\n                            cur = [b]\n                        prev_xc = b[\"xc\"]\n                    if cur:\n                        columns.append(cur)\n\n                def col_key(col: List[Dict[str, Any]]) -> float:\n                    left_edges = [b[\"x0\"] for b in col if isinstance(b.get(\"x0\"), (int, float))]\n                    if left_edges:\n                        return min(left_edges)\n                    centers = sorted(b[\"xc\"] for b in col)\n                    return centers[len(centers) // 2]\n\n                columns = [col for col in columns if col]\n                columns.sort(key=col_key)\n                ordered_columns: List[List[Dict[str, Any]]] = []\n                for col in columns:\n                    col_sorted = sorted(col, key=lambda b: (b[\"y0\"], b[\"x0\"]))\n                    if col_sorted:\n                        ordered_columns.append(col_sorted)\n                return ordered_columns\n\n            full_blocks: List[Dict[str, Any]] = []\n            normal_blocks: List[Dict[str, Any]] = []\n            for block in blocks:\n                if _is_full_width(block):\n                    block[\"full_width\"] = True\n                    full_blocks.append(block)\n                else:\n                    normal_blocks.append(block)\n\n            if not full_blocks:\n                return _order_columns(blocks)\n\n            full_blocks = sorted(full_blocks, key=lambda b: b.get(\"y0\", 0.0))\n            normal_sorted = sorted(normal_blocks, key=_center_y)\n            sections: List[Tuple[str, List[Dict[str, Any]]]] = []\n\n            normal_idx = 0\n            start_y = float(\"-inf\")\n\n            def _collect_until(y_max: float) -> List[Dict[str, Any]]:\n                nonlocal normal_idx, start_y\n                seg: List[Dict[str, Any]] = []\n                while normal_idx < len(normal_sorted):\n                    b = normal_sorted[normal_idx]\n                    yc = _center_y(b)\n                    if yc < start_y:\n                        normal_idx += 1\n                        continue\n                    if yc >= y_max:\n                        break\n                    seg.append(b)\n                    normal_idx += 1\n                return seg\n\n            for fb in full_blocks:\n                seg = _collect_until(float(fb.get(\"y0\", 0.0)))\n                if seg:\n                    sections.append((\"columns\", seg))\n                sections.append((\"full\", [fb]))\n                start_y = float(fb.get(\"y1\", fb.get(\"y0\", 0.0)))\n\n            tail: List[Dict[str, Any]] = []\n            while normal_idx < len(normal_sorted):\n                b = normal_sorted[normal_idx]\n                if _center_y(b) >= start_y:\n                    tail.append(b)\n                normal_idx += 1\n            if tail:\n                sections.append((\"columns\", tail))\n\n            ordered_columns: List[List[Dict[str, Any]]] = []\n            for kind, seg in sections:\n                if kind == \"full\":\n                    ordered_columns.append(seg)\n                else:\n                    ordered_columns.extend(_order_columns(seg))\n            return ordered_columns\n\n        def _columns_to_text(columns: List[List[Dict[str, Any]]]) -> str:\n            if not columns:\n                return \"\"\n            out_cols: List[str] = []\n            for col in columns:\n                lines = [str(b.get(\"text\", \"\")).strip() for b in col if str(b.get(\"text\", \"\")).strip()]\n                if lines:\n                    out_cols.append(\"\\n\".join(lines))\n            return \"\\n\\n\".join([c for c in out_cols if c])\n\n        def _rect_from_box(b: Any) -> Optional[Tuple[float, float, float, float]]:\n            try:\n                for names in ((\"x0\", \"y0\", \"x1\", \"y1\"), (\"xmin\", \"ymin\", \"xmax\", \"ymax\"), (\"left\", \"top\", \"right\", \"bottom\")):\n                    if all(hasattr(b, n) for n in names):\n                        x0 = float(getattr(b, names[0]))\n                        y0 = float(getattr(b, names[1]))\n                        x1 = float(getattr(b, names[2]))\n                        y1 = float(getattr(b, names[3]))\n                        return (x0, y0, x1, y1)\n                bb_attr = getattr(b, \"bbox\", None)\n                if bb_attr is not None:\n                    return _rect_from_box(bb_attr)\n            except Exception:\n                pass\n\n            if not isinstance(b, (dict, list, tuple)):\n                try:\n                    maybe = _paddle_obj_to_dict(b)\n                except Exception:\n                    maybe = None\n                if isinstance(maybe, dict):\n                    b = maybe\n\n            if isinstance(b, dict):\n                coord = b.get(\"coordinate\")\n                if coord is not None:\n                    try:\n                        import numpy as _np4  # type: ignore\n\n                        if isinstance(coord, _np4.ndarray):\n                            if coord.ndim == 2 and coord.shape[1] == 2:\n                                coord = coord.reshape(-1, 2).tolist()\n                            else:\n                                coord = coord.flatten().tolist()\n                    except Exception:\n                        pass\n                    if isinstance(coord, (list, tuple)):\n                        if coord and isinstance(coord[0], (list, tuple)) and len(coord[0]) >= 2:\n                            try:\n                                xs = [float(p[0]) for p in coord]\n                                ys = [float(p[1]) for p in coord]\n                                return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                            except Exception:\n                                pass\n                        if coord and isinstance(coord[0], dict):\n                            try:\n                                xs: List[float] = []\n                                ys: List[float] = []\n                                for entry in coord:\n                                    x = entry.get(\"x\") if \"x\" in entry else entry.get(\"X\")\n                                    y = entry.get(\"y\") if \"y\" in entry else entry.get(\"Y\")\n                                    if x is None or y is None:\n                                        continue\n                                    xs.append(float(x))\n                                    ys.append(float(y))\n                                if xs and ys:\n                                    return (min(xs), min(ys), max(xs), max(ys))\n                            except Exception:\n                                pass\n                        if len(coord) >= 8 and all(isinstance(v, numbers.Real) for v in coord):\n                            xs = [float(v) for v in coord[0::2]]\n                            ys = [float(v) for v in coord[1::2]]\n                            return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                        if len(coord) == 4 and all(isinstance(v, numbers.Real) for v in coord):\n                            x0, y0, a, b_val = map(float, coord)\n                            if a <= x0 or b_val <= y0:\n                                x1 = x0 + a\n                                y1 = y0 + b_val\n                            else:\n                                x1 = a\n                                y1 = b_val\n                            if x1 > x0 and y1 > y0:\n                                return (x0, y0, x1, y1)\n                if isinstance(coord, dict):\n                    try:\n                        for names in ((\"x0\", \"y0\", \"x1\", \"y1\"), (\"xmin\", \"ymin\", \"xmax\", \"ymax\"), (\"left\", \"top\", \"right\", \"bottom\")):\n                            if all(k in coord for k in names):\n                                x0 = float(coord[names[0]])\n                                y0 = float(coord[names[1]])\n                                x1 = float(coord[names[2]])\n                                y1 = float(coord[names[3]])\n                                return (x0, y0, x1, y1)\n                    except Exception:\n                        pass\n                    for key in (\"points\", \"poly\", \"polygon\", \"coords\", \"coordinates\"):\n                        pts = coord.get(key)\n                        if pts is None:\n                            continue\n                        try:\n                            import numpy as _np5  # type: ignore\n\n                            if isinstance(pts, _np5.ndarray):\n                                if pts.ndim == 2 and pts.shape[1] == 2:\n                                    pts = pts.reshape(-1, 2).tolist()\n                                else:\n                                    pts = pts.flatten().tolist()\n                        except Exception:\n                            pass\n                        if isinstance(pts, (list, tuple)):\n                            if pts and isinstance(pts[0], (list, tuple)) and len(pts[0]) >= 2:\n                                xs = [float(p[0]) for p in pts]\n                                ys = [float(p[1]) for p in pts]\n                                return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                            if pts and isinstance(pts[0], dict):\n                                xs = []\n                                ys = []\n                                for entry in pts:\n                                    x = entry.get(\"x\") if \"x\" in entry else entry.get(\"X\")\n                                    y = entry.get(\"y\") if \"y\" in entry else entry.get(\"Y\")\n                                    if x is None or y is None:\n                                        continue\n                                    xs.append(float(x))\n                                    ys.append(float(y))\n                                if xs and ys:\n                                    return (min(xs), min(ys), max(xs), max(ys))\n                            if len(pts) >= 8 and all(isinstance(v, numbers.Real) for v in pts):\n                                xs = [float(v) for v in pts[0::2]]\n                                ys = [float(v) for v in pts[1::2]]\n                                return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                            if len(pts) == 4 and all(isinstance(v, numbers.Real) for v in pts):\n                                x0, y0, a, b_val = map(float, pts[:4])\n                                if a <= x0 or b_val <= y0:\n                                    x1 = x0 + a\n                                    y1 = y0 + b_val\n                                else:\n                                    x1 = a\n                                    y1 = b_val\n                                if x1 > x0 and y1 > y0:\n                                    return (x0, y0, x1, y1)\n                bb = b.get(\"bbox\") or b.get(\"box\") or b.get(\"points\") or b.get(\"poly\")\n                if isinstance(bb, dict):\n                    try:\n                        x0 = float(bb.get(\"x0\", bb.get(\"left\", 0.0)))\n                        y0 = float(bb.get(\"y0\", bb.get(\"top\", 0.0)))\n                        x1 = float(bb.get(\"x1\", bb.get(\"right\", 0.0)))\n                        y1 = float(bb.get(\"y1\", bb.get(\"bottom\", 0.0)))\n                        return (x0, y0, x1, y1)\n                    except Exception:\n                        return None\n                try:\n                    x0 = b.get(\"x0\") or b.get(\"xmin\") or b.get(\"left\")\n                    y0 = b.get(\"y0\") or b.get(\"ymin\") or b.get(\"top\")\n                    x1 = b.get(\"x1\") or b.get(\"xmax\") or b.get(\"right\")\n                    y1 = b.get(\"y1\") or b.get(\"ymax\") or b.get(\"bottom\")\n                    if all(v is not None for v in (x0, y0, x1, y1)):\n                        return (float(x0), float(y0), float(x1), float(y1))\n                except Exception:\n                    pass\n                if isinstance(bb, (list, tuple)):\n                    if len(bb) == 4 and all(isinstance(v, numbers.Real) for v in bb):\n                        return (float(bb[0]), float(bb[1]), float(bb[2]), float(bb[3]))\n                    if len(bb) >= 8 and all(isinstance(v, numbers.Real) for v in bb):\n                        xs = [float(v) for v in bb[0::2]]\n                        ys = [float(v) for v in bb[1::2]]\n                        return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n\n            if isinstance(b, (list, tuple)):\n                if b and isinstance(b[0], (list, tuple)) and len(b[0]) >= 2:\n                    try:\n                        xs = [float(p[0]) for p in b]\n                        ys = [float(p[1]) for p in b]\n                        return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                    except Exception:\n                        pass\n                if len(b) >= 8 and all(isinstance(v, numbers.Real) for v in b):\n                    xs = [float(v) for v in b[0::2]]\n                    ys = [float(v) for v in b[1::2]]\n                    return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                if len(b) >= 4 and all(isinstance(v, numbers.Real) for v in b[:4]):\n                    return (float(b[0]), float(b[1]), float(b[2]), float(b[3]))\n\n            try:\n                import numpy as _np6  # type: ignore\n\n                if isinstance(b, _np6.ndarray):\n                    arr = b.flatten().tolist()\n                    if len(arr) >= 8:\n                        xs = [float(v) for v in arr[0::2]]\n                        ys = [float(v) for v in arr[1::2]]\n                        return (min(xs), min(ys), max(xs), max(ys)) if xs and ys else None\n                    if len(arr) >= 4:\n                        return (float(arr[0]), float(arr[1]), float(arr[2]), float(arr[3]))\n            except Exception:\n                pass\n\n            return None\n\n        rects: List[Dict[str, Any]] = []\n        kept_labels: List[str] = []\n        skipped_labels: List[str] = []\n        if layout_keep_labels:\n            allowed_text_labels = {lbl.strip().lower() for lbl in str(layout_keep_labels).split(\",\") if lbl.strip()}\n        else:\n            allowed_text_labels = {\n                \"text\", \"paragraph_title\", \"title\", \"heading\", \"caption\",\n                \"header\", \"number\", \"figure_title\", \"body\", \"section\",\n                \"text_block\", \"textblock\", \"paragraph\", \"textbox\", \"textline\",\n            }\n        for b in boxes:\n            label = None\n            if isinstance(b, dict):\n                try:\n                    label = str(b.get(\"label\") or \"\").strip().lower() or None\n                except Exception:\n                    label = None\n            else:\n                try:\n                    label = str(getattr(b, \"label\") or \"\").strip().lower() or None\n                except Exception:\n                    label = None\n            take = True\n            if label:\n                if label not in allowed_text_labels:\n                    skipped_labels.append(label)\n                    take = False\n                else:\n                    kept_labels.append(label)\n            if not take:\n                continue\n            r = _rect_from_box(b)\n            if r is not None:\n                x0, y0, x1, y1 = r\n                if x1 > x0 and y1 > y0:\n                    rects.append({\"x0\": x0, \"y0\": y0, \"x1\": x1, \"y1\": y1, \"label\": label or \"text\"})\n\n        w, h = image_obj.size if hasattr(image_obj, \"size\") else (0, 0)\n        blocks: List[Dict[str, Any]] = []\n\n        def _save_crop(crop_img: Any, ix0: int, iy0: int, ix1: int, iy1: int) -> None:\n            nonlocal crop_seq\n            if not save_crops_dir:\n                return\n            try:\n                os.makedirs(save_crops_dir, exist_ok=True)\n                crop_seq += 1\n                prefix = f\"p{page_num}\" if page_num is not None else \"p0\"\n                filename = f\"{prefix}_crop_{crop_seq:05d}_{ix0}_{iy0}_{ix1}_{iy1}.png\"\n                crop_img.save(os.path.join(save_crops_dir, filename))\n            except Exception as exc:\n                if dump:\n                    _dump_log(\"Failed to save crop: %s\", exc)\n\n        def _keep_if_text(res: Any) -> Any:\n            if not res:\n                return None\n            try:\n                for _, text_val in _iter_ocr_entries(res):\n                    if text_val:\n                        return res\n            except Exception:\n                return None\n            return None\n\n        def _iter_crop_variants(crop_img: Any):\n            base = crop_img.convert(\"RGB\")\n            yield \"orig\", base\n            try:\n                gray = ImageOps.grayscale(base)\n                yield \"gray\", gray.convert(\"RGB\")\n                yield \"autocontrast\", ImageOps.autocontrast(gray).convert(\"RGB\")\n                bw = ImageOps.autocontrast(gray).point(lambda x: 255 if x > 160 else 0, mode=\"L\").convert(\"RGB\")\n                yield \"bw\", bw\n            except Exception:\n                pass\n            try:\n                yield \"sharp\", base.filter(ImageFilter.SHARPEN)\n                yield \"unsharp\", base.filter(ImageFilter.UnsharpMask(radius=1.5, percent=150, threshold=3))\n            except Exception:\n                pass\n            max_upscale_side = 3500\n            for scale in (1.5, 2.0):\n                try:\n                    new_w = max(1, int(base.width * scale))\n                    new_h = max(1, int(base.height * scale))\n                    if max(new_w, new_h) > max_upscale_side:\n                        continue\n                    up = base.resize((new_w, new_h), resample=_PILImage.LANCZOS)\n                    yield f\"up{scale}\".replace(\".\", \"p\"), up\n                    try:\n                        up_gray = ImageOps.grayscale(up)\n                        yield f\"up{scale}\".replace(\".\", \"p\") + \"_gray\", up_gray.convert(\"RGB\")\n                    except Exception:\n                        pass\n                except Exception:\n                    continue\n\n        def _run_crop_ocr(crop_img: Any) -> Any:\n            for variant_name, variant_img in _iter_crop_variants(crop_img):\n                crop_arr = np.array(variant_img)\n                result = _ocr_predict(\n                    crop_arr,\n                    det=False,\n                    rec=True,\n                    cls=bool(getattr(config, \"paddle_use_textline_orientation\", True)),\n                )\n                result = _keep_if_text(result)\n                if not result:\n                    result = _ocr_predict(crop_arr, det=False, rec=True)\n                    result = _keep_if_text(result)\n                if not result:\n                    result = _ocr_predict(crop_arr)\n                    result = _keep_if_text(result)\n                if not result:\n                    result = _ocr_legacy(crop_arr)\n                    result = _keep_if_text(result)\n                if not result:\n                    result = _ocr_legacy(\n                        crop_arr,\n                        cls=bool(getattr(config, \"paddle_use_textline_orientation\", True)),\n                    )\n                    result = _keep_if_text(result)\n                if result:\n                    _dump_log(\"Crop OCR succeeded with variant: %s\", variant_name)\n                    return result\n            return None\n\n        if rects:\n            if dump:\n                try:\n                    first_rect = rects[0] if rects else None\n                    _dump_log(\"PaddleX rects parsed: %d; first=%s\", len(rects), first_rect)\n                    if kept_labels or skipped_labels:\n                        kept_counts = dict(Counter(kept_labels)) if kept_labels else {}\n                        skipped_counts = dict(Counter(skipped_labels)) if skipped_labels else {}\n                        _dump_log(\"PaddleX labels kept=%s skipped=%s\", kept_counts, skipped_counts)\n                except Exception:\n                    pass\n            for rect in rects:\n                try:\n                    x0 = rect.get(\"x0\"); y0 = rect.get(\"y0\")\n                    x1 = rect.get(\"x1\"); y1 = rect.get(\"y1\")\n                    label = rect.get(\"label\") or \"text\"\n                    if x0 is None or y0 is None or x1 is None or y1 is None:\n                        continue\n                    \n                    # Strict crop of the box content only (clamped to image)\n                    cx0 = max(0, int(x0)); cx1 = min(w, int(x1))\n                    cy0 = max(0, int(y0)); cy1 = min(h, int(y1))\n\n                    if cx1 <= cx0 or cy1 <= cy0:\n                        continue\n\n                    # Shift crop vertically (vbias>0 moves crop downward) while preserving height\n                    box_h = cy1 - cy0\n                    if crop_vbias:\n                        shifted_cy0 = cy0 + crop_vbias\n                        # Clamp start so height fits in image\n                        shifted_cy0 = min(max(0, shifted_cy0), max(0, h - box_h))\n                        cy0 = shifted_cy0\n                        cy1 = min(h, cy0 + box_h)\n\n                    # Asymmetric vertical padding: reduce top / add to bottom when crop_vbias > 0\n                    pad_top = max(0, crop_padding - crop_vbias)\n                    pad_bottom = max(0, crop_padding + crop_vbias)\n\n                    # Virtual padded coordinates (unclamped)\n                    vx0 = int(x0) - crop_padding\n                    vx1 = int(x1) + crop_padding\n                    vy0 = int(y0) - pad_top\n                    vy1 = int(y1) + pad_bottom\n                    \n                    dst_w = vx1 - vx0\n                    dst_h = vy1 - vy0\n                    \n                    # White canvas (passepartout)\n                    canvas = _PILImage.new(\"RGB\", (dst_w, dst_h), (255, 255, 255))\n                    \n                    # Paste strict content at correct offset\n                    dx = cx0 - vx0\n                    dy = cy0 - vy0\n                    src_crop = image_obj.crop((cx0, cy0, cx1, cy1))\n                    canvas.paste(src_crop, (dx, dy))\n                    crop = canvas\n                    \n                    # Use virtual coordinates for saving and OCR mapping\n                    ix0, iy0, ix1, iy1 = vx0, vy0, vx1, vy1\n                    _save_crop(crop, ix0, iy0, ix1, iy1)\n                except Exception:\n                    continue\n                result_crop = _run_crop_ocr(crop)\n                if not result_crop:\n                    continue\n                line_entries: List[Dict[str, Any]] = []\n                seq = 0\n                for quad, text_val in _iter_ocr_entries(result_crop):\n                    if not text_val:\n                        continue\n                    seq += 1\n                    line_x0 = float(ix0)\n                    line_y0 = float(iy0)\n                    if quad is not None:\n                        bb = _bbox_from_quad(quad)\n                        if bb:\n                            bx0, by0, _, _, _ = bb\n                            line_x0 = bx0 + float(ix0)\n                            line_y0 = by0 + float(iy0)\n                    line_entries.append({\"text\": text_val, \"y0\": line_y0, \"x0\": line_x0, \"seq\": seq})\n                if not line_entries:\n                    continue\n                line_entries.sort(key=lambda entry: (entry[\"y0\"], entry[\"x0\"], entry[\"seq\"]))\n                block_text = \"\\n\".join(entry[\"text\"] for entry in line_entries if entry[\"text\"])\n                if not block_text.strip():\n                    continue\n                bx0, by0, bx1, by1 = float(ix0), float(iy0), float(ix1), float(iy1)\n                bxc = 0.5 * (bx0 + bx1)\n                blocks.append({\n                    \"x0\": bx0,\n                    \"y0\": by0,\n                    \"x1\": bx1,\n                    \"y1\": by1,\n                    \"xc\": bxc,\n                    \"label\": label,\n                    \"text\": block_text,\n                })\n        else:\n            if dump and boxes:\n                try:\n                    _dump_log(\n                        \"PaddleX boxes present but no rects parsed; inspecting first %d box(es)\",\n                        min(len(boxes), 2),\n                    )\n                    for idx_box, bb in enumerate(boxes[:2]):\n                        _dump_log(\"  Box[%d] type: %s\", idx_box, type(bb))\n                        for names in ((\"x0\", \"y0\", \"x1\", \"y1\"), (\"xmin\", \"ymin\", \"xmax\", \"ymax\"), (\"left\", \"top\", \"right\", \"bottom\")):\n                            try:\n                                if all(hasattr(bb, n) for n in names):\n                                    vals = tuple(float(getattr(bb, n)) for n in names)\n                                    _dump_log(\"  Box[%d] attrs %s: %s\", idx_box, names, vals)\n                            except Exception:\n                                pass\n                        maybe_bb = None\n                        try:\n                            maybe_bb = _paddle_obj_to_dict(bb)\n                        except Exception:\n                            maybe_bb = None\n                        if isinstance(maybe_bb, dict):\n                            try:\n                                _dump_log(\"  Box[%d] dict keys: %s\", idx_box, sorted(maybe_bb.keys()))\n                            except Exception:\n                                _dump_log(\"  Box[%d] dict keys: %s\", idx_box, list(maybe_bb.keys()))\n                            try:\n                                coord = maybe_bb.get(\"coordinate\")\n                            except Exception:\n                                coord = None\n                            if coord is not None:\n                                try:\n                                    if isinstance(coord, np.ndarray):\n                                        _dump_log(\"    coordinate ndarray shape: %s\", getattr(coord, \"shape\", None))\n                                    elif isinstance(coord, (list, tuple)):\n                                        preview_vals = coord[:8] if len(coord) > 8 else coord\n                                        _dump_log(\n                                            \"    coordinate list len: %d preview: %s\",\n                                            len(coord),\n                                            preview_vals,\n                                        )\n                                        if coord and isinstance(coord[0], (list, tuple)):\n                                            _dump_log(\"    coordinate first pair: %s\", coord[0])\n                                        elif coord and isinstance(coord[0], dict):\n                                            _dump_log(\"    coordinate first dict keys: %s\", list(coord[0].keys()))\n                                    elif isinstance(coord, dict):\n                                        _dump_log(\"    coordinate dict keys: %s\", list(coord.keys()))\n                                except Exception:\n                                    pass\n                        elif isinstance(bb, (list, tuple)):\n                            preview = bb[:8] if len(bb) >= 8 else bb\n                            _dump_log(\"  Box[%d] list/tuple preview: %s\", idx_box, preview)\n                        else:\n                            try:\n                                if isinstance(bb, np.ndarray):\n                                    _dump_log(\"  Box[%d] ndarray shape: %s\", idx_box, bb.shape)\n                            except Exception:\n                                pass\n                except Exception:\n                    pass\n\n        if blocks:\n            columns = _order_blocks_into_columns(blocks)\n            ordered = _columns_to_text(columns)\n            if ordered.strip():\n                return ordered.splitlines(), True, columns, total, len(blocks)\n\n        if layout_has_boxes:\n            _dump_log(\"Layout boxes detected but crop OCR produced no text; enabling plain OCR fallback.\")\n            return [], True, [], total, 0\n        _dump_log(\"PaddleX layout produced no boxes; falling back to plain OCR.\")\n        return [], False, [], total, 0\n\n    def _render_layout_markdown(pages: List[List[List[Dict[str, Any]]]], fallback_text: Optional[str] = None) -> str:\n        def _normalize_block_text(text: str) -> str:\n            text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n            lines = [line.rstrip() for line in text.split(\"\\n\")]\n            while lines and not lines[0].strip():\n                lines.pop(0)\n            while lines and not lines[-1].strip():\n                lines.pop()\n            return \"\\n\".join(lines)\n\n        def _single_line(text: str) -> str:\n            return \" \".join(_normalize_block_text(text).split()).strip()\n\n        out_lines: List[str] = []\n        for page_idx, columns in enumerate(pages, start=1):\n            page_blocks = [b for col in columns for b in col] if columns else []\n            if not page_blocks:\n                continue\n\n            out_lines.append(f\"## Page {page_idx}\")\n            non_full_columns = [\n                col for col in columns\n                if not all(bool(block.get(\"full_width\")) for block in col)\n            ]\n            col_num = 0\n            for col in columns:\n                is_full = all(bool(block.get(\"full_width\")) for block in col)\n                if not is_full:\n                    col_num += 1\n                    if len(non_full_columns) > 1:\n                        out_lines.append(f\"### Column {col_num}\")\n\n                for block in col:\n                    text_val = _normalize_block_text(str(block.get(\"text\", \"\") or \"\"))\n                    if not text_val:\n                        continue\n                    label_val = str(block.get(\"label\", \"\") or \"\").strip().lower()\n\n                    if label_val in {\"paragraph_title\", \"title\", \"heading\", \"section\", \"header\"}:\n                        heading = _single_line(text_val)\n                        if heading:\n                            out_lines.append(f\"### {heading}\")\n                        out_lines.append(\"\")\n                        continue\n\n                    if label_val in {\"figure_title\", \"caption\", \"figure\", \"figure_caption\"}:\n                        caption = _single_line(text_val)\n                        if caption:\n                            out_lines.append(f\"**figure caption:** {caption}\")\n                        out_lines.append(\"\")\n                        continue\n\n                    out_lines.append(\"\")\n                    out_lines.append(text_val)\n\n            out_lines.append(\"\")\n\n        if not out_lines and fallback_text:\n            fb = _normalize_block_text(fallback_text)\n            if fb:\n                return f\"## Page 1\\n\\n{fb}\\n\"\n\n        return (\"\\n\".join(out_lines).rstrip() + \"\\n\") if out_lines else \"\"\n\n    pages: List[Dict[str, Any]] = []\n    layout_pages: List[List[List[Dict[str, Any]]]] = []\n    all_lines: List[str] = []\n    total_boxes = 0\n    total_blocks = 0\n    pages_with_boxes = 0\n    pages_with_blocks = 0\n    fullpage_fallback_pages = 0\n    total = max(1, len(images))\n    if progress_cb and progress_span > 0:\n        progress_cb(progress_base, \"layout\", f\"Paddle layout page 1/{total}\")\n\n    for idx, image in enumerate(images, start=1):\n        page_img = image.convert(\"RGB\")\n        if max_side_px > 0:\n            max_side = max(page_img.width, page_img.height)\n            if max_side > max_side_px:\n                scale = max_side_px / max_side\n                new_size = (max(1, int(page_img.width * scale)), max(1, int(page_img.height * scale)))\n                page_img = page_img.resize(new_size, resample=_PILImage.LANCZOS)\n        src_path = None\n        if use_file_path:\n            try:\n                fd, src_path = tempfile.mkstemp(prefix=\"paddlex_layout_\", suffix=\".png\")\n                os.close(fd)\n                page_img.save(src_path)\n            except Exception:\n                src_path = None\n        try:\n            page_lines, layout_boxes, page_columns, box_count, block_count = _paddlex_structure_extract_texts(\n                page_img,\n                languages,\n                src_path=src_path,\n                page_num=idx,\n            )\n        finally:\n            if src_path:\n                try:\n                    os.unlink(src_path)\n                except Exception:\n                    pass\n        layout_pages.append(page_columns)\n        if layout_boxes:\n            pages_with_boxes += 1\n            total_boxes += int(box_count or 0)\n        if page_columns:\n            pages_with_blocks += 1\n            total_blocks += int(block_count or 0)\n\n        if not page_lines:\n            if layout_boxes:\n                _dump_log(\"Page %d: layout boxes detected but no text lines produced; skipping plain OCR fallback.\", idx)\n                page_lines = []\n            else:\n                _dump_log(\"Page %d: layout produced no boxes; running plain OCR fallback.\", idx)\n                fullpage_fallback_pages += 1\n                if ocr is None:\n                    for kw in ctor_candidates:\n                        ocr = _try_create_direct(kw)\n                        if ocr is not None:\n                            break\n                    if ocr is None:\n                        raise RuntimeError(\"Failed to create PaddleOCR for plain OCR fallback.\")\n                result = None\n                try:\n                    img_np = np.array(page_img)\n                    result = _ocr_predict(img_np)\n                    if result is None:\n                        result = _ocr_legacy(img_np)\n                    if result is None:\n                        result = _ocr_legacy(\n                            img_np,\n                            cls=bool(getattr(config, \"paddle_use_textline_orientation\", True)),\n                        )\n                except Exception as exc:\n                    raise RuntimeError(f\"PaddleOCR failed: {exc}\") from exc\n                page_lines = _extract_texts(result) if result else []\n\n        if page_lines:\n            all_lines.extend(page_lines)\n        page_text = \"\\n\".join(page_lines).strip()\n        pages.append({\"page_num\": idx, \"text\": page_text})\n\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"layout\", f\"Paddle layout page {idx}/{total}\")\n\n    text = \"\\n\".join(all_lines).strip()\n    LOGGER.info(\n        \"PaddleX layout OCR complete: pages=%d, text_chars=%d\",\n        len(pages),\n        ocr_pages_text_chars(pages),\n    )\n    layout_markdown = _render_layout_markdown(layout_pages, fallback_text=text)\n    return pages, {\n        \"layout_used\": True,\n        \"layout_model\": layout_model,\n        \"layout_boxes_total\": total_boxes,\n        \"layout_blocks_total\": total_blocks,\n        \"layout_pages_with_boxes\": pages_with_boxes,\n        \"layout_pages_with_blocks\": pages_with_blocks,\n        \"layout_pages_fullpage_fallback\": fullpage_fallback_pages,\n        \"layout_markdown\": layout_markdown,\n    }\n\ndef ocr_pages_with_paddle_structure(\n    images: Sequence[Any],\n    languages: str,\n    config: Any,\n    helpers: Dict[str, Any],\n    progress_cb: Optional[Any] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    global LOGGER\n    LOGGER = helpers.get(\"logger\", LOGGER)\n    structure_api_disabled = bool(getattr(config, \"paddle_structure_api_disable\", False))\n    structure_api_url = getattr(config, \"paddle_structure_api_url\", None) or os.getenv(\"PADDLE_STRUCTURE_API_URL\")\n    structure_api_token = getattr(config, \"paddle_structure_api_token\", None) or os.getenv(\"PADDLE_STRUCTURE_API_TOKEN\")\n    structure_api_timeout = getattr(config, \"paddle_structure_api_timeout_sec\", 120)\n    if structure_api_url and structure_api_token and not structure_api_disabled:\n        orig_url = getattr(config, \"paddle_vl_api_url\", None)\n        orig_token = getattr(config, \"paddle_vl_api_token\", None)\n        orig_timeout = getattr(config, \"paddle_vl_api_timeout_sec\", None)\n        orig_disable = getattr(config, \"paddle_vl_api_disable\", None)\n        setattr(config, \"paddle_vl_api_url\", structure_api_url)\n        setattr(config, \"paddle_vl_api_token\", structure_api_token)\n        setattr(config, \"paddle_vl_api_timeout_sec\", structure_api_timeout)\n        setattr(config, \"paddle_vl_api_disable\", False)\n        try:\n            pages, stats = ocr_pages_with_paddle_vl(\n                images,\n                languages,\n                config,\n                helpers,\n                progress_cb,\n                progress_base,\n                progress_span,\n            )\n        finally:\n            setattr(config, \"paddle_vl_api_url\", orig_url)\n            setattr(config, \"paddle_vl_api_token\", orig_token)\n            setattr(config, \"paddle_vl_api_timeout_sec\", orig_timeout)\n            setattr(config, \"paddle_vl_api_disable\", orig_disable)\n        if isinstance(stats, dict):\n            stats[\"layout_model\"] = \"PP-StructureV3 API\"\n        return pages, stats\n    if bool(getattr(config, \"paddle_use_paddlex_layout\", True)):\n        try:\n            return _paddlex_layout_ocr_pages(\n                images,\n                languages,\n                config,\n                helpers,\n                progress_cb,\n                progress_base,\n                progress_span,\n            )\n        except Exception as exc:\n            LOGGER.warning(\"PaddleX layout OCR failed; falling back to PaddleOCR: %s\", exc)\n    return ocr_pages_with_paddle(\n        images,\n        languages,\n        config,\n        helpers,\n        progress_cb,\n        progress_base,\n        progress_span,\n    )\n\n\ndef ocr_pages_with_paddle_vl(\n    images: Sequence[Any],\n    languages: str,\n    config: Any,\n    helpers: Dict[str, Any],\n    progress_cb: Optional[Any] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    global LOGGER\n    LOGGER = helpers.get(\"logger\", LOGGER)\n    ocr_pages_text_chars = helpers[\"ocr_pages_text_chars\"]\n\n    api_url = getattr(config, \"paddle_vl_api_url\", None) or os.getenv(\"PADDLE_VL_API_URL\")\n    api_token = getattr(config, \"paddle_vl_api_token\", None) or os.getenv(\"PADDLE_VL_API_TOKEN\")\n    api_timeout = getattr(config, \"paddle_vl_api_timeout_sec\", 120)\n    source_path = helpers.get(\"ocr_source_path\")\n\n    api_disabled = bool(getattr(config, \"paddle_vl_api_disable\", False))\n    if api_url and api_token and not api_disabled:\n        api_max_pages = int(getattr(config, \"paddle_vl_api_max_pages\", 100) or 100)\n        if api_max_pages <= 0:\n            api_max_pages = 100\n        api_max_chunk_bytes = int(getattr(config, \"paddle_vl_api_max_chunk_bytes\", 0) or 0)\n        if api_max_chunk_bytes <= 0:\n            try:\n                env_value = os.getenv(\"PADDLE_VL_API_MAX_CHUNK_BYTES\", \"\")\n                if env_value:\n                    api_max_chunk_bytes = int(env_value)\n            except Exception:\n                api_max_chunk_bytes = 0\n        api_images = list(images) if images else []\n        source_page_count = None\n        source_reader = None\n        PdfWriter = None\n        if isinstance(source_path, str) and source_path.lower().endswith(\".pdf\") and os.path.isfile(source_path):\n            try:\n                from pypdf import PdfReader, PdfWriter  # type: ignore\n                source_reader = PdfReader(source_path)\n                source_page_count = len(source_reader.pages)\n            except Exception:\n                source_page_count = None\n                source_reader = None\n                PdfWriter = None\n        original_count = source_page_count if source_page_count else (len(api_images) if api_images else None)\n        if api_max_chunk_bytes > 0:\n            LOGGER.info(\n                \"PaddleOCR-VL API payload cap: %d bytes (max pages per chunk: %d).\",\n                api_max_chunk_bytes,\n                api_max_pages,\n            )\n        elif original_count and original_count > api_max_pages:\n            LOGGER.info(\n                \"PaddleOCR-VL API batch size %d; splitting %d pages into chunks.\",\n                api_max_pages,\n                original_count,\n            )\n        try:\n            import base64\n            import io\n            import requests\n        except Exception as exc:\n            raise RuntimeError(f\"PaddleOCR-VL API dependencies missing: {exc}\") from exc\n\n        headers = {\n            \"Authorization\": f\"token {api_token}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        def _normalize_ignore_labels(value: Any) -> Optional[List[str]]:\n            if not value:\n                return None\n            if isinstance(value, str):\n                labels = [item.strip() for item in value.split(\",\") if item.strip()]\n            elif isinstance(value, (list, tuple, set)):\n                labels = [str(item).strip() for item in value if str(item).strip()]\n            else:\n                labels = [str(value).strip()] if str(value).strip() else []\n            return labels or None\n\n        optional_payload: Dict[str, Any] = {}\n        ignore_labels = _normalize_ignore_labels(getattr(config, \"paddle_vl_markdown_ignore_labels\", None))\n        if ignore_labels:\n            optional_payload[\"markdownIgnoreLabels\"] = ignore_labels\n        if getattr(config, \"paddle_use_doc_orientation_classify\", None) is not None:\n            optional_payload[\"useDocOrientationClassify\"] = bool(config.paddle_use_doc_orientation_classify)\n        if getattr(config, \"paddle_use_doc_unwarping\", None) is not None:\n            optional_payload[\"useDocUnwarping\"] = bool(config.paddle_use_doc_unwarping)\n        use_layout_detection = getattr(config, \"paddle_vl_use_layout_detection\", None)\n        if use_layout_detection is not None:\n            optional_payload[\"useLayoutDetection\"] = bool(use_layout_detection)\n        if getattr(config, \"paddle_vl_use_chart_recognition\", None) is not None:\n            optional_payload[\"useChartRecognition\"] = bool(config.paddle_vl_use_chart_recognition)\n        if getattr(config, \"paddle_vl_prompt_label\", None):\n            optional_payload[\"promptLabel\"] = str(config.paddle_vl_prompt_label)\n        layout_nms = getattr(config, \"paddle_vl_layout_nms\", None)\n        if layout_nms is None:\n            layout_nms = getattr(config, \"paddle_layout_nms\", None)\n        if layout_nms is not None:\n            optional_payload[\"layoutNms\"] = bool(layout_nms)\n        if getattr(config, \"paddle_vl_repetition_penalty\", None) is not None:\n            optional_payload[\"repetitionPenalty\"] = getattr(config, \"paddle_vl_repetition_penalty\")\n        if getattr(config, \"paddle_vl_temperature\", None) is not None:\n            optional_payload[\"temperature\"] = getattr(config, \"paddle_vl_temperature\")\n        if getattr(config, \"paddle_vl_top_p\", None) is not None:\n            optional_payload[\"topP\"] = getattr(config, \"paddle_vl_top_p\")\n        if getattr(config, \"paddle_vl_min_pixels\", None) is not None:\n            optional_payload[\"minPixels\"] = int(getattr(config, \"paddle_vl_min_pixels\"))\n        if getattr(config, \"paddle_vl_max_pixels\", None) is not None:\n            optional_payload[\"maxPixels\"] = int(getattr(config, \"paddle_vl_max_pixels\"))\n\n        def _strip_markup(text: str) -> str:\n            text = re.sub(r\"<[^>]+>\", \" \", text)\n            text = re.sub(r\"!\\[[^\\]]*]\\([^)]+\\)\", \" \", text)\n            text = re.sub(r\"\\s+\", \" \", text)\n            return text.strip()\n\n        def _estimate_payload_bytes(file_bytes: bytes) -> int:\n            if not file_bytes:\n                return 0\n            b64_len = 4 * ((len(file_bytes) + 2) // 3)\n            return b64_len + 200\n\n        def _build_pdf_bytes(page_list: Sequence[Any]) -> bytes:\n            if PdfWriter is None:\n                raise RuntimeError(\"PDF chunking requires pypdf.\")\n            writer = PdfWriter()\n            for page in page_list:\n                writer.add_page(page)\n            buffer = io.BytesIO()\n            writer.write(buffer)\n            buffer.seek(0)\n            return buffer.read()\n\n        def _build_payload(file_bytes: bytes, file_type: int) -> Dict[str, Any]:\n            payload: Dict[str, Any] = {\n                \"file\": base64.b64encode(file_bytes).decode(\"ascii\"),\n                \"fileType\": file_type,\n            }\n            payload.update(optional_payload)\n            return payload\n\n        def _shorten_text(value: str, limit: int = 240) -> str:\n            if len(value) <= limit:\n                return value\n            return f\"{value[:limit]}...<truncated {len(value) - limit} chars>\"\n\n        def _keys_preview(value: Dict[str, Any], limit: int = 12) -> List[str]:\n            keys = [str(k) for k in value.keys()]\n            keys.sort()\n            return keys[:limit]\n\n        def _collect_block_labels_summary(value: Any, counts: Dict[str, int], limit: int = 24) -> None:\n            if len(counts) >= limit:\n                return\n            if isinstance(value, dict):\n                for key, item in value.items():\n                    key_str = str(key)\n                    if key_str in {\n                        \"block_label\",\n                        \"blockLabel\",\n                        \"block_label_name\",\n                        \"blockLabelName\",\n                        \"block_label_type\",\n                        \"blockLabelType\",\n                    }:\n                        if isinstance(item, str):\n                            label = item.strip()\n                            if label:\n                                counts[label] = counts.get(label, 0) + 1\n                        continue\n                    _collect_block_labels_summary(item, counts, limit)\n            elif isinstance(value, list):\n                for item in value:\n                    _collect_block_labels_summary(item, counts, limit)\n\n        def _summarize_layout_entry(entry: Any) -> Any:\n            if not isinstance(entry, dict):\n                return {\"type\": type(entry).__name__}\n            summary: Dict[str, Any] = {\"keys\": _keys_preview(entry)}\n            label_counts: Dict[str, int] = {}\n            _collect_block_labels_summary(entry, label_counts)\n            if label_counts:\n                summary[\"block_label_count\"] = sum(label_counts.values())\n                top = sorted(label_counts.items(), key=lambda item: (-item[1], item[0]))\n                summary[\"block_label_values\"] = [label for label, _ in top[:12]]\n            markdown = entry.get(\"markdown\")\n            if isinstance(markdown, dict):\n                md_text = markdown.get(\"text\") or markdown.get(\"markdown\") or markdown.get(\"content\")\n                if isinstance(md_text, str):\n                    summary[\"markdown_len\"] = len(md_text)\n                    summary[\"markdown_preview\"] = _shorten_text(md_text)\n                md_images = markdown.get(\"images\") or markdown.get(\"markdown_images\") or markdown.get(\"markdownImages\")\n                if isinstance(md_images, dict):\n                    summary[\"markdown_images_count\"] = len(md_images)\n                    summary[\"markdown_images_keys\"] = _keys_preview(md_images, limit=6)\n            elif isinstance(markdown, str):\n                summary[\"markdown_len\"] = len(markdown)\n                summary[\"markdown_preview\"] = _shorten_text(markdown)\n            output_images = entry.get(\"outputImages\")\n            if isinstance(output_images, dict):\n                summary[\"output_images_count\"] = len(output_images)\n                summary[\"output_images_keys\"] = _keys_preview(output_images, limit=6)\n            pruned = entry.get(\"prunedResult\")\n            if isinstance(pruned, list):\n                summary[\"pruned_result_count\"] = len(pruned)\n                preview: List[Dict[str, Any]] = []\n                for block in pruned[:5]:\n                    if not isinstance(block, dict):\n                        preview.append({\"type\": type(block).__name__})\n                        continue\n                    preview.append(_summarize_pruned_block(block))\n                summary[\"pruned_result_preview\"] = preview\n            elif isinstance(pruned, dict):\n                summary[\"pruned_result_count\"] = 1\n                preview: Dict[str, Any] = {\"keys\": _keys_preview(pruned, limit=12)}\n                parsing_list = pruned.get(\"parsing_res_list\") if isinstance(pruned, dict) else None\n                if isinstance(parsing_list, list):\n                    preview[\"parsing_res_count\"] = len(parsing_list)\n                    parsing_preview: List[Dict[str, Any]] = []\n                    for block in parsing_list[:5]:\n                        if not isinstance(block, dict):\n                            parsing_preview.append({\"type\": type(block).__name__})\n                            continue\n                        parsing_preview.append(_summarize_pruned_block(block))\n                    preview[\"parsing_res_preview\"] = parsing_preview\n                summary[\"pruned_result_preview\"] = preview\n            return summary\n\n        def _summarize_pruned_block(block: Dict[str, Any]) -> Dict[str, Any]:\n            def _find_string_by_keys(value: Any, keys: Set[str], depth: int = 0, limit: int = 3) -> Optional[str]:\n                if depth > limit:\n                    return None\n                if isinstance(value, dict):\n                    for key, item in value.items():\n                        if key in keys and isinstance(item, str) and item.strip():\n                            return item.strip()\n                    for item in value.values():\n                        found = _find_string_by_keys(item, keys, depth + 1, limit)\n                        if found:\n                            return found\n                elif isinstance(value, list):\n                    for item in value[:5]:\n                        found = _find_string_by_keys(item, keys, depth + 1, limit)\n                        if found:\n                            return found\n                return None\n\n            def _find_bbox(value: Any, depth: int = 0, limit: int = 3) -> Optional[List[float]]:\n                if depth > limit:\n                    return None\n                if isinstance(value, (list, tuple)) and len(value) >= 4:\n                    try:\n                        return [round(float(x), 2) for x in value[:4]]\n                    except Exception:\n                        return None\n                if isinstance(value, dict):\n                    if all(k in value for k in (\"x0\", \"y0\", \"x1\", \"y1\")):\n                        try:\n                            return [\n                                round(float(value[\"x0\"]), 2),\n                                round(float(value[\"y0\"]), 2),\n                                round(float(value[\"x1\"]), 2),\n                                round(float(value[\"y1\"]), 2),\n                            ]\n                        except Exception:\n                            return None\n                    if all(k in value for k in (\"left\", \"top\", \"width\", \"height\")):\n                        try:\n                            left = float(value[\"left\"])\n                            top = float(value[\"top\"])\n                            return [\n                                round(left, 2),\n                                round(top, 2),\n                                round(left + float(value[\"width\"]), 2),\n                                round(top + float(value[\"height\"]), 2),\n                            ]\n                        except Exception:\n                            return None\n                    for item in value.values():\n                        found = _find_bbox(item, depth + 1, limit)\n                        if found:\n                            return found\n                if isinstance(value, list):\n                    for item in value[:5]:\n                        found = _find_bbox(item, depth + 1, limit)\n                        if found:\n                            return found\n                return None\n\n            preview: Dict[str, Any] = {}\n            for key in (\"block_label\", \"blockLabel\", \"label\", \"type\", \"block_type\", \"blockType\"):\n                val = block.get(key)\n                if isinstance(val, str) and val.strip():\n                    preview[\"block_label\"] = val.strip()\n                    break\n            for key in (\"id\", \"block_id\", \"blockId\", \"uuid\", \"uid\"):\n                val = block.get(key)\n                if isinstance(val, (int, str)) and str(val).strip():\n                    preview[key] = str(val).strip()\n                    break\n            for key in (\"parent_id\", \"parentId\", \"group_id\", \"groupId\", \"layout_id\", \"layoutId\"):\n                val = block.get(key)\n                if isinstance(val, (int, str)) and str(val).strip():\n                    preview[key] = str(val).strip()\n            for key in (\"image_id\", \"imageId\", \"img_id\", \"imgId\", \"image_index\", \"img_idx\"):\n                val = block.get(key)\n                if isinstance(val, (int, str)) and str(val).strip():\n                    preview[key] = str(val).strip()\n                    break\n            image_keys = {\n                \"image\",\n                \"img\",\n                \"image_path\",\n                \"imagePath\",\n                \"img_path\",\n                \"src\",\n                \"url\",\n                \"path\",\n                \"file\",\n                \"file_path\",\n                \"filePath\",\n            }\n            image_ref = _find_string_by_keys(block, image_keys)\n            if image_ref:\n                preview[\"image_ref\"] = image_ref\n            text_keys = {\n                \"text\",\n                \"content\",\n                \"ocr_text\",\n                \"ocrText\",\n                \"caption\",\n                \"figure_caption\",\n                \"footnote\",\n                \"note\",\n                \"value\",\n            }\n            text_val = _find_string_by_keys(block, text_keys)\n            if text_val:\n                preview[\"text_preview\"] = _shorten_text(text_val, limit=160)\n            bbox_val = _find_bbox(block)\n            if bbox_val:\n                preview[\"bbox\"] = bbox_val\n            preview[\"keys\"] = _keys_preview(block, limit=12)\n            return preview\n\n        def _summarize_result(value: Any) -> Any:\n            if isinstance(value, dict):\n                summary: Dict[str, Any] = {\"keys\": _keys_preview(value)}\n                layout_key = None\n                layout_val = None\n                for key in (\n                    \"layoutParsingResults\",\n                    \"layout_parsing_results\",\n                    \"layoutParsingResult\",\n                    \"layout_parsing_result\",\n                ):\n                    if key in value:\n                        layout_key = key\n                        layout_val = value.get(key)\n                        break\n                if layout_key is not None:\n                    summary[\"layout_key\"] = layout_key\n                    if isinstance(layout_val, list):\n                        summary[\"layout_count\"] = len(layout_val)\n                        if layout_val:\n                            summary[\"layout_preview\"] = _summarize_layout_entry(layout_val[0])\n                    elif layout_val is not None:\n                        summary[\"layout_count\"] = 1\n                        summary[\"layout_preview\"] = _summarize_layout_entry(layout_val)\n                return summary\n            if isinstance(value, list):\n                preview: Dict[str, Any] = {\"list_len\": len(value)}\n                if value:\n                    preview[\"first_item_type\"] = type(value[0]).__name__\n                    if isinstance(value[0], dict):\n                        preview[\"first_item_keys\"] = _keys_preview(value[0])\n                return preview\n            if isinstance(value, str):\n                return {\"text_preview\": _shorten_text(value)}\n            return {\"type\": type(value).__name__}\n\n        def _summarize_api_response(value: Any) -> Dict[str, Any]:\n            summary: Dict[str, Any] = {}\n            if isinstance(value, dict):\n                for key in (\"code\", \"status\", \"message\", \"msg\", \"error\", \"error_msg\", \"errorMsg\"):\n                    if key in value:\n                        summary[key] = _shorten_text(str(value.get(key)))\n                if \"result\" in value:\n                    summary[\"result\"] = _summarize_result(value.get(\"result\"))\n                else:\n                    summary[\"result\"] = _summarize_result(value)\n                summary[\"keys\"] = _keys_preview(value)\n                return summary\n            summary[\"result\"] = _summarize_result(value)\n            return summary\n\n        def _is_timeout_error(exc: Exception) -> bool:\n            try:\n                if isinstance(exc, requests.exceptions.Timeout):\n                    return True\n                if isinstance(exc, requests.exceptions.ConnectionError):\n                    message = str(exc).lower()\n                    if \"timed out\" in message or \"timeout\" in message:\n                        return True\n            except Exception:\n                pass\n            if isinstance(exc, TimeoutError):\n                return True\n            message = str(exc).lower()\n            return \"timed out\" in message or \"timeout\" in message\n\n        def _is_http_500_error(exc: Exception) -> bool:\n            message = str(exc).lower()\n            if \"status=500\" in message:\n                return True\n            if \"errorcode\\\":500\" in message or \"errorcode':500\" in message:\n                return True\n            if \"internal server error\" in message:\n                return True\n            return False\n\n        def _request_api(file_bytes: bytes, file_type: int, label: str) -> Dict[str, Any]:\n            payload = _build_payload(file_bytes, file_type)\n            max_attempts = 3\n            delay_sec = 2\n            response = None\n            for attempt in range(1, max_attempts + 1):\n                try:\n                    response = requests.post(api_url, json=payload, headers=headers, timeout=api_timeout)\n                    break\n                except Exception as exc:\n                    if _is_timeout_error(exc) and attempt < max_attempts:\n                        LOGGER.warning(\n                            \"PaddleOCR-VL API timeout (%s). Retrying %d/%d in %ds.\",\n                            label,\n                            attempt,\n                            max_attempts,\n                            delay_sec,\n                        )\n                        time.sleep(delay_sec)\n                        delay_sec *= 2\n                        continue\n                    raise RuntimeError(f\"PaddleOCR-VL API request failed ({label}): {exc}\") from exc\n            if response is None:\n                raise RuntimeError(f\"PaddleOCR-VL API request failed ({label}): no response\")\n            if response.status_code != 200:\n                if response.status_code == 429:\n                    retry_after = response.headers.get(\"Retry-After\")\n                    message = (\n                        \"PaddleOCR-VL API rate limited (429): daily 3000-page limit reached. \"\n                        \"Wait for the quota reset or request whitelist access.\"\n                    )\n                    if retry_after:\n                        message = f\"{message} Retry-After: {retry_after}\"\n                    raise RuntimeError(message)\n                body = \"\"\n                try:\n                    body = response.text.strip()\n                except Exception:\n                    body = \"\"\n                raise RuntimeError(\n                    f\"PaddleOCR-VL API request failed ({label}): status={response.status_code} {body}\"\n                )\n            try:\n                data = response.json()\n            except Exception as exc:\n                raise RuntimeError(f\"PaddleOCR-VL API response parse failed ({label}): {exc}\") from exc\n            summary = _summarize_api_response(data)\n            try:\n                LOGGER.info(\"PaddleOCR-VL API response (%s): %s\", label, json.dumps(summary, ensure_ascii=True))\n            except Exception:\n                LOGGER.info(\"PaddleOCR-VL API response (%s): %r\", label, summary)\n            return data\n\n        def _extract_layout_results(data: Any) -> List[Dict[str, Any]]:\n            if isinstance(data, dict):\n                def _is_success_message(value: Any) -> bool:\n                    if value is None:\n                        return False\n                    text = str(value).strip().lower()\n                    return text in {\"success\", \"ok\", \"ok.\"}\n\n                error_code = data.get(\"errorCode\")\n                if error_code is None:\n                    error_code = data.get(\"error_code\")\n                error_msg = data.get(\"error_msg\") or data.get(\"errorMsg\")\n                error_field = data.get(\"error\")\n                if error_msg is None and isinstance(error_field, dict):\n                    error_msg = error_field.get(\"message\") or error_field.get(\"msg\")\n                if error_code is not None:\n                    try:\n                        code_int = int(error_code)\n                    except Exception:\n                        code_int = None\n                    if code_int is not None and code_int != 0:\n                        err = error_msg or error_field or error_code\n                        raise RuntimeError(f\"PaddleOCR-VL API error: {err}\")\n                    if code_int is None and error_msg and not _is_success_message(error_msg):\n                        raise RuntimeError(f\"PaddleOCR-VL API error: {error_msg}\")\n                else:\n                    if isinstance(error_field, bool):\n                        if error_field and not _is_success_message(error_msg):\n                            err = error_msg or error_field\n                            raise RuntimeError(f\"PaddleOCR-VL API error: {err}\")\n                    elif error_msg and not _is_success_message(error_msg):\n                        raise RuntimeError(f\"PaddleOCR-VL API error: {error_msg}\")\n                result = data.get(\"result\") if \"result\" in data else data\n            else:\n                result = data\n            if isinstance(result, dict):\n                for key in (\n                    \"layoutParsingResults\",\n                    \"layout_parsing_results\",\n                    \"layoutParsingResult\",\n                    \"layout_parsing_result\",\n                ):\n                    val = result.get(key)\n                    if isinstance(val, list):\n                        return val\n                    if isinstance(val, dict):\n                        return [val]\n            if isinstance(result, list):\n                return [r for r in result if isinstance(r, dict)]\n            return []\n\n        def _extract_markdown_text(entry: Dict[str, Any]) -> Optional[str]:\n            md_info = entry.get(\"markdown\")\n            if isinstance(md_info, dict):\n                for key in (\"text\", \"markdown\", \"content\"):\n                    val = md_info.get(key)\n                    if isinstance(val, str) and val.strip():\n                        return val.strip()\n            if isinstance(md_info, str) and md_info.strip():\n                return md_info.strip()\n            for key in (\"markdown\", \"markdown_text\", \"text\", \"content\"):\n                val = entry.get(key)\n                if isinstance(val, str) and val.strip():\n                    return val.strip()\n            return None\n\n        def _extract_markdown_images(entry: Dict[str, Any]) -> Dict[str, Any]:\n            images: Dict[str, Any] = {}\n            md_info = entry.get(\"markdown\")\n            if isinstance(md_info, dict):\n                candidate = md_info.get(\"images\") or md_info.get(\"markdown_images\") or md_info.get(\"markdownImages\")\n                if isinstance(candidate, dict):\n                    images.update(candidate)\n            for key in (\"markdown_images\", \"markdownImages\"):\n                candidate = entry.get(key)\n                if isinstance(candidate, dict):\n                    images.update(candidate)\n            return images\n\n        def _extract_page_text(entry: Dict[str, Any], md_text: Optional[str]) -> str:\n            for key in (\"text\", \"ocrText\", \"ocr_text\", \"content\"):\n                val = entry.get(key)\n                if isinstance(val, str) and val.strip():\n                    return val.strip()\n            if md_text:\n                return _strip_markup(md_text)\n            return \"\"\n\n        def _image_to_bytes(image: Any) -> bytes:\n            if isinstance(image, (bytes, bytearray)):\n                return bytes(image)\n            if isinstance(image, str) and os.path.isfile(image):\n                with open(image, \"rb\") as handle:\n                    return handle.read()\n            if hasattr(image, \"save\"):\n                img = image\n                if hasattr(img, \"convert\"):\n                    try:\n                        img = img.convert(\"RGB\")\n                    except Exception:\n                        img = image\n                buffer = io.BytesIO()\n                try:\n                    img.save(buffer, format=\"PNG\")\n                except Exception:\n                    buffer = io.BytesIO()\n                    img.save(buffer, format=\"JPEG\")\n                return buffer.getvalue()\n            try:\n                import numpy as np\n                from PIL import Image as _PILImage\n            except Exception:\n                raise RuntimeError(\"Unsupported image type for PaddleOCR-VL API.\")\n            if isinstance(image, np.ndarray):\n                buffer = io.BytesIO()\n                _PILImage.fromarray(image).save(buffer, format=\"PNG\")\n                return buffer.getvalue()\n            raise RuntimeError(\"Unsupported image type for PaddleOCR-VL API.\")\n\n        pages: List[Dict[str, Any]] = []\n        markdown_items: List[str] = []\n        markdown_images: Dict[str, Any] = {}\n        markdown_image_labels: Dict[str, str] = {}\n        page_counter = 0\n        if progress_cb and progress_span > 0:\n            progress_cb(progress_base, \"ocr\", \"Paddle OCR-VL API initializing\")\n\n        def _normalize_image_ref(value: Any) -> Optional[str]:\n            if isinstance(value, str):\n                return value.strip() or None\n            if isinstance(value, dict):\n                for key in (\"image\", \"img\", \"src\", \"url\", \"path\", \"file\", \"file_path\", \"filePath\"):\n                    cand = value.get(key)\n                    if isinstance(cand, str) and cand.strip():\n                        return cand.strip()\n            return None\n\n        def _merge_label(existing: Optional[str], incoming: str) -> str:\n            if not existing:\n                return incoming\n            if incoming.lower() in existing.lower():\n                return existing\n            if existing.lower() in incoming.lower():\n                return incoming\n            return f\"{existing}; {incoming}\"\n\n        def _store_image_label(ref: str, label: str) -> None:\n            if not ref or not label:\n                return\n            label = label.strip()\n            if not label:\n                return\n            markdown_image_labels[ref] = _merge_label(markdown_image_labels.get(ref), label)\n            filename = os.path.basename(ref)\n            if filename:\n                markdown_image_labels[filename] = _merge_label(markdown_image_labels.get(filename), label)\n\n        def _extract_block_bbox(block: Dict[str, Any]) -> Optional[List[float]]:\n            for key in (\"block_bbox\", \"bbox\", \"box\", \"rect\", \"xyxy\"):\n                val = block.get(key)\n                if isinstance(val, (list, tuple)) and len(val) >= 4:\n                    try:\n                        return [float(val[0]), float(val[1]), float(val[2]), float(val[3])]\n                    except Exception:\n                        continue\n                if isinstance(val, dict):\n                    if all(k in val for k in (\"x0\", \"y0\", \"x1\", \"y1\")):\n                        try:\n                            return [float(val[\"x0\"]), float(val[\"y0\"]), float(val[\"x1\"]), float(val[\"y1\"])]\n                        except Exception:\n                            continue\n                    if all(k in val for k in (\"left\", \"top\", \"width\", \"height\")):\n                        try:\n                            left = float(val[\"left\"])\n                            top = float(val[\"top\"])\n                            return [left, top, left + float(val[\"width\"]), top + float(val[\"height\"])]\n                        except Exception:\n                            continue\n            return None\n\n        def _extract_block_text(block: Dict[str, Any]) -> Optional[str]:\n            text_keys = {\n                \"block_content\",\n                \"text\",\n                \"content\",\n                \"ocr_text\",\n                \"ocrText\",\n                \"caption\",\n                \"figure_caption\",\n                \"footnote\",\n                \"note\",\n                \"value\",\n            }\n            fragments: List[str] = []\n\n            def walk(value: Any, depth: int = 0) -> None:\n                if depth > 4 or len(fragments) >= 8:\n                    return\n                if isinstance(value, str):\n                    chunk = value.strip()\n                    if chunk:\n                        fragments.append(chunk)\n                    return\n                if isinstance(value, dict):\n                    for key in text_keys:\n                        if key in value:\n                            walk(value[key], depth + 1)\n                    for item in value.values():\n                        walk(item, depth + 1)\n                elif isinstance(value, list):\n                    for item in value[:8]:\n                        walk(item, depth + 1)\n\n            walk(block)\n            if not fragments:\n                return None\n            deduped: List[str] = []\n            seen: Set[str] = set()\n            for frag in fragments:\n                if frag in seen:\n                    continue\n                seen.add(frag)\n                deduped.append(frag)\n            return \" \".join(deduped).strip() or None\n\n        def _parse_bbox_from_image_key(key: str) -> Optional[List[float]]:\n            match = re.search(r\"_(\\d+(?:\\.\\d+)?)_(\\d+(?:\\.\\d+)?)_(\\d+(?:\\.\\d+)?)_(\\d+(?:\\.\\d+)?)\\.(?:png|jpg|jpeg|webp)$\", key, re.IGNORECASE)\n            if not match:\n                return None\n            try:\n                return [float(match.group(1)), float(match.group(2)), float(match.group(3)), float(match.group(4))]\n            except Exception:\n                return None\n\n        def _bbox_overlap_x(a: List[float], b: List[float]) -> float:\n            overlap = max(0.0, min(a[2], b[2]) - max(a[0], b[0]))\n            width = max(1.0, min(a[2] - a[0], b[2] - b[0]))\n            return overlap / width if width > 0 else 0.0\n\n        def _attach_vision_footnotes(entry: Dict[str, Any], md_images: Dict[str, Any]) -> None:\n            if not md_images:\n                return\n            pruned = entry.get(\"prunedResult\")\n            parsing_list = None\n            if isinstance(pruned, dict):\n                parsing_list = pruned.get(\"parsing_res_list\")\n            elif isinstance(pruned, list):\n                parsing_list = pruned\n            if not isinstance(parsing_list, list):\n                return\n            image_blocks: List[Dict[str, Any]] = []\n            footnote_blocks: List[Dict[str, Any]] = []\n            for block in parsing_list:\n                if not isinstance(block, dict):\n                    continue\n                label = (\n                    block.get(\"block_label\")\n                    or block.get(\"blockLabel\")\n                    or block.get(\"label\")\n                    or block.get(\"type\")\n                    or \"\"\n                )\n                label = str(label).strip().lower()\n                bbox = _extract_block_bbox(block)\n                if label == \"image\" and bbox:\n                    image_blocks.append({\"bbox\": bbox})\n                elif label == \"vision_footnote\" and bbox:\n                    text = _extract_block_text(block)\n                    if text:\n                        footnote_blocks.append({\"bbox\": bbox, \"text\": text})\n            if not image_blocks or not footnote_blocks:\n                return\n            image_keys = [key for key in md_images.keys() if isinstance(key, str)]\n            image_key_bboxes: List[Tuple[str, List[float]]] = []\n            for key in image_keys:\n                bbox = _parse_bbox_from_image_key(key)\n                if bbox:\n                    image_key_bboxes.append((key, bbox))\n            image_block_to_key: Dict[int, str] = {}\n            if image_key_bboxes:\n                for idx, block in enumerate(image_blocks):\n                    best_key = None\n                    best_score = None\n                    for key, bbox in image_key_bboxes:\n                        score = sum(abs(a - b) for a, b in zip(block[\"bbox\"], bbox))\n                        if best_score is None or score < best_score:\n                            best_score = score\n                            best_key = key\n                    if best_key:\n                        image_block_to_key[idx] = best_key\n            if not image_block_to_key and len(image_blocks) == 1 and len(image_keys) == 1:\n                image_block_to_key[0] = image_keys[0]\n\n            for footnote in footnote_blocks:\n                best_idx = None\n                best_gap = None\n                for idx, image_block in enumerate(image_blocks):\n                    img_bbox = image_block[\"bbox\"]\n                    foot_bbox = footnote[\"bbox\"]\n                    overlap_ratio = _bbox_overlap_x(img_bbox, foot_bbox)\n                    if overlap_ratio < 0.2:\n                        continue\n                    vertical_gap = foot_bbox[1] - img_bbox[3]\n                    if vertical_gap < -10:\n                        continue\n                    gap_score = max(0.0, vertical_gap)\n                    if best_gap is None or gap_score < best_gap:\n                        best_gap = gap_score\n                        best_idx = idx\n                if best_idx is None:\n                    continue\n                key = image_block_to_key.get(best_idx)\n                if key:\n                    _store_image_label(key, footnote[\"text\"])\n\n        def _collect_block_labels(value: Any) -> None:\n            if isinstance(value, dict):\n                label = (\n                    value.get(\"block_label\")\n                    or value.get(\"blockLabel\")\n                    or value.get(\"label\")\n                    or value.get(\"blockLabelName\")\n                )\n                image_ref = _normalize_image_ref(\n                    value.get(\"image\")\n                    or value.get(\"img\")\n                    or value.get(\"image_path\")\n                    or value.get(\"imagePath\")\n                    or value.get(\"img_path\")\n                    or value.get(\"src\")\n                )\n                if isinstance(label, str) and image_ref:\n                    _store_image_label(image_ref, label)\n                for item in value.values():\n                    _collect_block_labels(item)\n            elif isinstance(value, list):\n                for item in value:\n                    _collect_block_labels(item)\n\n        def _append_page(entry: Dict[str, Any]) -> None:\n            nonlocal page_counter\n            md_text = _extract_markdown_text(entry)\n            if md_text:\n                markdown_items.append(md_text)\n            md_images = _extract_markdown_images(entry)\n            if md_images:\n                markdown_images.update(md_images)\n                _attach_vision_footnotes(entry, md_images)\n            _collect_block_labels(entry)\n            page_counter += 1\n            text = _extract_page_text(entry, md_text)\n            page_entry = {\"page_num\": page_counter, \"text\": (text or \"\").strip()}\n            if isinstance(md_text, str) and md_text.strip():\n                page_entry[\"markdown\"] = md_text.strip()\n            pages.append(page_entry)\n\n        def _run_api_for_images(\n            image_list: Optional[List[Any]] = None,\n            overall_total: Optional[int] = None,\n            page_offset: int = 0,\n        ) -> None:\n            images_to_process = image_list if image_list is not None else api_images\n            total = max(1, len(images_to_process))\n            for idx, image in enumerate(images_to_process, start=1):\n                if progress_cb and progress_span > 0:\n                    if overall_total:\n                        current_idx = page_offset + idx\n                        percent = progress_base + int(max(0, current_idx - 1) / overall_total * progress_span)\n                        progress_cb(percent, \"ocr\", f\"Paddle OCR-VL API page {current_idx}/{overall_total}\")\n                    else:\n                        percent = progress_base + int((idx - 1) / total * progress_span)\n                        progress_cb(percent, \"ocr\", f\"Paddle OCR-VL API page {idx}/{total}\")\n                file_bytes = _image_to_bytes(image)\n                data = _request_api(file_bytes, 1, f\"page {idx}/{total}\")\n                layout_results = _extract_layout_results(data)\n                if not layout_results:\n                    _append_page({})\n                else:\n                    for entry in layout_results:\n                        _append_page(entry)\n                if progress_cb and progress_span > 0:\n                    if overall_total:\n                        current_idx = page_offset + idx\n                        percent = progress_base + int(current_idx / overall_total * progress_span)\n                        progress_cb(percent, \"ocr\", f\"Paddle OCR-VL API page {current_idx}/{overall_total}\")\n                    else:\n                        percent = progress_base + int(idx / total * progress_span)\n                        progress_cb(percent, \"ocr\", f\"Paddle OCR-VL API page {idx}/{total}\")\n\n        if isinstance(source_path, str) and os.path.isfile(source_path):\n            file_type = 0 if source_path.lower().endswith(\".pdf\") else 1\n            chunked = False\n            needs_chunking = file_type == 0 and (\n                api_max_chunk_bytes > 0\n                or (source_page_count and source_page_count > api_max_pages)\n            )\n            if needs_chunking and source_reader is not None:\n                total_pages = source_page_count or len(source_reader.pages)\n                start = 0\n                processed_any = False\n\n                def _process_pdf_chunk(page_start: int, page_list: Sequence[Any]) -> None:\n                    if not page_list:\n                        return\n                    chunk_len = len(page_list)\n                    label = f\"{os.path.basename(source_path)} p{page_start + 1}-{page_start + chunk_len}\"\n                    try:\n                        file_bytes = _build_pdf_bytes(page_list)\n                    except Exception as exc:\n                        LOGGER.warning(\"Failed to build PDF chunk: %s\", exc)\n                        if api_images:\n                            image_slice = api_images[page_start : page_start + chunk_len]\n                            _run_api_for_images(image_slice, total_pages, page_start)\n                        return\n                    try:\n                        data = _request_api(file_bytes, file_type, label)\n                    except Exception as exc:\n                        if _is_http_500_error(exc) and chunk_len > 1:\n                            mid = chunk_len // 2\n                            LOGGER.warning(\n                                \"PaddleOCR-VL API 500 for pages %d-%d; splitting and retrying.\",\n                                page_start + 1,\n                                page_start + chunk_len,\n                            )\n                            _process_pdf_chunk(page_start, page_list[:mid])\n                            _process_pdf_chunk(page_start + mid, page_list[mid:])\n                            return\n                        if _is_http_500_error(exc) and api_images:\n                            LOGGER.warning(\n                                \"PaddleOCR-VL API 500 for pages %d-%d; retrying per-page images.\",\n                                page_start + 1,\n                                page_start + chunk_len,\n                            )\n                            image_slice = api_images[page_start : page_start + chunk_len]\n                            _run_api_for_images(image_slice, total_pages, page_start)\n                            return\n                        raise\n                    layout_results = _extract_layout_results(data)\n                    if not layout_results and api_images:\n                        LOGGER.warning(\n                            \"PaddleOCR-VL API returned no layout results for pages %d-%d; retrying per-page images.\",\n                            page_start + 1,\n                            page_start + chunk_len,\n                        )\n                        image_slice = api_images[page_start : page_start + chunk_len]\n                        _run_api_for_images(image_slice, total_pages, page_start)\n                    else:\n                        for entry in layout_results:\n                            _append_page(entry)\n                            if progress_cb and progress_span > 0:\n                                percent = progress_base + int(page_counter / total_pages * progress_span)\n                                progress_cb(percent, \"ocr\", f\"Paddle OCR-VL API page {page_counter}/{total_pages}\")\n                        if not layout_results:\n                            for _ in range(max(1, chunk_len)):\n                                _append_page({})\n\n                while start < total_pages:\n                    chunk_pages: List[Any] = []\n                    chunk_bytes = b\"\"\n                    page_idx = start\n                    while page_idx < total_pages and len(chunk_pages) < api_max_pages:\n                        chunk_pages.append(source_reader.pages[page_idx])\n                        page_idx += 1\n                        if api_max_chunk_bytes > 0:\n                            try:\n                                chunk_bytes = _build_pdf_bytes(chunk_pages)\n                            except Exception as exc:\n                                LOGGER.warning(\"Failed to build PDF chunk: %s\", exc)\n                                chunk_pages.pop()\n                                break\n                            if _estimate_payload_bytes(chunk_bytes) > api_max_chunk_bytes:\n                                if len(chunk_pages) == 1:\n                                    LOGGER.warning(\n                                        \"Single-page PDF chunk exceeds payload cap (%d bytes).\",\n                                        api_max_chunk_bytes,\n                                    )\n                                else:\n                                    chunk_pages.pop()\n                                    try:\n                                        chunk_bytes = _build_pdf_bytes(chunk_pages)\n                                    except Exception as exc:\n                                        LOGGER.warning(\"Failed to build PDF chunk: %s\", exc)\n                                        chunk_pages = []\n                                break\n                    if not chunk_pages:\n                        break\n                    if api_max_chunk_bytes <= 0:\n                        try:\n                            chunk_bytes = _build_pdf_bytes(chunk_pages)\n                        except Exception as exc:\n                            LOGGER.warning(\"Failed to build PDF chunk: %s\", exc)\n                            break\n                    chunk_len = len(chunk_pages)\n                    _process_pdf_chunk(start, chunk_pages)\n                    processed_any = True\n                    start += chunk_len\n                if start < total_pages and api_images:\n                    LOGGER.warning(\n                        \"PaddleOCR-VL API chunking incomplete; retrying remaining pages per-image.\",\n                    )\n                    image_slice = api_images[start:total_pages]\n                    _run_api_for_images(image_slice, total_pages, start)\n                    processed_any = True\n                    start = total_pages\n                chunked = processed_any and start >= total_pages\n            if not chunked:\n                if needs_chunking and file_type == 0 and api_images:\n                    LOGGER.warning(\n                        \"PaddleOCR-VL API chunking unavailable; using per-page images instead.\",\n                    )\n                    _run_api_for_images(api_images, source_page_count or len(api_images))\n                else:\n                    with open(source_path, \"rb\") as handle:\n                        file_bytes = handle.read()\n                    data = _request_api(file_bytes, file_type, os.path.basename(source_path))\n                    layout_results = _extract_layout_results(data)\n                    if not layout_results and file_type == 0 and api_images:\n                        LOGGER.warning(\"PaddleOCR-VL API returned no layout results for PDF; retrying per-page images.\")\n                        _run_api_for_images(api_images, source_page_count)\n                    else:\n                        total_pages = len(layout_results) or max(1, len(api_images))\n                        for entry in layout_results:\n                            _append_page(entry)\n                            if progress_cb and progress_span > 0:\n                                percent = progress_base + int(page_counter / total_pages * progress_span)\n                                progress_cb(percent, \"ocr\", f\"Paddle OCR-VL API page {page_counter}/{total_pages}\")\n                        if not layout_results:\n                            for _ in range(max(1, len(api_images))):\n                                _append_page({})\n        else:\n            _run_api_for_images()\n\n        layout_markdown = \"\\n\\n\".join(markdown_items) if markdown_items else None\n        if isinstance(layout_markdown, str) and layout_markdown.strip():\n            layout_markdown = _normalize_inline_math_for_obsidian(\n                layout_markdown,\n                add_footnote_defs=True,\n            )\n        for page in pages:\n            md_value = page.get(\"markdown\")\n            if isinstance(md_value, str) and md_value.strip():\n                page[\"markdown\"] = _normalize_inline_math_for_obsidian(md_value)\n        text_chars = ocr_pages_text_chars(pages)\n        if text_chars == 0 and isinstance(layout_markdown, str) and layout_markdown.strip():\n            fallback_text = _strip_markup(layout_markdown)\n            if fallback_text:\n                if pages:\n                    pages[0][\"text\"] = fallback_text\n                else:\n                    pages = [{\"page_num\": 1, \"text\": fallback_text}]\n                text_chars = ocr_pages_text_chars(pages)\n        LOGGER.info(\n            \"PaddleOCR-VL API OCR complete: pages=%d, text_chars=%d\",\n            len(pages),\n            text_chars,\n        )\n        stats: Dict[str, Any] = {\n            \"layout_used\": True,\n            \"layout_model\": \"PaddleOCR-VL API\",\n        }\n        if isinstance(layout_markdown, str) and layout_markdown.strip():\n            stats[\"layout_markdown\"] = layout_markdown\n        if markdown_images:\n            stats[\"layout_markdown_images\"] = markdown_images\n        if markdown_image_labels:\n            stats[\"layout_markdown_image_labels\"] = markdown_image_labels\n        return pages, stats\n\n    try:\n        import numpy as np\n        from paddleocr import PaddleOCRVL\n    except Exception as exc:\n        raise RuntimeError(f\"PaddleOCR-VL dependencies missing (install paddleocr[doc-parser]): {exc}\") from exc\n\n    pipeline_kwargs: Dict[str, Any] = {}\n    if getattr(config, \"paddle_use_doc_orientation_classify\", None) is not None:\n        pipeline_kwargs[\"use_doc_orientation_classify\"] = bool(config.paddle_use_doc_orientation_classify)\n    if getattr(config, \"paddle_use_doc_unwarping\", None) is not None:\n        pipeline_kwargs[\"use_doc_unwarping\"] = bool(config.paddle_use_doc_unwarping)\n    use_layout_detection = getattr(config, \"paddle_vl_use_layout_detection\", None)\n    if use_layout_detection is not None:\n        pipeline_kwargs[\"use_layout_detection\"] = bool(use_layout_detection)\n    if getattr(config, \"paddle_vl_use_chart_recognition\", None) is not None:\n        pipeline_kwargs[\"use_chart_recognition\"] = bool(config.paddle_vl_use_chart_recognition)\n    if getattr(config, \"paddle_vl_format_block_content\", None) is not None:\n        pipeline_kwargs[\"format_block_content\"] = bool(config.paddle_vl_format_block_content)\n    if getattr(config, \"paddle_vl_device\", None):\n        pipeline_kwargs[\"device\"] = str(config.paddle_vl_device)\n    if getattr(config, \"paddle_vl_rec_backend\", None):\n        pipeline_kwargs[\"vl_rec_backend\"] = str(config.paddle_vl_rec_backend)\n    if getattr(config, \"paddle_vl_rec_server_url\", None):\n        pipeline_kwargs[\"vl_rec_server_url\"] = str(config.paddle_vl_rec_server_url)\n    if getattr(config, \"paddle_vl_rec_max_concurrency\", None) is not None:\n        pipeline_kwargs[\"vl_rec_max_concurrency\"] = int(config.paddle_vl_rec_max_concurrency)\n    if getattr(config, \"paddle_vl_rec_api_key\", None):\n        pipeline_kwargs[\"vl_rec_api_key\"] = str(config.paddle_vl_rec_api_key)\n\n    predict_kwargs: Dict[str, Any] = {}\n    if getattr(config, \"paddle_use_doc_orientation_classify\", None) is not None:\n        predict_kwargs[\"use_doc_orientation_classify\"] = bool(config.paddle_use_doc_orientation_classify)\n    if getattr(config, \"paddle_use_doc_unwarping\", None) is not None:\n        predict_kwargs[\"use_doc_unwarping\"] = bool(config.paddle_use_doc_unwarping)\n    if use_layout_detection is not None:\n        predict_kwargs[\"use_layout_detection\"] = bool(use_layout_detection)\n    if getattr(config, \"paddle_vl_use_chart_recognition\", None) is not None:\n        predict_kwargs[\"use_chart_recognition\"] = bool(config.paddle_vl_use_chart_recognition)\n    if getattr(config, \"paddle_vl_format_block_content\", None) is not None:\n        predict_kwargs[\"format_block_content\"] = bool(config.paddle_vl_format_block_content)\n    layout_threshold = getattr(config, \"paddle_vl_layout_threshold\", None)\n    if layout_threshold is None:\n        layout_threshold = getattr(config, \"paddle_layout_threshold\", None)\n    if layout_threshold is not None:\n        predict_kwargs[\"layout_threshold\"] = layout_threshold\n    layout_nms = getattr(config, \"paddle_vl_layout_nms\", None)\n    if layout_nms is None:\n        layout_nms = getattr(config, \"paddle_layout_nms\", None)\n    if layout_nms is not None:\n        predict_kwargs[\"layout_nms\"] = bool(layout_nms)\n    layout_unclip = getattr(config, \"paddle_vl_layout_unclip\", None)\n    if layout_unclip is None:\n        layout_unclip = getattr(config, \"paddle_layout_unclip\", None)\n    if layout_unclip is not None:\n        predict_kwargs[\"layout_unclip_ratio\"] = layout_unclip\n    layout_merge = getattr(config, \"paddle_vl_layout_merge\", None)\n    if layout_merge is None:\n        layout_merge = getattr(config, \"paddle_layout_merge\", None)\n    if layout_merge:\n        predict_kwargs[\"layout_merge_bboxes_mode\"] = layout_merge\n    if getattr(config, \"paddle_vl_prompt_label\", None) and use_layout_detection is False:\n        predict_kwargs[\"prompt_label\"] = str(config.paddle_vl_prompt_label)\n    if getattr(config, \"paddle_vl_use_queues\", None) is not None:\n        predict_kwargs[\"use_queues\"] = bool(config.paddle_vl_use_queues)\n\n    pipeline = PaddleOCRVL(**pipeline_kwargs)\n\n    def _as_dict(obj: Any) -> Optional[Dict[str, Any]]:\n        if isinstance(obj, dict):\n            return obj\n        to_dict = getattr(obj, \"to_dict\", None)\n        if callable(to_dict):\n            try:\n                converted = to_dict()\n                if isinstance(converted, dict):\n                    return converted\n            except Exception:\n                return None\n        return None\n\n    def _result_to_dict(res: Any) -> Optional[Dict[str, Any]]:\n        direct = _as_dict(res)\n        if direct is not None:\n            return direct\n        for attr in (\"json\", \"res\", \"result\"):\n            val = getattr(res, attr, None)\n            val_dict = _as_dict(val)\n            if val_dict is not None:\n                return val_dict\n        return None\n\n    def _extract_markdown_text(md_info: Any, md_dict: Optional[Dict[str, Any]]) -> Optional[str]:\n        if isinstance(md_dict, dict):\n            for key in (\"markdown\", \"markdown_text\", \"text\", \"content\"):\n                val = md_dict.get(key)\n                if isinstance(val, str) and val.strip():\n                    return val.strip()\n        if isinstance(md_info, str) and md_info.strip():\n            return md_info.strip()\n        if md_info is not None:\n            for attr in (\"markdown\", \"markdown_text\", \"text\", \"content\"):\n                val = getattr(md_info, attr, None)\n                if isinstance(val, str) and val.strip():\n                    return val.strip()\n        return None\n\n    def _extract_markdown(\n        res: Any,\n        res_dict: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[Optional[str], Optional[Any], Optional[Dict[str, Any]]]:\n        md_info = getattr(res, \"markdown\", None)\n        if md_info is None and isinstance(res_dict, dict):\n            md_info = res_dict.get(\"markdown\") or res_dict.get(\"layout_markdown\")\n        md_dict = _as_dict(md_info)\n        md_text = _extract_markdown_text(md_info, md_dict)\n        if not md_text and isinstance(res_dict, dict):\n            for key in (\"markdown\", \"layout_markdown\", \"markdown_text\", \"text\"):\n                val = res_dict.get(key)\n                if isinstance(val, str) and val.strip():\n                    md_text = val.strip()\n                    break\n        return md_text, md_info, md_dict\n\n    def _extract_markdown_images(\n        md_info: Any,\n        md_dict: Optional[Dict[str, Any]],\n        res_dict: Optional[Dict[str, Any]],\n    ) -> Dict[str, Any]:\n        images: Dict[str, Any] = {}\n        for source in (md_dict, res_dict):\n            if not isinstance(source, dict):\n                continue\n            candidate = source.get(\"markdown_images\") or source.get(\"images\")\n            if isinstance(candidate, dict):\n                images.update(candidate)\n        if md_info is not None:\n            for attr in (\"markdown_images\", \"images\"):\n                candidate = getattr(md_info, attr, None)\n                if isinstance(candidate, dict):\n                    images.update(candidate)\n        return images\n\n    def _extract_block_text(res: Any, res_dict: Optional[Dict[str, Any]] = None) -> str:\n        candidates: List[Any] = []\n        if isinstance(res_dict, dict):\n            candidates.append(res_dict)\n            if \"res\" in res_dict:\n                candidates.append(res_dict.get(\"res\"))\n        inner_res = getattr(res, \"res\", None)\n        if inner_res is not None:\n            candidates.append(inner_res)\n        candidates.append(res)\n        for candidate in candidates:\n            if candidate is None:\n                continue\n            if isinstance(candidate, dict):\n                blocks = candidate.get(\"parsing_res_list\") or candidate.get(\"layout_parsing_res\")\n            else:\n                blocks = getattr(candidate, \"parsing_res_list\", None) or getattr(candidate, \"layout_parsing_res\", None)\n            if not isinstance(blocks, list) or not blocks:\n                continue\n            lines: List[str] = []\n            for block in blocks:\n                if isinstance(block, dict):\n                    text_val = block.get(\"block_content\") or block.get(\"content\") or block.get(\"text\")\n                else:\n                    text_val = getattr(block, \"block_content\", None)\n                    if text_val is None:\n                        text_val = getattr(block, \"content\", None)\n                    if text_val is None:\n                        text_val = getattr(block, \"text\", None)\n                if isinstance(text_val, str) and text_val.strip():\n                    lines.append(text_val.strip())\n            if lines:\n                return \"\\n\".join(lines).strip()\n        return \"\"\n\n    def _strip_markup(text: str) -> str:\n        text = re.sub(r\"<[^>]+>\", \" \", text)\n        text = re.sub(r\"!\\[[^\\]]*]\\([^)]+\\)\", \" \", text)\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text.strip()\n\n    pages: List[Dict[str, Any]] = []\n    markdown_items: List[Any] = []\n    markdown_images: Dict[str, Any] = {}\n    total = max(1, len(images))\n    if progress_cb and progress_span > 0:\n        progress_cb(progress_base, \"ocr\", \"Paddle OCR-VL initializing\")\n\n    for idx, image in enumerate(images, start=1):\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int((idx - 1) / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"Paddle OCR-VL page {idx}/{total}\")\n        try:\n            img = image.convert(\"RGB\") if hasattr(image, \"convert\") else image\n        except Exception:\n            img = image\n        img_arr = np.array(img)\n        results = pipeline.predict(img_arr, **predict_kwargs)\n        if not results:\n            pages.append({\"page_num\": idx, \"text\": \"\"})\n            continue\n        res = results[0]\n        res_dict = _result_to_dict(res)\n        md_text, md_info, md_dict = _extract_markdown(res, res_dict)\n        md_images = _extract_markdown_images(md_info, md_dict, res_dict)\n        if md_info is not None:\n            if isinstance(md_info, str) and md_info.strip():\n                markdown_items.append({\"markdown\": md_info.strip()})\n            else:\n                markdown_items.append(md_dict if md_dict is not None else md_info)\n        elif md_text:\n            markdown_items.append({\"markdown\": md_text})\n        if md_images:\n            markdown_images.update(md_images)\n        text = _extract_block_text(res, res_dict)\n        if not text and md_text:\n            text = _strip_markup(md_text)\n        pages.append({\"page_num\": idx, \"text\": (text or \"\").strip()})\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"Paddle OCR-VL page {idx}/{total}\")\n\n    layout_markdown = None\n    if markdown_items:\n        concat = getattr(pipeline, \"concatenate_markdown_pages\", None)\n        if callable(concat):\n            try:\n                layout_markdown = concat(markdown_items)\n            except Exception:\n                layout_markdown = None\n        if layout_markdown is None:\n            page_texts: List[str] = []\n            for md in markdown_items:\n                text_val = _extract_markdown_text(md, _as_dict(md))\n                if isinstance(text_val, str) and text_val.strip():\n                    page_texts.append(text_val.strip())\n            if page_texts:\n                layout_markdown = \"\\n\\n\".join(page_texts)\n\n    text_chars = ocr_pages_text_chars(pages)\n    if text_chars == 0 and isinstance(layout_markdown, str) and layout_markdown.strip():\n        fallback_text = _strip_markup(layout_markdown)\n        if fallback_text:\n            if pages:\n                pages[0][\"text\"] = fallback_text\n            else:\n                pages = [{\"page_num\": 1, \"text\": fallback_text}]\n            text_chars = ocr_pages_text_chars(pages)\n    LOGGER.info(\n        \"PaddleOCR-VL OCR complete: pages=%d, text_chars=%d\",\n        len(pages),\n        text_chars,\n    )\n    stats: Dict[str, Any] = {\n        \"layout_used\": True,\n        \"layout_model\": \"PaddleOCR-VL\",\n    }\n    if isinstance(layout_markdown, str) and layout_markdown.strip():\n        stats[\"layout_markdown\"] = layout_markdown\n    if markdown_images:\n        stats[\"layout_markdown_images\"] = markdown_images\n    return pages, stats\n\n\ndef ocr_pages_with_paddle(\n    images: Sequence[Any],\n    languages: str,\n    config: Any,\n    helpers: Dict[str, Any],\n    progress_cb: Optional[Any] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    global LOGGER\n    LOGGER = helpers.get(\"logger\", LOGGER)\n    ocr_pages_text_chars = helpers[\"ocr_pages_text_chars\"]\n    detect_repeated_line_clusters = helpers[\"detect_repeated_line_clusters\"]\n    normalize_boilerplate_line = helpers[\"normalize_boilerplate_line\"]\n    matches_repeated_cluster = helpers[\"matches_repeated_cluster\"]\n    is_boilerplate_line = helpers[\"is_boilerplate_line\"]\n    edge_ids_by_y = helpers[\"edge_ids_by_y\"]\n    select_edge_texts_by_y = helpers[\"select_edge_texts_by_y\"]\n    order_blocks_into_columns = helpers[\"order_blocks_into_columns\"]\n\n    from paddleocr import PaddleOCR\n\n    try:\n        import numpy as np\n    except Exception as exc:\n        raise RuntimeError(f\"numpy is required for PaddleOCR: {exc}\") from exc\n\n    # PaddleOCR orientation classification uses use_textline_orientation\n    ocr_kwargs: Dict[str, Any] = {\"lang\": languages}\n    if config.paddle_target_max_side_px > 0:\n        ocr_kwargs[\"text_det_limit_side_len\"] = config.paddle_target_max_side_px\n        ocr_kwargs[\"text_det_limit_type\"] = \"max\"\n    if config.paddle_use_doc_orientation_classify:\n        ocr_kwargs[\"use_doc_orientation_classify\"] = True\n    if config.paddle_use_doc_unwarping:\n        ocr_kwargs[\"use_doc_unwarping\"] = True\n\n    # Robust PaddleOCR construction to handle API differences across versions\n    def _create_ocr_direct(kwargs: Dict[str, Any]) -> PaddleOCR:\n        return PaddleOCR(**kwargs)\n\n    def _try_create_direct(kwargs: Dict[str, Any]) -> Optional[PaddleOCR]:\n        try:\n            return _create_ocr_direct(kwargs)\n        except TypeError:\n            return None\n        except Exception:\n            return None\n\n    reduced_kwargs = dict(ocr_kwargs)\n    reduced_kwargs.pop(\"use_doc_orientation_classify\", None)\n    reduced_kwargs.pop(\"use_doc_unwarping\", None)\n\n    ctor_candidates: List[Dict[str, Any]] = []\n    use_tlo = bool(getattr(config, \"paddle_use_textline_orientation\", False))\n    # Prefer explicit textline orientation when supported\n    ctor_candidates.append({**ocr_kwargs, \"use_textline_orientation\": use_tlo})\n    ctor_candidates.append({**reduced_kwargs, \"use_textline_orientation\": use_tlo})\n    # Without textline flag\n    ctor_candidates.append({**ocr_kwargs})\n    ctor_candidates.append({**reduced_kwargs})\n    # Legacy angle classifier flag\n    ctor_candidates.append({**ocr_kwargs, \"use_angle_cls\": use_tlo})\n    ctor_candidates.append({**reduced_kwargs, \"use_angle_cls\": use_tlo})\n\n    ocr: Optional[PaddleOCR] = None\n    for kw in ctor_candidates:\n        ocr = _try_create_direct(kw)\n        if ocr is not None:\n            break\n    if ocr is None:\n        # Final hard attempt to surface a meaningful error\n        ocr = _create_ocr_direct(ocr_kwargs)\n    pages: List[Dict[str, Any]] = []\n    confidences: List[float] = []\n\n    def _bbox_from_quad(quad: Sequence[Sequence[float]]) -> Tuple[float, float, float, float, float]:\n        xs = [p[0] for p in quad]\n        ys = [p[1] for p in quad]\n        x0, y0, x1, y1 = float(min(xs)), float(min(ys)), float(max(xs)), float(max(ys))\n        xc = 0.5 * (x0 + x1)\n        return x0, y0, x1, y1, xc\n\n    def _image_to_array(img: Any) -> Any:\n        if hasattr(img, \"convert\"):\n            try:\n                img = img.convert(\"RGB\")\n            except Exception:\n                pass\n        return np.array(img)\n\n    def _paddle_obj_to_dict(obj: Any) -> Optional[Dict[str, Any]]:\n        if obj is None:\n            return None\n        if isinstance(obj, dict):\n            return obj\n        to_dict = getattr(obj, \"to_dict\", None)\n        if callable(to_dict):\n            try:\n                converted = to_dict()\n                if isinstance(converted, dict):\n                    return converted\n            except Exception:\n                return None\n        rec_texts = getattr(obj, \"rec_texts\", None)\n        dt_polys = getattr(obj, \"dt_polys\", None)\n        if rec_texts is not None or dt_polys is not None:\n            return {\"rec_texts\": rec_texts, \"dt_polys\": dt_polys, \"rec_scores\": getattr(obj, \"rec_scores\", None)}\n        return None\n\n    def _extract_from_paddle_dict(result: Dict[str, Any]) -> List[Tuple[Any, str, Optional[float]]]:\n        texts = result.get(\"rec_texts\") or result.get(\"texts\") or result.get(\"rec_text\")\n        if not isinstance(texts, list):\n            return []\n        boxes = (\n            result.get(\"dt_polys\")\n            or result.get(\"det_polys\")\n            or result.get(\"dt_boxes\")\n            or result.get(\"boxes\")\n        )\n        scores = result.get(\"rec_scores\") or result.get(\"scores\") or result.get(\"rec_score\")\n        entries: List[Tuple[Any, str, Optional[float]]] = []\n        for idx, text_val in enumerate(texts):\n            text_str = str(text_val or \"\").strip()\n            if not text_str:\n                continue\n            quad = None\n            if isinstance(boxes, list) and idx < len(boxes):\n                quad = boxes[idx]\n            conf_val = None\n            if isinstance(scores, list) and idx < len(scores):\n                try:\n                    conf_val = float(scores[idx])\n                except Exception:\n                    conf_val = None\n            entries.append((quad, text_str, conf_val))\n        return entries\n\n    def _iter_paddle_entries(result: Any) -> List[Tuple[Any, str, Optional[float]]]:\n        if isinstance(result, dict):\n            return _extract_from_paddle_dict(result)\n        if isinstance(result, list):\n            entries = result\n            if len(result) == 1:\n                maybe_dict = _paddle_obj_to_dict(result[0])\n                if maybe_dict is not None:\n                    return _extract_from_paddle_dict(maybe_dict)\n                if isinstance(result[0], (list, tuple, dict)):\n                    entries = result[0]\n            if isinstance(entries, dict):\n                return _extract_from_paddle_dict(entries)\n            if isinstance(entries, list) and entries and isinstance(entries[0], dict):\n                combined: List[Tuple[Any, str, Optional[float]]] = []\n                for entry in entries:\n                    if isinstance(entry, dict):\n                        combined.extend(_extract_from_paddle_dict(entry))\n                    else:\n                        maybe_dict = _paddle_obj_to_dict(entry)\n                        if maybe_dict is not None:\n                            combined.extend(_extract_from_paddle_dict(maybe_dict))\n                return combined\n            extracted: List[Tuple[Any, str, Optional[float]]] = []\n            for entry in entries:\n                if not entry or not isinstance(entry, (list, tuple)):\n                    continue\n                quad = entry[0] if len(entry) > 0 else None\n                text_part = entry[1] if len(entry) > 1 else None\n                if text_part is None:\n                    continue\n                text_str = \"\"\n                conf_val = None\n                if isinstance(text_part, (list, tuple)) and text_part:\n                    text_str = str(text_part[0] or \"\").strip()\n                    if len(text_part) > 1 and isinstance(text_part[1], (float, int)):\n                        conf_val = float(text_part[1])\n                else:\n                    text_str = str(text_part or \"\").strip()\n                if text_str:\n                    extracted.append((quad, text_str, conf_val))\n            return extracted\n        return []\n\n    total = max(1, len(images))\n    # Emit an immediate progress update so the UI replaces the initial 'initializing' label\n    if progress_cb and progress_span > 0:\n        progress_cb(progress_base, \"ocr\", f\"Paddle OCR page 1/{total} (running)\")\n    total_pages = len(images)\n    boilerplate_enabled = bool(\n        config.enable_boilerplate_removal and helpers.get(\"boilerplate_prepass_enabled\", True)\n    )\n    repeat_threshold = 0\n    repeated_clusters: List[Any] = []\n    page_edge_candidates: List[List[str]] = []\n    source_path = helpers.get(\"ocr_source_path\")\n    doc_label = os.path.basename(source_path) if isinstance(source_path, str) and source_path else \"\"\n    doc_suffix = f\" ({doc_label})\" if doc_label else \"\"\n\n    for idx, image in enumerate(images, start=1):\n        if boilerplate_enabled:\n            LOGGER.info(\"Paddle OCR prepass %d/%d%s: start\", idx, total_pages, doc_suffix)\n        t_start = time.perf_counter()\n        edge_lines: List[Tuple[str, float]] = []\n        result = None\n        image_arr = _image_to_array(image)\n        # Try inference with multiple APIs for compatibility\n        def _run_ocr_inference(img_arr: Any) -> Any:\n            res = None\n            # Try modern API first\n            if hasattr(ocr, \"predict\"):\n                try:\n                    res = ocr.predict(img_arr)  # type: ignore[attr-defined]\n                except TypeError:\n                    res = None\n                except Exception:\n                    res = None\n            # Legacy API without cls\n            if res is None and hasattr(ocr, \"ocr\"):\n                try:\n                    res = ocr.ocr(img_arr)  # type: ignore[attr-defined]\n                except TypeError:\n                    res = None\n                except Exception:\n                    res = None\n            # Legacy API with cls flag\n            if res is None and hasattr(ocr, \"ocr\"):\n                try:\n                    res = ocr.ocr(img_arr, cls=use_tlo)  # type: ignore[attr-defined]\n                except Exception:\n                    res = None\n            return res\n\n        try:\n            result = _run_ocr_inference(image_arr)\n        except Exception as exc:\n            LOGGER.debug(\"PaddleOCR inference failed: %s\", exc)\n            result = None\n\n        if result is not None:\n            for quad, text_val, _ in _iter_paddle_entries(result):\n                if not text_val:\n                    continue\n                if quad is None:\n                    continue\n                try:\n                    _, y0_val, _, _, _ = _bbox_from_quad(quad)\n                except Exception:\n                    y0_val = 0.0\n                edge_lines.append((text_val, y0_val))\n        if boilerplate_enabled and edge_lines:\n            page_edge_candidates.append(\n                select_edge_texts_by_y(edge_lines, config.boilerplate_edge_lines)\n            )\n        if boilerplate_enabled:\n            elapsed = time.perf_counter() - t_start\n            LOGGER.info(\n                \"Paddle OCR prepass %d/%d%s: done in %.2fs (edge_lines=%d)\",\n                idx,\n                total_pages,\n                doc_suffix,\n                elapsed,\n                len(edge_lines),\n            )\n\n    if boilerplate_enabled and total_pages >= config.boilerplate_min_pages:\n        repeated_clusters, repeat_threshold = detect_repeated_line_clusters(\n            page_edge_candidates,\n            total_pages,\n            config,\n        )\n    removed_total = 0\n\n    for idx, image in enumerate(images, start=1):\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int((idx - 1) / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"Paddle OCR page {idx}/{total} (running)\")\n        LOGGER.info(\"Paddle OCR page %d/%d: start\", idx, total)\n        t_start = time.perf_counter()\n        # Prefer new API: predict(); fall back to ocr() with/without cls\n        image_arr = _image_to_array(image)\n        # Prefer new API, but fall back as needed\n        try:\n            result = _run_ocr_inference(image_arr)\n        except Exception:\n            result = None\n        blocks: List[Dict[str, Any]] = []\n        fallback_lines: List[str] = []\n        if result:\n            for quad, text_val, conf_val in _iter_paddle_entries(result):\n                if conf_val is not None:\n                    confidences.append(conf_val)\n                if not text_val:\n                    continue\n                if quad is None:\n                    fallback_lines.append(text_val)\n                    continue\n                try:\n                    x0, y0, x1, y1, xc = _bbox_from_quad(quad)\n                except Exception:\n                    fallback_lines.append(text_val)\n                    continue\n                blocks.append({\n                    \"x0\": x0,\n                    \"y0\": y0,\n                    \"x1\": x1,\n                    \"y1\": y1,\n                    \"xc\": xc,\n                    \"text\": text_val,\n                    \"line_id\": len(blocks),\n                })\n        edge_ids: Set[int] = set()\n        if boilerplate_enabled and blocks:\n            edge_ids = edge_ids_by_y(\n                [(b[\"line_id\"], b[\"y0\"]) for b in blocks],\n                config.boilerplate_edge_lines,\n            )\n        if edge_ids:\n            filtered_blocks: List[Dict[str, Any]] = []\n            for b in blocks:\n                normalized = normalize_boilerplate_line(str(b.get(\"text\", \"\")).strip())\n                is_edge = b.get(\"line_id\") in edge_ids\n                if is_edge and (\n                    matches_repeated_cluster(str(b.get(\"text\", \"\")), repeated_clusters, config)\n                    or is_boilerplate_line(normalized)\n                ):\n                    removed_total += 1\n                    continue\n                filtered_blocks.append(b)\n            blocks = filtered_blocks\n        if blocks:\n            ordered_text = order_blocks_into_columns(\n                blocks,\n                log_label=\"Paddle\",\n                preserve_single_column_order=True,\n            )\n        else:\n            ordered_text = \"\\n\".join(fallback_lines)\n        pages.append({\"page_num\": idx, \"text\": ordered_text})\n        elapsed = time.perf_counter() - t_start\n        LOGGER.info(\n            \"Paddle OCR page %d/%d: done in %.2fs (text_chars=%d, blocks=%d)\",\n            idx,\n            total,\n            elapsed,\n            len(ordered_text),\n            len(blocks),\n        )\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"Paddle OCR page {idx}/{total}\")\n\n    if removed_total and boilerplate_enabled:\n        LOGGER.info(\n            \"Boilerplate removal (OCR lines): removed %s lines (repeat_threshold=%s, repeated_lines=%s)\",\n            removed_total,\n            repeat_threshold,\n            len(repeated_clusters),\n        )\n\n    avg_conf = sum(confidences) / len(confidences) if confidences else None\n    LOGGER.info(\n        \"Paddle OCR complete: pages=%d, text_chars=%d\",\n        len(pages),\n        ocr_pages_text_chars(pages),\n    )\n    return pages, {\"ocr_confidence_avg\": avg_conf}\n",
  "ocr_tesseract.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport shutil\nimport time\nfrom typing import Any, Dict, List, Optional, Sequence, Set, Tuple\n\nLOGGER = logging.getLogger(\"docling_extract\")\nTESSERACT_LOGGED_ONCE = False\n\n\ndef find_tesseract_path() -> Optional[str]:\n    env_cmd = os.environ.get(\"TESSERACT_CMD\") or os.environ.get(\"TESSERACT_PATH\")\n    if env_cmd and os.path.isfile(env_cmd):\n        return env_cmd\n    tesseract_cmd = shutil.which(\"tesseract\")\n    if tesseract_cmd:\n        return tesseract_cmd\n    for candidate in (\"/opt/homebrew/bin/tesseract\", \"/usr/local/bin/tesseract\", \"/usr/bin/tesseract\"):\n        if os.path.isfile(candidate):\n            return candidate\n    return None\n\n\ndef ocr_pages_with_tesseract(\n    images: Sequence[Any],\n    languages: str,\n    config: Any,\n    helpers: Dict[str, Any],\n    progress_cb: Optional[Any] = None,\n    progress_base: int = 0,\n    progress_span: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    global LOGGER\n    LOGGER = helpers.get(\"logger\", LOGGER)\n    ocr_pages_text_chars = helpers[\"ocr_pages_text_chars\"]\n    detect_repeated_line_clusters = helpers[\"detect_repeated_line_clusters\"]\n    normalize_boilerplate_line = helpers[\"normalize_boilerplate_line\"]\n    matches_repeated_cluster = helpers[\"matches_repeated_cluster\"]\n    is_boilerplate_line = helpers[\"is_boilerplate_line\"]\n    edge_ids_by_y = helpers[\"edge_ids_by_y\"]\n    select_edge_texts_by_y = helpers[\"select_edge_texts_by_y\"]\n    split_blocks_into_columns = helpers[\"split_blocks_into_columns\"]\n\n    import pytesseract\n    tesseract_cmd = find_tesseract_path()\n    if tesseract_cmd:\n        global TESSERACT_LOGGED_ONCE\n        if shutil.which(\"tesseract\") is None and not TESSERACT_LOGGED_ONCE:\n            LOGGER.info(\"Tesseract not on PATH; using %s\", tesseract_cmd)\n            TESSERACT_LOGGED_ONCE = True\n        pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n    else:\n        raise RuntimeError(\"Tesseract not found on PATH; set TESSERACT_CMD or install tesseract.\")\n\n    def _safe_float(values: Any, idx: int) -> float:\n        if isinstance(values, list) and idx < len(values):\n            try:\n                return float(values[idx])\n            except Exception:\n                return 0.0\n        return 0.0\n\n    def _group_words_into_lines(words: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:\n        if not words:\n            return []\n        heights = sorted((w[\"y1\"] - w[\"y0\"]) for w in words)\n        h_med = heights[len(heights) // 2] if heights else 1.0\n        y_thr = max(4.0, 0.6 * h_med)\n        words_sorted = sorted(words, key=lambda w: (w[\"yc\"], w[\"x0\"]))\n        lines: List[List[Dict[str, Any]]] = []\n        current: List[Dict[str, Any]] = []\n        current_y: Optional[float] = None\n        for w in words_sorted:\n            if current_y is None or abs(w[\"yc\"] - current_y) <= y_thr:\n                current.append(w)\n            else:\n                lines.append(current)\n                current = [w]\n            current_y = w[\"yc\"]\n        if current:\n            lines.append(current)\n        return lines\n\n    pages: List[Dict[str, Any]] = []\n    confidences: List[float] = []\n    total = max(1, len(images))\n    repeat_threshold = 0\n    repeated_clusters: List[Any] = []\n    removed_total = 0\n    if config.enable_boilerplate_removal and total >= config.boilerplate_min_pages:\n        page_edge_candidates: List[List[str]] = []\n        for idx, image in enumerate(images, start=1):\n            LOGGER.info(\"Tesseract OCR prepass %d/%d: start\", idx, total)\n            t_start = time.perf_counter()\n            line_items: List[Tuple[str, float]] = []\n            try:\n                data = pytesseract.image_to_data(\n                    image, lang=languages, output_type=pytesseract.Output.DICT\n                )\n                items = len(data.get(\"text\", []))\n                words: List[Dict[str, Any]] = []\n                for i in range(items):\n                    raw_text = str(data[\"text\"][i] or \"\").strip()\n                    if not raw_text:\n                        continue\n                    x0 = _safe_float(data.get(\"left\"), i)\n                    y0 = _safe_float(data.get(\"top\"), i)\n                    x1 = x0 + _safe_float(data.get(\"width\"), i)\n                    y1 = y0 + _safe_float(data.get(\"height\"), i)\n                    yc = 0.5 * (y0 + y1)\n                    words.append(\n                        {\n                            \"x0\": x0,\n                            \"y0\": y0,\n                            \"x1\": x1,\n                            \"y1\": y1,\n                            \"yc\": yc,\n                            \"text\": raw_text,\n                        }\n                    )\n                for line_words in _group_words_into_lines(words):\n                    line_sorted = sorted(line_words, key=lambda w: w[\"x0\"])\n                    line_text = \" \".join(w[\"text\"] for w in line_sorted if w[\"text\"]).strip()\n                    if not line_text:\n                        continue\n                    line_y0 = min(w[\"y0\"] for w in line_sorted)\n                    line_items.append((line_text, line_y0))\n            except Exception:\n                line_items = []\n            elapsed = time.perf_counter() - t_start\n            LOGGER.info(\n                \"Tesseract OCR prepass %d/%d: done in %.2fs (edge_lines=%d)\",\n                idx,\n                total,\n                elapsed,\n                len(line_items),\n            )\n            if line_items:\n                page_edge_candidates.append(\n                    select_edge_texts_by_y(line_items, config.boilerplate_edge_lines)\n                )\n        repeated_clusters, repeat_threshold = detect_repeated_line_clusters(\n            page_edge_candidates,\n            total,\n            config,\n        )\n    for idx, image in enumerate(images, start=1):\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int((idx - 1) / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"Tesseract OCR page {idx}/{total} (running)\")\n        LOGGER.info(\"Tesseract OCR page %d/%d: start\", idx, total)\n        t_start = time.perf_counter()\n        text = \"\"\n        words: List[Dict[str, Any]] = []\n        try:\n            data = pytesseract.image_to_data(\n                image, lang=languages, output_type=pytesseract.Output.DICT\n            )\n            items = len(data.get(\"text\", []))\n            for i in range(items):\n                raw_text = str(data[\"text\"][i] or \"\").strip()\n                if not raw_text:\n                    continue\n                x0 = _safe_float(data.get(\"left\"), i)\n                y0 = _safe_float(data.get(\"top\"), i)\n                x1 = x0 + _safe_float(data.get(\"width\"), i)\n                y1 = y0 + _safe_float(data.get(\"height\"), i)\n                xc = 0.5 * (x0 + x1)\n                yc = 0.5 * (y0 + y1)\n                words.append(\n                    {\n                        \"x0\": x0,\n                        \"y0\": y0,\n                        \"x1\": x1,\n                        \"y1\": y1,\n                        \"xc\": xc,\n                        \"yc\": yc,\n                        \"text\": raw_text,\n                    }\n                )\n\n                conf_raw = data.get(\"conf\", [None])[i]\n                try:\n                    conf_val = float(conf_raw)\n                except Exception:\n                    conf_val = None\n                if conf_val is not None and conf_val >= 0:\n                    confidences.append(conf_val)\n\n            if words:\n                columns, _, _ = split_blocks_into_columns(words, log_label=\"Tesseract\")\n                column_lines: List[List[Dict[str, Any]]] = []\n                line_id_counter = 0\n                for col in columns:\n                    lines: List[Dict[str, Any]] = []\n                    for line_words in _group_words_into_lines(col):\n                        line_sorted = sorted(line_words, key=lambda w: w[\"x0\"])\n                        line_text = \" \".join(w[\"text\"] for w in line_sorted if w[\"text\"])\n                        if not line_text:\n                            continue\n                        line_y0 = min(w[\"y0\"] for w in line_sorted)\n                        line_y1 = max(w[\"y1\"] for w in line_sorted)\n                        line_x0 = min(w[\"x0\"] for w in line_sorted)\n                        lines.append(\n                            {\n                                \"text\": line_text,\n                                \"y0\": line_y0,\n                                \"y1\": line_y1,\n                                \"x0\": line_x0,\n                                \"line_id\": line_id_counter,\n                            }\n                        )\n                        line_id_counter += 1\n                    lines.sort(key=lambda l: (l[\"y0\"], l[\"x0\"]))\n                    column_lines.append(lines)\n                edge_ids: Set[int] = set()\n                if config.enable_boilerplate_removal and column_lines:\n                    all_lines = [line for col_lines in column_lines for line in col_lines]\n                    edge_ids = edge_ids_by_y(\n                        [(line[\"line_id\"], line[\"y0\"]) for line in all_lines],\n                        config.boilerplate_edge_lines,\n                    )\n                if edge_ids:\n                    filtered_columns: List[List[Dict[str, Any]]] = []\n                    for lines in column_lines:\n                        filtered_lines: List[Dict[str, Any]] = []\n                        for line in lines:\n                            normalized = normalize_boilerplate_line(str(line.get(\"text\", \"\")).strip())\n                            is_edge = line.get(\"line_id\") in edge_ids\n                            if is_edge and (\n                                matches_repeated_cluster(str(line.get(\"text\", \"\")), repeated_clusters, config)\n                                or is_boilerplate_line(normalized)\n                            ):\n                                removed_total += 1\n                                continue\n                            filtered_lines.append(line)\n                        filtered_columns.append(filtered_lines)\n                    column_lines = filtered_columns\n                def _join_lines(lines: List[Dict[str, Any]]) -> str:\n                    heights = [line[\"y1\"] - line[\"y0\"] for line in lines if line.get(\"y1\") is not None]\n                    heights = sorted(h for h in heights if h > 0)\n                    h_med = heights[len(heights) // 2] if heights else 10.0\n                    gap_thr = max(6.0, 1.6 * h_med)\n                    paragraphs: List[str] = []\n                    current = \"\"\n                    prev_y1: Optional[float] = None\n                    for line in lines:\n                        line_text = str(line.get(\"text\", \"\")).strip()\n                        if not line_text:\n                            continue\n                        y0 = float(line.get(\"y0\") or 0.0)\n                        y1 = float(line.get(\"y1\") or y0)\n                        if current and prev_y1 is not None and (y0 - prev_y1) > gap_thr:\n                            paragraphs.append(current.strip())\n                            current = \"\"\n                        if not current:\n                            current = line_text\n                        else:\n                            if current.endswith(\"-\"):\n                                current = current[:-1] + line_text.lstrip()\n                            else:\n                                current = current.rstrip() + \" \" + line_text.lstrip()\n                        prev_y1 = y1\n                    if current:\n                        paragraphs.append(current.strip())\n                    return \"\\n\\n\".join(paragraphs)\n\n                col_texts = [_join_lines(lines) for lines in column_lines if lines]\n                text = \"\\n\\n\".join(t for t in col_texts if t)\n        except Exception:\n            text = \"\"\n\n        if not text:\n            text = pytesseract.image_to_string(image, lang=languages)\n        pages.append({\"page_num\": idx, \"text\": text})\n        elapsed = time.perf_counter() - t_start\n        LOGGER.info(\n            \"Tesseract OCR page %d/%d: done in %.2fs (text_chars=%d, words=%d)\",\n            idx,\n            total,\n            elapsed,\n            len(text),\n            len(words),\n        )\n        if progress_cb and progress_span > 0:\n            percent = progress_base + int(idx / total * progress_span)\n            progress_cb(percent, \"ocr\", f\"Tesseract OCR page {idx}/{total}\")\n    if removed_total and config.enable_boilerplate_removal:\n        LOGGER.info(\n            \"Boilerplate removal (OCR lines): removed %s lines (repeat_threshold=%s, repeated_lines=%s)\",\n            removed_total,\n            repeat_threshold,\n            len(repeated_clusters),\n        )\n    avg_conf = sum(confidences) / len(confidences) if confidences else None\n    LOGGER.info(\n        \"Tesseract OCR complete: pages=%d, text_chars=%d\",\n        len(pages),\n        ocr_pages_text_chars(pages),\n    )\n    return pages, {\"ocr_confidence_avg\": avg_conf}\n",
  "index_redisearch.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nimport argparse\nimport html\nimport json\nimport os\nimport re\nimport sys\nfrom typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple\n\nfrom utils_embedding import normalize_vector, vector_to_bytes, request_embedding\nimport redis\nimport requests\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\nEMBED_MAX_CHARS = 12000\nEMBED_MAX_CHARS_NON_ASCII = 8000\nEMBED_SUBCHUNK_CHARS_DEFAULT = 3500\nEMBED_SUBCHUNK_OVERLAP_DEFAULT = 200\nEMBED_CONTEXT_WINDOW_DEFAULT = 0\nEMBED_CONTEXT_CHARS_DEFAULT = 220\n\n\ndef truncate_for_embedding(text: str) -> Tuple[str, bool]:\n    if not text:\n        return text, False\n    max_chars = EMBED_MAX_CHARS\n    non_ascii = sum(1 for ch in text if ord(ch) > 127)\n    if non_ascii / max(1, len(text)) > 0.2:\n        max_chars = EMBED_MAX_CHARS_NON_ASCII\n    if len(text) <= max_chars:\n        return text, False\n    sep = \"\\n...\\n\"\n    head_len = max(0, (max_chars - len(sep)) // 2)\n    tail_len = max_chars - len(sep) - head_len\n    trimmed = f\"{text[:head_len]}{sep}{text[-tail_len:]}\" if tail_len > 0 else text[:max_chars]\n    return trimmed, True\n\n\ndef _list_to_dict(items: Sequence[Any]) -> Dict[str, Any]:\n    data: Dict[str, Any] = {}\n    for i in range(0, len(items) - 1, 2):\n        key = items[i]\n        value = items[i + 1]\n        if isinstance(key, bytes):\n            key = key.decode(\"utf-8\", \"ignore\")\n        if isinstance(value, bytes):\n            value = value.decode(\"utf-8\", \"ignore\")\n        data[str(key)] = value\n    return data\n\n\ndef _iter_attributes(info_value: Any) -> Iterable[Dict[str, Any]]:\n    if not isinstance(info_value, list):\n        return []\n    for entry in info_value:\n        if isinstance(entry, list):\n            yield _list_to_dict(entry)\n\n\ndef get_index_vector_dim(\n    client: redis.Redis, index_name: str, field_name: str = \"embedding\"\n) -> Optional[int]:\n    try:\n        info = client.execute_command(\"FT.INFO\", index_name)\n    except Exception:\n        return None\n    info_dict = _list_to_dict(info if isinstance(info, list) else [])\n    attrs = info_dict.get(\"attributes\")\n    for attr in _iter_attributes(attrs):\n        attr_name = attr.get(\"attribute\") or attr.get(\"identifier\")\n        if attr_name != field_name:\n            continue\n        if str(attr.get(\"type\", \"\")).upper() != \"VECTOR\":\n            continue\n        dim_value = attr.get(\"dimension\") or attr.get(\"dim\")\n        try:\n            return int(dim_value)\n        except Exception:\n            return None\n    return None\n\n\ndef ensure_index(client: redis.Redis, index_name: str, prefix: str, embedding_dim: int) -> None:\n    try:\n        client.execute_command(\"FT.INFO\", index_name)\n        existing_dim = get_index_vector_dim(client, index_name)\n        if existing_dim and existing_dim != embedding_dim:\n            raise RuntimeError(\n                f\"Embedding dim mismatch: index={existing_dim} model={embedding_dim}\"\n            )\n        ensure_schema_fields(client, index_name)\n        return\n    except redis.exceptions.ResponseError as exc:\n        message = str(exc).lower()\n        if \"unknown index name\" not in message:\n            raise\n\n    client.execute_command(\n        \"FT.CREATE\",\n        index_name,\n        \"ON\",\n        \"HASH\",\n        \"PREFIX\",\n        \"1\",\n        prefix,\n        \"SCHEMA\",\n        \"doc_id\",\n        \"TAG\",\n        \"chunk_id\",\n        \"TAG\",\n        \"attachment_key\",\n        \"TAG\",\n        \"title\",\n        \"TEXT\",\n        \"authors\",\n        \"TAG\",\n        \"SEPARATOR\",\n        \"|\",\n        \"tags\",\n        \"TAG\",\n        \"SEPARATOR\",\n        \"|\",\n        \"chunk_tags\",\n        \"TAG\",\n        \"SEPARATOR\",\n        \"|\",\n        \"year\",\n        \"NUMERIC\",\n        \"item_type\",\n        \"TAG\",\n        \"SEPARATOR\",\n        \"|\",\n        \"source_pdf\",\n        \"TEXT\",\n        \"page_start\",\n        \"NUMERIC\",\n        \"page_end\",\n        \"NUMERIC\",\n        \"section\",\n        \"TEXT\",\n        \"text\",\n        \"TEXT\",\n        \"embedding\",\n        \"VECTOR\",\n        \"HNSW\",\n        \"6\",\n        \"TYPE\",\n        \"FLOAT32\",\n        \"DIM\",\n        str(embedding_dim),\n        \"DISTANCE_METRIC\",\n        \"COSINE\",\n    )\n\n\ndef ensure_schema_fields(client: redis.Redis, index_name: str) -> None:\n    fields: List[Tuple[str, List[str]]] = [\n        (\"attachment_key\", [\"TAG\"]),\n        (\"title\", [\"TEXT\"]),\n        (\"authors\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"tags\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"chunk_tags\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n        (\"year\", [\"NUMERIC\"]),\n        (\"item_type\", [\"TAG\", \"SEPARATOR\", \"|\"]),\n    ]\n    for name, spec in fields:\n        try:\n            client.execute_command(\"FT.ALTER\", index_name, \"SCHEMA\", \"ADD\", name, *spec)\n        except redis.exceptions.ResponseError as exc:\n            message = str(exc).lower()\n            if \"duplicate\" in message or \"already exists\" in message:\n                continue\n            raise\n\n\ndef infer_item_json_path(chunks_json: str, doc_id: str) -> Optional[str]:\n    base_name = f\"{doc_id}.json\"\n    chunks_dir = os.path.dirname(chunks_json)\n    candidates: List[str] = []\n    if os.path.basename(chunks_dir) == \"chunks\":\n        candidates.append(os.path.join(os.path.dirname(chunks_dir), \"items\", base_name))\n    marker = f\"{os.sep}chunks{os.sep}\"\n    if marker in chunks_json:\n        candidates.append(chunks_json.replace(marker, f\"{os.sep}items{os.sep}\"))\n    candidates.append(os.path.join(chunks_dir, base_name))\n    for candidate in candidates:\n        if os.path.isfile(candidate):\n            return candidate\n    return None\n\n\ndef parse_item_metadata(item_payload: Dict[str, Any]) -> Dict[str, Any]:\n    data = item_payload.get(\"data\") if isinstance(item_payload.get(\"data\"), dict) else item_payload\n    title = str(data.get(\"title\", \"\")).strip()\n    item_type = str(data.get(\"itemType\", \"\")).strip()\n    tags: List[str] = []\n    for tag in data.get(\"tags\", []) or []:\n        if isinstance(tag, dict):\n            value = str(tag.get(\"tag\", \"\")).strip()\n        else:\n            value = str(tag).strip()\n        if value:\n            tags.append(value)\n\n    creators = data.get(\"creators\", []) or []\n    authors: List[str] = []\n    for creator in creators:\n        if not isinstance(creator, dict):\n            continue\n        name = \"\"\n        if creator.get(\"name\"):\n            name = str(creator.get(\"name\", \"\")).strip()\n        else:\n            first = str(creator.get(\"firstName\", \"\")).strip()\n            last = str(creator.get(\"lastName\", \"\")).strip()\n            name = \" \".join(part for part in (first, last) if part)\n        if name:\n            authors.append(name)\n\n    year = 0\n    date_field = str(data.get(\"date\", \"\")).strip()\n    match = None\n    if date_field:\n        match = next(iter(__import__(\"re\").findall(r\"(1[5-9]\\d{2}|20\\d{2})\", date_field)), None)\n    if match:\n        try:\n            year = int(match)\n        except ValueError:\n            year = 0\n    elif isinstance(data.get(\"year\"), (int, float)):\n        year = int(data.get(\"year\"))\n\n    return {\n        \"title\": title,\n        \"authors\": \"|\".join(authors),\n        \"tags\": \"|\".join(tags),\n        \"year\": year,\n        \"item_type\": item_type,\n    }\n\n\ndef parse_chunk_id_list(raw: Optional[str], doc_id: str) -> List[str]:\n    if not raw:\n        return []\n    items: List[str] = []\n    for part in raw.split(\",\"):\n        cleaned = part.strip()\n        if not cleaned:\n            continue\n        if doc_id and cleaned.startswith(f\"{doc_id}:\"):\n            cleaned = cleaned.split(\":\", 1)[1]\n        items.append(cleaned)\n    return items\n\n\ndef delete_existing_chunk_keys(\n    client: redis.Redis,\n    prefix: str,\n    doc_id: str,\n    chunk_id: str,\n) -> int:\n    deleted = 0\n    base = f\"{prefix}{doc_id}:{chunk_id}\"\n    try:\n        if client.exists(base):\n            client.delete(base)\n            deleted += 1\n    except Exception:\n        pass\n    pattern = f\"{base}#*\"\n    batch: List[bytes] = []\n    for key in client.scan_iter(match=pattern, count=500):\n        batch.append(key)\n        if len(batch) >= 500:\n            client.delete(*batch)\n            deleted += len(batch)\n            batch = []\n    if batch:\n        client.delete(*batch)\n        deleted += len(batch)\n    return deleted\n\n\ndef markdown_to_text(text: str) -> str:\n    if not text:\n        return \"\"\n    text = strip_image_references(text)\n    try:\n        import markdown as md\n    except Exception:\n        return text\n    try:\n        html_text = md.markdown(text, extensions=[\"extra\", \"sane_lists\"])\n    except Exception:\n        return text\n    html_text = re.sub(r\"<br\\s*/?>\", \"\\n\", html_text, flags=re.IGNORECASE)\n    stripped = re.sub(r\"<[^>]+>\", \" \", html_text)\n    stripped = html.unescape(stripped)\n    stripped = re.sub(r\"[ \\t]+\", \" \", stripped)\n    stripped = re.sub(r\"\\s*\\n\\s*\", \"\\n\", stripped)\n    return stripped.strip()\n\n\n_OBSIDIAN_IMAGE_RE = re.compile(r\"!\\[\\[(?P<target>[^\\]|]+)(?:\\|(?P<label>[^\\]]+))?\\]\\]\")\n_MARKDOWN_IMAGE_RE = re.compile(r\"!\\[(?P<label>[^\\]]*)]\\([^)]+\\)\")\n_HTML_IMAGE_RE = re.compile(r\"<img[^>]*>\", re.IGNORECASE)\n\n\ndef strip_image_references(text: str) -> str:\n    if not text:\n        return \"\"\n    def _image_marker(label: str) -> str:\n        label = label.strip()\n        if label:\n            return f\" Image caption: {label} \"\n        return \" Image \"\n\n    def obsidian_repl(match: re.Match[str]) -> str:\n        label = (match.group(\"label\") or \"\").strip()\n        return _image_marker(label)\n\n    def markdown_repl(match: re.Match[str]) -> str:\n        label = (match.group(\"label\") or \"\").strip()\n        return _image_marker(label)\n\n    def html_repl(match: re.Match[str]) -> str:\n        tag = match.group(0)\n        alt_match = re.search(r\"\\balt=(['\\\"])(?P<alt>[^'\\\"]*)\\1\", tag, re.IGNORECASE)\n        if alt_match:\n            alt = (alt_match.group(\"alt\") or \"\").strip()\n            return _image_marker(alt)\n        return _image_marker(\"\")\n\n    text = _OBSIDIAN_IMAGE_RE.sub(obsidian_repl, text)\n    text = _MARKDOWN_IMAGE_RE.sub(markdown_repl, text)\n    text = _HTML_IMAGE_RE.sub(html_repl, text)\n    return text\n\n\ndef split_paragraphs(text: str) -> List[str]:\n    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n    return [para.strip() for para in paragraphs if para.strip()]\n\n\ndef split_long_text(text: str, max_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n    if len(sentences) <= 1:\n        return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n    for sentence in sentences:\n        sent = sentence.strip()\n        if not sent:\n            continue\n        if current_len + len(sent) + 1 > max_chars and current:\n            chunks.append(\" \".join(current).strip())\n            current = [sent]\n            current_len = len(sent)\n        else:\n            current.append(sent)\n            current_len += len(sent) + 1\n    if current:\n        chunks.append(\" \".join(current).strip())\n    return chunks\n\n\ndef split_text_by_size(text: str, max_chars: int, overlap_chars: int) -> List[str]:\n    if max_chars <= 0 or len(text) <= max_chars:\n        return [text]\n    paragraphs = split_paragraphs(text) or [text]\n    chunks: List[str] = []\n    current: List[str] = []\n    current_len = 0\n\n    def flush() -> None:\n        nonlocal current, current_len\n        if not current:\n            return\n        chunk = \"\\n\\n\".join(current).strip()\n        if chunk:\n            chunks.append(chunk)\n        current = []\n        current_len = 0\n\n    for para in paragraphs:\n        for piece in split_long_text(para, max_chars):\n            piece_len = len(piece)\n            if current_len + piece_len + 2 > max_chars and current:\n                flush()\n            current.append(piece)\n            current_len += piece_len + 2\n\n    flush()\n\n    if overlap_chars <= 0 or len(chunks) <= 1:\n        return chunks\n\n    overlapped: List[str] = []\n    previous = \"\"\n    for chunk in chunks:\n        if previous:\n            overlap = previous[-overlap_chars:]\n            combined = f\"{overlap}\\n{chunk}\".strip()\n        else:\n            combined = chunk\n        overlapped.append(combined)\n        previous = chunk\n    return overlapped\n\n\ndef split_for_embedding(text: str, max_chars: int, overlap_chars: int) -> List[str]:\n    if not text:\n        return []\n    max_chars = int(max_chars or 0)\n    overlap_chars = max(0, int(overlap_chars or 0))\n    if max_chars <= 0:\n        return [text]\n    chunks = split_text_by_size(text, max_chars, overlap_chars)\n    return chunks or [text]\n\n\ndef markdown_to_index_text(text: str) -> str:\n    if not text:\n        return \"\"\n    text = strip_image_references(text)\n    try:\n        from markdown_it import MarkdownIt\n    except Exception:\n        return markdown_to_text(text)\n\n    def inline_text(token: Any) -> str:\n        if not getattr(token, \"children\", None):\n            return str(getattr(token, \"content\", \"\") or \"\")\n        parts: List[str] = []\n        for child in token.children:\n            t = getattr(child, \"type\", \"\")\n            if t in (\"text\", \"code_inline\"):\n                parts.append(str(child.content or \"\"))\n            elif t == \"softbreak\":\n                parts.append(\" \")\n            elif t == \"hardbreak\":\n                parts.append(\"\\n\")\n        return \"\".join(parts)\n\n    def extract_table(tokens: Sequence[Any], start: int) -> Tuple[List[str], int]:\n        headers: List[str] = []\n        rows: List[List[str]] = []\n        current: List[str] = []\n        in_header = False\n        i = start + 1\n        while i < len(tokens):\n            token = tokens[i]\n            ttype = token.type\n            if ttype == \"thead_open\":\n                in_header = True\n            elif ttype == \"tbody_open\":\n                in_header = False\n            elif ttype == \"tr_open\":\n                current = []\n            elif ttype in (\"th_open\", \"td_open\"):\n                cell = \"\"\n                if i + 1 < len(tokens) and tokens[i + 1].type == \"inline\":\n                    cell = inline_text(tokens[i + 1]).strip()\n                current.append(cell)\n            elif ttype == \"tr_close\":\n                if in_header and not headers:\n                    headers = current\n                else:\n                    rows.append(current)\n            elif ttype == \"table_close\":\n                break\n            i += 1\n\n        lines: List[str] = []\n        for row in rows:\n            if headers:\n                pairs: List[str] = []\n                for idx, cell in enumerate(row):\n                    if not cell:\n                        continue\n                    header = headers[idx] if idx < len(headers) and headers[idx] else f\"Column {idx + 1}\"\n                    pairs.append(f\"{header}: {cell}\")\n                if pairs:\n                    lines.append(\"; \".join(pairs))\n            else:\n                row_line = \" | \".join([cell for cell in row if cell])\n                if row_line:\n                    lines.append(row_line)\n        return lines, i\n\n    md = MarkdownIt(\"commonmark\", {\"html\": False})\n    try:\n        md.enable(\"table\")\n    except Exception:\n        pass\n    tokens = md.parse(text)\n\n    lines: List[str] = []\n    list_depth = 0\n    in_list_item = False\n    list_item_parts: List[str] = []\n    i = 0\n    while i < len(tokens):\n        token = tokens[i]\n        ttype = token.type\n\n        if ttype == \"table_open\":\n            table_lines, i = extract_table(tokens, i)\n            lines.extend(table_lines)\n            i += 1\n            continue\n\n        if ttype in (\"bullet_list_open\", \"ordered_list_open\"):\n            list_depth += 1\n        elif ttype in (\"bullet_list_close\", \"ordered_list_close\"):\n            list_depth = max(0, list_depth - 1)\n        elif ttype == \"list_item_open\":\n            in_list_item = True\n            list_item_parts = []\n        elif ttype == \"list_item_close\":\n            content = \" \".join(list_item_parts).strip()\n            if content:\n                indent = \"  \" * max(0, list_depth - 1)\n                lines.append(f\"{indent}- {content}\")\n            in_list_item = False\n        elif ttype == \"heading_open\":\n            if i + 1 < len(tokens) and tokens[i + 1].type == \"inline\":\n                heading = inline_text(tokens[i + 1]).strip()\n                if heading:\n                    lines.append(heading)\n            while i < len(tokens) and tokens[i].type != \"heading_close\":\n                i += 1\n        elif ttype == \"inline\":\n            text_val = inline_text(token).strip()\n            if text_val:\n                if in_list_item:\n                    list_item_parts.append(text_val)\n                else:\n                    lines.append(text_val)\n        elif ttype in (\"fence\", \"code_block\"):\n            content = str(token.content or \"\").strip()\n            if content:\n                lines.append(content)\n\n        i += 1\n\n    return \"\\n\".join(lines).strip()\n\n\ndef normalize_index_text(text: str) -> str:\n    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    text = re.sub(r\"[ \\t]+\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    text = re.sub(r\"[ \\t]*\\n[ \\t]*\", \"\\n\", text)\n    return text.strip()\n\n\ndef normalize_meta_value(value: Any) -> str:\n    if value is None:\n        return \"\"\n    if isinstance(value, (list, tuple)):\n        cleaned = [str(item).strip() for item in value if str(item).strip()]\n        return \", \".join(cleaned)\n    text = str(value).strip()\n    if not text:\n        return \"\"\n    return text.replace(\"|\", \", \")\n\n\ndef is_chunk_excluded(chunk: Dict[str, Any]) -> bool:\n    value = chunk.get(\"excluded\")\n    if value is None:\n        value = chunk.get(\"exclude\")\n    if isinstance(value, bool):\n        return value\n    if isinstance(value, (int, float)):\n        return bool(value)\n    if isinstance(value, str):\n        return value.strip().lower() in {\"1\", \"true\", \"yes\", \"exclude\", \"excluded\"}\n    return False\n\n\ndef build_embedding_text(\n    text: str,\n    chunk: Dict[str, Any],\n    item_metadata: Dict[str, Any],\n) -> str:\n    parts: List[str] = []\n    title = normalize_meta_value(item_metadata.get(\"title\", \"\"))\n    if title:\n        parts.append(f\"Title: {title}\")\n    authors = normalize_meta_value(item_metadata.get(\"authors\", \"\"))\n    if authors:\n        parts.append(f\"Authors: {authors}\")\n    tags = normalize_meta_value(item_metadata.get(\"tags\", \"\"))\n    if tags:\n        parts.append(f\"Tags: {tags}\")\n    year = item_metadata.get(\"year\")\n    if isinstance(year, (int, float)) and int(year) > 0:\n        parts.append(f\"Year: {int(year)}\")\n    item_type = normalize_meta_value(item_metadata.get(\"item_type\", \"\"))\n    if item_type:\n        parts.append(f\"Item type: {item_type}\")\n    section = normalize_meta_value(chunk.get(\"section\", \"\"))\n    if section:\n        parts.append(f\"Section: {section}\")\n    chunk_tags = normalize_meta_value(chunk.get(\"chunk_tags\", \"\"))\n    if chunk_tags:\n        parts.append(f\"Chunk tags: {chunk_tags}\")\n    page_start = chunk.get(\"page_start\")\n    page_end = chunk.get(\"page_end\")\n    if isinstance(page_start, (int, float)) and isinstance(page_end, (int, float)):\n        parts.append(f\"Pages: {int(page_start)}-{int(page_end)}\")\n    if not parts:\n        return text\n    return \"\\n\".join(parts) + \"\\n\\n\" + text\n\n\ndef truncate_context_text(text: str, limit: int) -> str:\n    if limit <= 0:\n        return \"\"\n    cleaned = text.strip()\n    if not cleaned:\n        return \"\"\n    if len(cleaned) <= limit:\n        return cleaned\n    trimmed = cleaned[:limit]\n    last_space = trimmed.rfind(\" \")\n    if last_space > 0:\n        trimmed = trimmed[:last_space]\n    return trimmed.rstrip() + \"...\"\n\n\ndef build_context_text(\n    focus_text: str,\n    prev_snippets: Sequence[str],\n    next_snippets: Sequence[str],\n) -> str:\n    parts: List[str] = []\n    if prev_snippets:\n        parts.append(\"Previous context:\\n\" + \"\\n\".join(prev_snippets))\n    parts.append(focus_text)\n    if next_snippets:\n        parts.append(\"Next context:\\n\" + \"\\n\".join(next_snippets))\n    return \"\\n\\n\".join(parts)\n\n\ndef normalize_tag(tag: str) -> str:\n    cleaned = tag.strip()\n    cleaned = cleaned.strip(\"-,;:•\")\n    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n    return cleaned.strip()\n\n\ndef parse_tag_payload(content: str) -> List[str]:\n    if not content:\n        return []\n    raw = content.strip()\n    if raw.startswith(\"[\") and raw.endswith(\"]\"):\n        try:\n            parsed = json.loads(raw)\n            if isinstance(parsed, list):\n                return [normalize_tag(str(item)) for item in parsed if normalize_tag(str(item))]\n        except Exception:\n            pass\n    parts = re.split(r\"[,;\\n]+\", raw)\n    tags: List[str] = []\n    for part in parts:\n        cleaned = normalize_tag(part)\n        if cleaned:\n            tags.append(cleaned)\n    return tags\n\n\ndef request_chunk_tags(\n    base_url: str,\n    api_key: str,\n    model: str,\n    text: str,\n    max_tags: int,\n    temperature: float,\n) -> List[str]:\n    if not base_url or not model:\n        return []\n    snippet = text.strip()\n    if len(snippet) > 2000:\n        snippet = snippet[:2000]\n    if not snippet:\n        return []\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n    system_prompt = (\n        \"Return 3 to {max_tags} high-signal, concrete noun-phrase tags. \"\n        \"Avoid generic terms (study, paper, method), verbs, and filler. \"\n        \"Prefer specific entities, methods, datasets, and named concepts. \"\n        \"Output comma-separated tags only. No extra text.\"\n    ).format(max_tags=max_tags)\n    payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": snippet},\n        ],\n    }\n    model_name = (model or \"\").lower()\n    requires_default_temp = \"gpt-5\" in model_name or model_name.startswith(\"gpt5\")\n    if not requires_default_temp or temperature == 1.0:\n        payload[\"temperature\"] = temperature\n    response = requests.post(url, json=payload, headers=headers, timeout=60)\n    if response.status_code >= 400:\n        raise RuntimeError(f\"Tag request failed: {response.status_code} {response.text}\")\n    data = response.json()\n    content = (\n        data.get(\"choices\", [{}])[0]\n        .get(\"message\", {})\n        .get(\"content\", \"\")\n    )\n    tags = parse_tag_payload(str(content))\n    deduped: List[str] = []\n    seen = set()\n    for tag in tags:\n        key = tag.lower()\n        if key in seen:\n            continue\n        seen.add(key)\n        deduped.append(tag)\n        if len(deduped) >= max_tags:\n            break\n    return deduped\n\n\ndef tags_to_pipe(tags: Sequence[str]) -> str:\n    cleaned = [normalize_tag(tag) for tag in tags if normalize_tag(tag)]\n    return \"|\".join(cleaned)\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Index Docling chunks into RedisSearch.\")\n    parser.add_argument(\"--chunks-json\", required=True)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--item-json\")\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\n        \"--embed-subchunk-chars\",\n        type=int,\n        default=EMBED_SUBCHUNK_CHARS_DEFAULT,\n        help=\"Max chars per embedding subchunk (0 disables splitting).\",\n    )\n    parser.add_argument(\n        \"--embed-subchunk-overlap\",\n        type=int,\n        default=EMBED_SUBCHUNK_OVERLAP_DEFAULT,\n        help=\"Overlap chars between embedding subchunks.\",\n    )\n    parser.add_argument(\n        \"--embed-context-window\",\n        type=int,\n        default=EMBED_CONTEXT_WINDOW_DEFAULT,\n        help=\"Neighboring chunk count to include around each chunk in embeddings (0 disables).\",\n    )\n    parser.add_argument(\n        \"--embed-context-chars\",\n        type=int,\n        default=EMBED_CONTEXT_CHARS_DEFAULT,\n        help=\"Max chars per neighboring chunk included in embeddings.\",\n    )\n    parser.add_argument(\n        \"--embed-include-metadata\",\n        action=\"store_true\",\n        help=\"Include title/authors/tags/section metadata in the embedding text\",\n    )\n    parser.add_argument(\n        \"--generate-chunk-tags\",\n        action=\"store_true\",\n        help=\"Generate short tags per chunk using the LLM cleanup model\",\n    )\n    parser.add_argument(\"--tag-base-url\", default=\"\")\n    parser.add_argument(\"--tag-api-key\", default=\"\")\n    parser.add_argument(\"--tag-model\", default=\"\")\n    parser.add_argument(\"--tag-temperature\", type=float, default=0.2)\n    parser.add_argument(\"--tag-max\", type=int, default=5)\n    parser.add_argument(\"--upsert\", action=\"store_true\")\n    parser.add_argument(\"--progress\", action=\"store_true\")\n    parser.add_argument(\"--chunk-ids\", help=\"Comma-separated chunk IDs to index\")\n    parser.add_argument(\"--delete-chunk-ids\", help=\"Comma-separated chunk IDs to delete\")\n    args = parser.parse_args()\n\n    if not os.path.isfile(args.chunks_json):\n        eprint(f\"Chunks JSON not found: {args.chunks_json}\")\n        return 2\n\n    try:\n        with open(args.chunks_json, \"r\", encoding=\"utf-8\") as handle:\n            payload = json.load(handle)\n    except Exception as exc:\n        eprint(f\"Failed to read chunks JSON: {exc}\")\n        return 2\n\n\n    doc_id = payload.get(\"doc_id\")\n    chunks = payload.get(\"chunks\")\n    if not doc_id or not isinstance(chunks, list):\n        eprint(\"Invalid chunks JSON schema\")\n        return 2\n    doc_id = str(doc_id)\n\n    chunk_id_filter = set(parse_chunk_id_list(args.chunk_ids, doc_id))\n    delete_ids = set(parse_chunk_id_list(args.delete_chunk_ids, doc_id))\n    excluded_ids: Set[str] = set()\n    for chunk in chunks:\n        chunk_id = chunk.get(\"chunk_id\")\n        if not chunk_id:\n            continue\n        chunk_id = str(chunk_id)\n        if doc_id and chunk_id.startswith(f\"{doc_id}:\"):\n            chunk_id = chunk_id.split(\":\", 1)[1]\n        if is_chunk_excluded(chunk):\n            excluded_ids.add(chunk_id)\n    if excluded_ids:\n        delete_ids |= excluded_ids\n    incremental = bool(chunk_id_filter or delete_ids)\n\n    attachment_key = None\n    try:\n        meta = payload.get(\"metadata\") if isinstance(payload.get(\"metadata\"), dict) else {}\n        key_val = meta.get(\"attachment_key\") if isinstance(meta, dict) else None\n        if isinstance(key_val, str) and key_val.strip():\n            attachment_key = key_val.strip()\n    except Exception:\n        attachment_key = None\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n\n    if not incremental:\n        # Delete all existing chunk keys for this doc_id before indexing\n        pattern = f\"{args.prefix}{doc_id}:*\"\n        deleted = 0\n        try:\n            batch: List[bytes] = []\n            for key in client.scan_iter(match=pattern, count=500):\n                batch.append(key)\n                if len(batch) >= 500:\n                    client.delete(*batch)\n                    deleted += len(batch)\n                    batch = []\n            if batch:\n                client.delete(*batch)\n                deleted += len(batch)\n            if deleted:\n                eprint(f\"Deleted {deleted} existing chunk keys for doc_id {doc_id}\")\n        except Exception as exc:\n            eprint(f\"Failed to delete old chunk keys for doc_id {doc_id}: {exc}\")\n\n    item_metadata: Dict[str, Any] = {}\n    item_json_path = args.item_json or infer_item_json_path(args.chunks_json, str(doc_id))\n    if item_json_path and os.path.isfile(item_json_path):\n        try:\n            with open(item_json_path, \"r\", encoding=\"utf-8\") as handle:\n                item_payload = json.load(handle)\n            item_metadata = parse_item_metadata(item_payload)\n        except Exception as exc:\n            eprint(f\"Failed to read item JSON metadata: {exc}\")\n\n    if delete_ids or chunk_id_filter:\n        try:\n            to_clear = set(delete_ids) | set(chunk_id_filter)\n            deleted = 0\n            for chunk_id in to_clear:\n                deleted += delete_existing_chunk_keys(client, args.prefix, doc_id, chunk_id)\n            if deleted:\n                eprint(f\"Deleted {deleted} existing chunk keys for doc_id {doc_id}\")\n        except Exception as exc:\n            eprint(f\"Failed to delete chunk keys for doc_id {doc_id}: {exc}\")\n\n    embed_subchunk_chars = int(args.embed_subchunk_chars or 0)\n    embed_subchunk_overlap = max(0, int(args.embed_subchunk_overlap or 0))\n    context_window = max(0, int(args.embed_context_window or 0))\n    context_chars = max(0, int(args.embed_context_chars or 0))\n\n    prepared_chunks: List[Dict[str, Any]] = []\n    for chunk in chunks:\n        if is_chunk_excluded(chunk):\n            continue\n        raw_text = str(chunk.get(\"text\", \"\"))\n        text = normalize_index_text(markdown_to_index_text(raw_text))\n        if not text.strip():\n            continue\n        chunk_id = chunk.get(\"chunk_id\")\n        if not chunk_id:\n            continue\n        chunk_id = str(chunk_id)\n        if chunk_id_filter and chunk_id not in chunk_id_filter:\n            continue\n        if chunk_id in delete_ids:\n            continue\n        sub_texts = split_for_embedding(text, embed_subchunk_chars, embed_subchunk_overlap)\n        if not sub_texts:\n            sub_texts = [text]\n        prepared_chunks.append({\n            \"chunk\": chunk,\n            \"chunk_id\": chunk_id,\n            \"text\": text,\n            \"sub_texts\": sub_texts,\n        })\n\n    if not prepared_chunks:\n        return 0\n\n    if context_window > 0 and context_chars > 0:\n        for idx, entry in enumerate(prepared_chunks):\n            prev_snippets: List[str] = []\n            next_snippets: List[str] = []\n            for offset in range(1, context_window + 1):\n                prev_idx = idx - offset\n                if prev_idx < 0:\n                    break\n                snippet = truncate_context_text(prepared_chunks[prev_idx][\"text\"], context_chars)\n                if snippet:\n                    prev_snippets.append(snippet)\n            for offset in range(1, context_window + 1):\n                next_idx = idx + offset\n                if next_idx >= len(prepared_chunks):\n                    break\n                snippet = truncate_context_text(prepared_chunks[next_idx][\"text\"], context_chars)\n                if snippet:\n                    next_snippets.append(snippet)\n            entry[\"context_prev\"] = prev_snippets\n            entry[\"context_next\"] = next_snippets\n\n    first_chunk = prepared_chunks[0][\"chunk\"]\n    first_entry = prepared_chunks[0]\n    first_text = first_entry[\"sub_texts\"][0]\n    first_context_text = first_text\n    if context_window > 0 and context_chars > 0:\n        first_context_text = build_context_text(\n            first_text,\n            first_entry.get(\"context_prev\", []),\n            first_entry.get(\"context_next\", []),\n        )\n    first_embedding_text = (\n        build_embedding_text(first_context_text, first_chunk, item_metadata)\n        if args.embed_include_metadata\n        else first_context_text\n    )\n    first_len = len(first_embedding_text)\n    first_embedding_text, truncated = truncate_for_embedding(first_embedding_text)\n    if truncated:\n        eprint(\n            \"Embedding input truncated for chunk %s:%s (chars=%d->%d)\"\n            % (doc_id, first_chunk.get(\"chunk_id\"), first_len, len(first_embedding_text))\n        )\n    try:\n        sample_embedding = request_embedding(\n            args.embed_base_url,\n            args.embed_api_key,\n            args.embed_model,\n            first_embedding_text,\n        )\n    except Exception as exc:\n        eprint(f\"Embedding failed for chunk {doc_id}:{first_chunk.get('chunk_id')}: {exc}\")\n        return 2\n\n    embedding_dim = len(sample_embedding)\n    if embedding_dim <= 0:\n        eprint(\"Embedding dim mismatch: empty embedding returned\")\n        return 2\n\n    try:\n        ensure_index(client, args.index, args.prefix, embedding_dim)\n    except Exception as exc:\n        eprint(f\"Failed to ensure index: {exc}\")\n        return 2\n\n    sample_embedding = normalize_vector(sample_embedding)\n\n    total = sum(len(entry[\"sub_texts\"]) for entry in prepared_chunks)\n    current = 0\n    updated_chunks = False\n\n    for entry in prepared_chunks:\n        chunk = entry[\"chunk\"]\n        text = entry[\"text\"]\n        chunk_id = entry[\"chunk_id\"]\n        sub_texts = entry[\"sub_texts\"]\n        chunk_tags_value = \"\"\n        existing_tags = chunk.get(\"chunk_tags\")\n        has_existing_tags = False\n        if isinstance(existing_tags, (list, tuple)):\n            cleaned = [normalize_tag(str(tag)) for tag in existing_tags if normalize_tag(str(tag))]\n            if cleaned:\n                chunk_tags_value = tags_to_pipe(cleaned)\n                has_existing_tags = True\n        elif isinstance(existing_tags, str) and existing_tags.strip():\n            chunk_tags_value = existing_tags.strip()\n            has_existing_tags = True\n        if args.generate_chunk_tags and args.tag_base_url and args.tag_model and not has_existing_tags:\n            try:\n                tags = request_chunk_tags(\n                    args.tag_base_url,\n                    args.tag_api_key,\n                    args.tag_model,\n                    text,\n                    args.tag_max,\n                    args.tag_temperature,\n                )\n                if tags:\n                    chunk_tags_value = tags_to_pipe(tags)\n                    if chunk.get(\"chunk_tags\") != tags:\n                        chunk[\"chunk_tags\"] = tags\n                        updated_chunks = True\n            except Exception as exc:\n                eprint(f\"Tagging failed for chunk {chunk_id}: {exc}\")\n\n        stable_parent_id = f\"{doc_id}:{chunk_id}\"\n        sub_total = len(sub_texts)\n        for sub_idx, sub_text in enumerate(sub_texts, start=1):\n            current += 1\n            stable_chunk_id = (\n                stable_parent_id if sub_total <= 1 else f\"{stable_parent_id}#s{sub_idx}\"\n            )\n            key = f\"{args.prefix}{stable_chunk_id}\"\n\n            if not args.upsert and client.exists(key):\n                continue\n\n            try:\n                if args.progress:\n                    print(\n                        json.dumps(\n                            {\n                                \"type\": \"progress\",\n                                \"stage\": \"embedding\",\n                                \"current\": current,\n                                \"total\": total,\n                                \"message\": f\"Embedding {stable_chunk_id} ({current}/{total})\",\n                            }\n                        ),\n                        flush=True,\n                    )\n                if chunk is first_chunk and sub_idx == 1:\n                    embedding = sample_embedding\n                else:\n                    context_text = sub_text\n                    if context_window > 0 and context_chars > 0:\n                        context_text = build_context_text(\n                            sub_text,\n                            entry.get(\"context_prev\", []),\n                            entry.get(\"context_next\", []),\n                        )\n                    embedding_text = (\n                        build_embedding_text(context_text, chunk, item_metadata)\n                        if args.embed_include_metadata\n                        else context_text\n                    )\n                    embed_len = len(embedding_text)\n                    embedding_text, truncated = truncate_for_embedding(embedding_text)\n                    if truncated:\n                        eprint(\n                            \"Embedding input truncated for chunk %s (chars=%d->%d)\"\n                            % (stable_chunk_id, embed_len, len(embedding_text))\n                        )\n                    embedding = request_embedding(\n                        args.embed_base_url,\n                        args.embed_api_key,\n                        args.embed_model,\n                        embedding_text,\n                    )\n                    if len(embedding) != embedding_dim:\n                        raise RuntimeError(\n                            f\"Embedding dim mismatch: expected {embedding_dim} got {len(embedding)}\"\n                        )\n                    embedding = normalize_vector(embedding)\n            except Exception as exc:\n                eprint(f\"Embedding failed for chunk {stable_chunk_id}: {exc}\")\n                return 2\n\n            fields: Dict[str, Any] = {\n                \"doc_id\": str(doc_id),\n                \"chunk_id\": stable_parent_id,\n                \"attachment_key\": str(attachment_key or \"\"),\n                \"title\": str(item_metadata.get(\"title\", \"\")),\n                \"authors\": str(item_metadata.get(\"authors\", \"\")),\n                \"tags\": str(item_metadata.get(\"tags\", \"\")),\n                \"chunk_tags\": str(chunk_tags_value),\n                \"year\": int(item_metadata.get(\"year\", 0)),\n                \"item_type\": str(item_metadata.get(\"item_type\", \"\")),\n                \"source_pdf\": str(payload.get(\"source_pdf\", \"\")),\n                \"page_start\": int(chunk.get(\"page_start\", 0)),\n                \"page_end\": int(chunk.get(\"page_end\", 0)),\n                \"section\": str(chunk.get(\"section\", \"\")),\n                \"text\": sub_text,\n                \"embedding\": vector_to_bytes(embedding),\n            }\n\n            if sub_total > 1:\n                fields[\"chunk_sub_id\"] = stable_chunk_id\n\n            try:\n                client.hset(key, mapping=fields)\n            except Exception as exc:\n                eprint(f\"Failed to index chunk {stable_chunk_id}: {exc}\")\n                return 2\n\n            if args.progress:\n                print(\n                    json.dumps(\n                        {\n                            \"type\": \"progress\",\n                            \"stage\": \"index\",\n                            \"current\": current,\n                            \"total\": total,\n                            \"message\": f\"Indexing {stable_chunk_id} ({current}/{total})\",\n                        }\n                    ),\n                    flush=True,\n                )\n\n    if updated_chunks:\n        try:\n            with open(args.chunks_json, \"w\", encoding=\"utf-8\") as handle:\n                json.dump(payload, handle, indent=2)\n        except Exception as exc:\n            eprint(f\"Failed to write updated chunks JSON: {exc}\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "drop_redis_index.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nimport argparse\nimport sys\n\nimport redis\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Drop a RedisSearch index.\")\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--drop-docs\", action=\"store_true\", help=\"Drop indexed documents too (DD).\")\n    args = parser.parse_args()\n\n    try:\n        client = redis.Redis.from_url(args.redis_url, decode_responses=True)\n    except Exception as exc:\n        print(f\"Failed to connect to Redis: {exc}\", file=sys.stderr)\n        return 2\n\n    try:\n        if args.drop_docs:\n            client.execute_command(\"FT.DROPINDEX\", args.index, \"DD\")\n        else:\n            client.execute_command(\"FT.DROPINDEX\", args.index)\n    except Exception as exc:\n        message = str(exc)\n        if \"Unknown Index name\" in message or \"Unknown index name\" in message:\n            print(f\"Index {args.index} did not exist; continuing.\", file=sys.stderr)\n            return 0\n        print(f\"Failed to drop index {args.index}: {exc}\", file=sys.stderr)\n        return 2\n\n    print(f\"Dropped index {args.index}{' (DD)' if args.drop_docs else ''}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "ocr_layered_pdf.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nimport argparse\nimport io\nimport json\nimport os\nimport shutil\nimport sys\n\nfrom typing import Optional\n\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader, PdfWriter\nimport pytesseract\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef emit_progress(current: int, total: int) -> None:\n    print(json.dumps({\"type\": \"progress\", \"current\": current, \"total\": total}), flush=True)\n\n\ndef resolve_poppler_path(explicit: Optional[str]) -> Optional[str]:\n    if explicit:\n        return explicit\n    if shutil.which(\"pdfinfo\") or shutil.which(\"pdftoppm\"):\n        return None\n    for candidate in (\"/opt/homebrew/bin\", \"/usr/local/bin\"):\n        if os.path.isfile(os.path.join(candidate, \"pdfinfo\")) or os.path.isfile(\n            os.path.join(candidate, \"pdftoppm\")\n        ):\n            return candidate\n    return None\n\n\ndef resolve_tesseract_path(explicit: Optional[str]) -> Optional[str]:\n    if explicit:\n        return explicit\n    found = shutil.which(\"tesseract\")\n    if found:\n        return found\n    for candidate in (\"/opt/homebrew/bin/tesseract\", \"/usr/local/bin/tesseract\", \"/usr/bin/tesseract\"):\n        if os.path.isfile(candidate):\n            return candidate\n    return None\n\n\ndef get_page_count(pdf_path: str) -> int:\n    try:\n        reader = PdfReader(pdf_path)\n        return len(reader.pages)\n    except Exception:\n        return 0\n\n\ndef ocr_page_to_pdf(image, language: str) -> Optional[bytes]:\n    try:\n        return pytesseract.image_to_pdf_or_hocr(image, extension=\"pdf\", lang=language)\n    except Exception as exc:\n        eprint(f\"Tesseract OCR failed: {exc}\")\n        return None\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Create a PDF with an OCR text layer via Tesseract.\")\n    parser.add_argument(\"--pdf\", required=True, help=\"Input PDF path\")\n    parser.add_argument(\"--out-pdf\", required=True, help=\"Output PDF path\")\n    parser.add_argument(\"--language\", default=\"eng\", help=\"Tesseract language code, e.g. eng, deu+eng\")\n    parser.add_argument(\"--dpi\", type=int, default=300, help=\"Rasterization DPI\")\n    parser.add_argument(\"--poppler-path\", help=\"Optional poppler bin path (pdfinfo/pdftoppm)\")\n    parser.add_argument(\"--tesseract-path\", help=\"Optional tesseract binary path\")\n    parser.add_argument(\"--progress\", action=\"store_true\", help=\"Emit JSON progress events\")\n    args = parser.parse_args()\n\n    if not os.path.isfile(args.pdf):\n        eprint(f\"PDF not found: {args.pdf}\")\n        return 2\n\n    out_dir = os.path.dirname(args.out_pdf)\n    if out_dir:\n        os.makedirs(out_dir, exist_ok=True)\n\n    poppler_path = resolve_poppler_path(args.poppler_path)\n    if poppler_path:\n        eprint(f\"Poppler not on PATH; using {poppler_path}\")\n    tesseract_path = resolve_tesseract_path(args.tesseract_path)\n    if tesseract_path:\n        pytesseract.pytesseract.tesseract_cmd = tesseract_path\n        if not shutil.which(\"tesseract\"):\n            eprint(f\"Tesseract not on PATH; using {tesseract_path}\")\n\n    total_pages = get_page_count(args.pdf)\n    if total_pages <= 0:\n        try:\n            images = convert_from_path(args.pdf, dpi=args.dpi, poppler_path=poppler_path)\n        except Exception as exc:\n            eprint(f\"Failed to rasterize PDF: {exc}\")\n            return 2\n        total_pages = len(images)\n        images_by_index = {idx + 1: img for idx, img in enumerate(images)}\n    else:\n        images_by_index = {}\n\n    if total_pages == 0:\n        eprint(\"No pages detected in PDF.\")\n        return 2\n\n    writer = PdfWriter()\n    language = (args.language or \"eng\").strip() or \"eng\"\n\n    for page_idx in range(1, total_pages + 1):\n        if page_idx in images_by_index:\n            image = images_by_index[page_idx]\n        else:\n            try:\n                images = convert_from_path(\n                    args.pdf,\n                    dpi=args.dpi,\n                    first_page=page_idx,\n                    last_page=page_idx,\n                    poppler_path=poppler_path,\n                )\n            except Exception as exc:\n                eprint(f\"Failed to rasterize page {page_idx}: {exc}\")\n                return 2\n            if not images:\n                continue\n            image = images[0]\n\n        pdf_bytes = ocr_page_to_pdf(image, language)\n        if not pdf_bytes:\n            return 2\n        try:\n            reader = PdfReader(io.BytesIO(pdf_bytes))\n            if reader.pages:\n                writer.add_page(reader.pages[0])\n        except Exception as exc:\n            eprint(f\"Failed to parse OCR page {page_idx}: {exc}\")\n            return 2\n\n        if args.progress:\n            emit_progress(page_idx, total_pages)\n\n    try:\n        with open(args.out_pdf, \"wb\") as handle:\n            writer.write(handle)\n    except Exception as exc:\n        eprint(f\"Failed to write output PDF: {exc}\")\n        return 2\n\n    if args.progress:\n        print(json.dumps({\"type\": \"final\", \"output_pdf\": args.out_pdf, \"pages\": total_pages}), flush=True)\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "rag_query_redisearch.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\n\nimport argparse\nimport json\nimport math\nfrom utils_embedding import normalize_vector, vector_to_bytes, request_embedding\nimport re\nimport struct\nimport sys\nfrom typing import Any, Dict, List, Optional, Sequence, Set, Tuple\n\nimport redis\nimport requests\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\ndef is_temperature_unsupported(message: str) -> bool:\n    lowered = message.lower()\n    return \"temperature\" in lowered and (\n        \"not supported\" in lowered or \"unsupported\" in lowered or \"unknown parameter\" in lowered\n    )\n\n\ndef is_stream_unsupported(message: str) -> bool:\n    lowered = message.lower()\n    return \"stream\" in lowered and (\"not supported\" in lowered or \"unsupported\" in lowered or \"unknown parameter\" in lowered)\n\n\ndef request_chat(\n    base_url: str,\n    api_key: str,\n    model: str,\n    temperature: float,\n    system_prompt: str,\n    user_prompt: str,\n) -> str:\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    base_payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n    }\n    payload = dict(base_payload)\n    payload[\"temperature\"] = temperature\n\n    response = requests.post(url, json=payload, headers=headers, timeout=120)\n    response.encoding = \"utf-8\"\n    if response.status_code >= 400:\n        error_text = response.text\n        if is_temperature_unsupported(error_text):\n            response = requests.post(url, json=base_payload, headers=headers, timeout=120)\n            response.encoding = \"utf-8\"\n            if response.status_code >= 400:\n                raise RuntimeError(f\"Chat request failed: {response.status_code} {response.text}\")\n        else:\n            raise RuntimeError(f\"Chat request failed: {response.status_code} {error_text}\")\n\n    data = response.json()\n    choices = data.get(\"choices\")\n    if not choices:\n        raise RuntimeError(\"Chat response missing choices\")\n    message = choices[0].get(\"message\") or {}\n    content = message.get(\"content\")\n    if not content:\n        raise RuntimeError(\"Chat response missing content\")\n    return content\n\n\ndef request_chat_stream(\n    base_url: str,\n    api_key: str,\n    model: str,\n    temperature: float,\n    system_prompt: str,\n    user_prompt: str,\n    on_delta,\n) -> str:\n    url = base_url.rstrip(\"/\") + \"/chat/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    base_payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_prompt},\n        ],\n        \"stream\": True,\n    }\n    payload = dict(base_payload)\n    payload[\"temperature\"] = temperature\n\n    response = requests.post(url, json=payload, headers=headers, timeout=120, stream=True)\n    response.encoding = \"utf-8\"\n    if response.status_code >= 400:\n        error_text = response.text\n        if is_temperature_unsupported(error_text):\n            response = requests.post(url, json=base_payload, headers=headers, timeout=120, stream=True)\n            response.encoding = \"utf-8\"\n            if response.status_code >= 400:\n                raise RuntimeError(f\"Chat request failed: {response.status_code} {response.text}\")\n        else:\n            raise RuntimeError(f\"Chat request failed: {response.status_code} {error_text}\")\n\n    content_parts: List[str] = []\n    for raw_line in response.iter_lines(decode_unicode=True):\n        if not raw_line:\n            continue\n        line = raw_line.strip()\n        if not line.startswith(\"data:\"):\n            continue\n        data = line[5:].strip()\n        if data == \"[DONE]\":\n            break\n        try:\n            payload = json.loads(data)\n        except Exception:\n            continue\n        choices = payload.get(\"choices\") or []\n        if not choices:\n            continue\n        delta = choices[0].get(\"delta\") or {}\n        piece = delta.get(\"content\")\n        if not piece:\n            continue\n        content_parts.append(piece)\n        on_delta(piece)\n\n    return \"\".join(content_parts)\n\n\ndef parse_json_list(raw: str) -> List[str]:\n    if not raw:\n        return []\n    text = raw.strip()\n    try:\n        data = json.loads(text)\n    except Exception:\n        data = None\n    if isinstance(data, dict):\n        for key in (\"queries\", \"expanded\", \"expansions\", \"items\"):\n            if isinstance(data.get(key), list):\n                data = data.get(key)\n                break\n    if isinstance(data, list):\n        cleaned: List[str] = []\n        for item in data:\n            if isinstance(item, str):\n                value = item.strip()\n                if value:\n                    cleaned.append(value)\n        return cleaned\n    # Fallback: split lines or bullets\n    lines = [line.strip(\" -\\t\") for line in text.splitlines()]\n    return [line for line in lines if line]\n\n\ndef expand_query(\n    base_url: str,\n    api_key: str,\n    model: str,\n    query: str,\n    count: int,\n) -> List[str]:\n    if not base_url or not model or not query or count <= 0:\n        return []\n    system_prompt = (\n        \"You expand search queries for retrieval. \"\n        \"Return only a JSON array of strings with concise alternative queries. \"\n        \"Do not include the original query.\"\n    )\n    user_prompt = (\n        f\"Original query: {query}\\n\"\n        f\"Return {count} expanded queries as a JSON array of strings.\"\n    )\n    try:\n        response = request_chat(base_url, api_key, model, 0.0, system_prompt, user_prompt)\n        expanded = parse_json_list(response)\n    except Exception as exc:\n        eprint(f\"Query expansion failed: {exc}\")\n        return []\n    cleaned: List[str] = []\n    seen: Set[str] = set()\n    for item in expanded:\n        value = item.strip()\n        if not value:\n            continue\n        key = value.lower()\n        if key in seen or key == query.lower():\n            continue\n        seen.add(key)\n        cleaned.append(value)\n        if len(cleaned) >= count:\n            break\n    return cleaned\n\n\ndef load_reranker(model_name: str):\n    try:\n        from sentence_transformers import CrossEncoder  # type: ignore\n    except Exception as exc:\n        eprint(f\"Reranker unavailable (sentence-transformers not installed): {exc}\")\n        return None\n    try:\n        return CrossEncoder(model_name)\n    except Exception as exc:\n        eprint(f\"Failed to load reranker model '{model_name}': {exc}\")\n        return None\n\n\ndef truncate_rerank_text(text: str, max_chars: int) -> str:\n    if max_chars <= 0:\n        return text\n    cleaned = text.strip()\n    if len(cleaned) <= max_chars:\n        return cleaned\n    trimmed = cleaned[:max_chars]\n    last_space = trimmed.rfind(\" \")\n    if last_space > 0:\n        trimmed = trimmed[:last_space]\n    return trimmed.rstrip() + \"...\"\n\n\ndef rerank_candidates(\n    reranker,\n    query: str,\n    candidates: List[Dict[str, Any]],\n    max_chars: int,\n) -> List[Dict[str, Any]]:\n    if reranker is None:\n        return candidates\n    pairs: List[List[str]] = []\n    items: List[Dict[str, Any]] = []\n    for item in candidates:\n        text = str(item.get(\"text\", \"\") or \"\").strip()\n        if not text:\n            continue\n        trimmed = truncate_rerank_text(text, max_chars)\n        pairs.append([query, trimmed])\n        items.append(item)\n    if not pairs:\n        return candidates\n    try:\n        scores = reranker.predict(pairs)\n    except Exception as exc:\n        eprint(f\"Reranking failed: {exc}\")\n        return candidates\n    scored: List[Tuple[float, int, Dict[str, Any]]] = []\n    for idx, item in enumerate(items):\n        try:\n            score = float(scores[idx])\n        except Exception:\n            score = 0.0\n        item[\"rerank_score\"] = score\n        scored.append((score, idx, item))\n    scored.sort(key=lambda row: (-row[0], row[1]))\n    return [row[2] for row in scored]\n\n\ndef decode_value(value: Any) -> Any:\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\", errors=\"ignore\")\n    return value\n\n\ndef parse_results(raw: List[Any]) -> List[Dict[str, Any]]:\n    results: List[Dict[str, Any]] = []\n    if not raw or len(raw) < 2:\n        return results\n\n    for idx in range(1, len(raw), 2):\n        if idx + 1 >= len(raw):\n            break\n        fields_raw = raw[idx + 1]\n        if not isinstance(fields_raw, list):\n            continue\n        field_map: Dict[str, Any] = {}\n        for i in range(0, len(fields_raw), 2):\n            key = decode_value(fields_raw[i])\n            value = decode_value(fields_raw[i + 1]) if i + 1 < len(fields_raw) else \"\"\n            field_map[str(key)] = value\n        results.append(field_map)\n    return results\n\n\nFIELD_TYPE_CACHE: Dict[str, Dict[str, str]] = {}\n\n\ndef parse_info_map(info: Any) -> Dict[str, Any]:\n    if not isinstance(info, (list, tuple)):\n        return {}\n    it = iter(info)\n    result: Dict[str, Any] = {}\n    for key in it:\n        value = next(it, None)\n        result[str(decode_value(key))] = value\n    return result\n\n\ndef get_field_types(client: redis.Redis, index: str) -> Dict[str, str]:\n    if index in FIELD_TYPE_CACHE:\n        return FIELD_TYPE_CACHE[index]\n    try:\n        info = client.execute_command(\"FT.INFO\", index)\n    except Exception:\n        return {}\n    info_map = parse_info_map(info)\n    attributes = info_map.get(\"attributes\") or info_map.get(\"fields\") or []\n    field_types: Dict[str, str] = {}\n    if isinstance(attributes, (list, tuple)):\n        for attr in attributes:\n            if not isinstance(attr, (list, tuple)):\n                continue\n            attr_map: Dict[str, Any] = {}\n            for i in range(0, len(attr) - 1, 2):\n                attr_map[str(decode_value(attr[i]))] = decode_value(attr[i + 1])\n            name = attr_map.get(\"identifier\") or attr_map.get(\"attribute\") or attr_map.get(\"name\")\n            ftype = attr_map.get(\"type\")\n            if name and ftype:\n                field_types[str(name)] = str(ftype).upper()\n    FIELD_TYPE_CACHE[index] = field_types\n    return field_types\n\n\ndef get_index_vector_dim(\n    client: redis.Redis, index_name: str, field_name: str = \"embedding\"\n) -> Optional[int]:\n    try:\n        info = client.execute_command(\"FT.INFO\", index_name)\n    except Exception:\n        return None\n    info_map = parse_info_map(info)\n    attributes = info_map.get(\"attributes\") or info_map.get(\"fields\") or []\n    if not isinstance(attributes, (list, tuple)):\n        return None\n    for attr in attributes:\n        if not isinstance(attr, (list, tuple)):\n            continue\n        attr_map: Dict[str, Any] = {}\n        for i in range(0, len(attr) - 1, 2):\n            attr_map[str(decode_value(attr[i]))] = decode_value(attr[i + 1])\n        name = attr_map.get(\"attribute\") or attr_map.get(\"identifier\") or attr_map.get(\"name\")\n        if name != field_name:\n            continue\n        if str(attr_map.get(\"type\", \"\")).upper() != \"VECTOR\":\n            continue\n        dim_value = attr_map.get(\"dimension\") or attr_map.get(\"dim\")\n        try:\n            return int(dim_value)\n        except Exception:\n            return None\n    return None\n\n\n_QUERY_STOPWORDS = {\n    \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"into\", \"over\",\n    \"under\", \"after\", \"before\", \"were\", \"was\", \"are\", \"is\", \"its\", \"their\",\n    \"then\", \"than\", \"which\", \"when\", \"where\", \"have\", \"has\", \"had\", \"onto\",\n    \"upon\", \"your\", \"yours\", \"they\", \"them\", \"these\", \"those\", \"will\", \"would\",\n    \"could\", \"should\", \"about\", \"there\", \"here\", \"while\", \"what\", \"why\", \"how\",\n    \"not\", \"but\", \"you\", \"your\", \"our\", \"ours\", \"his\", \"her\", \"she\", \"him\",\n    \"also\", \"such\", \"been\", \"being\", \"out\", \"one\", \"two\", \"three\", \"four\",\n    \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"more\", \"most\", \"some\",\n    \"many\", \"few\", \"each\", \"per\", \"was\", \"were\", \"did\", \"does\", \"do\",\n}\n\n\ndef extract_keywords(query: str) -> List[str]:\n    raw_tokens = re.findall(r\"[\\\\w'\\\\-\\u2011]{2,}\", query, flags=re.UNICODE)\n    keywords: List[str] = []\n    def add_keyword(token: str, raw: str) -> None:\n        if not token:\n            return\n        lower = token.lower()\n        if lower in _QUERY_STOPWORDS:\n            return\n        keywords.append(lower)\n        raw_lower = raw.lower()\n        if raw_lower.endswith((\"'s\", \"\\u2019s\")) and len(lower) > 3:\n            stem = lower[:-1]\n            if stem and stem not in _QUERY_STOPWORDS:\n                keywords.append(stem)\n\n    for token in raw_tokens:\n        cleaned = \"\".join(ch for ch in token if ch.isalnum())\n        if not cleaned:\n            continue\n        if token[:1].isupper() or len(cleaned) >= 5:\n            add_keyword(cleaned, token)\n        if \"-\" in token or \"\\u2011\" in token:\n            for part in re.split(r\"[-\\u2011]+\", token):\n                part_clean = \"\".join(ch for ch in part if ch.isalnum())\n                if not part_clean:\n                    continue\n                if part[:1].isupper() or len(part_clean) >= 4:\n                    add_keyword(part_clean, part)\n    seen = set()\n    ordered: List[str] = []\n    for token in keywords:\n        if token in seen:\n            continue\n        seen.add(token)\n        ordered.append(token)\n    return ordered\n\n\ndef normalize_tag_token(tag: str) -> str:\n    cleaned = tag.strip().lower()\n    cleaned = cleaned.strip(\"-_,;:•\")\n    cleaned = re.sub(r\"\\s+\", \" \", cleaned)\n    return cleaned.strip()\n\n\ndef parse_tag_field(value: Any) -> List[str]:\n    if value is None:\n        return []\n    if isinstance(value, (list, tuple, set)):\n        parts = [str(item) for item in value]\n    else:\n        parts = re.split(r\"[|,;]\", str(value))\n    cleaned: List[str] = []\n    for part in parts:\n        token = normalize_tag_token(str(part))\n        if token:\n            cleaned.append(token)\n    return cleaned\n\n\ndef tag_tokens_from_tags(tags: Sequence[str]) -> Set[str]:\n    tokens: Set[str] = set()\n    for tag in tags:\n        cleaned = normalize_tag_token(tag)\n        if not cleaned:\n            continue\n        tokens.add(cleaned)\n        tokens.update(re.findall(r\"[A-Za-z0-9]+\", cleaned))\n    return tokens\n\n\ndef apply_tag_boosting(\n    results: List[Dict[str, Any]],\n    keywords: Sequence[str],\n) -> List[Dict[str, Any]]:\n    if not results or not keywords:\n        return results\n    keyword_set = {token.lower() for token in keywords if token}\n    if not keyword_set:\n        return results\n\n    scored: List[Tuple[int, int, Dict[str, Any]]] = []\n    max_score = 0\n    for idx, chunk in enumerate(results):\n        chunk_tags = parse_tag_field(chunk.get(\"chunk_tags\", \"\"))\n        item_tags = parse_tag_field(chunk.get(\"tags\", \"\"))\n        chunk_tokens = tag_tokens_from_tags(chunk_tags)\n        item_tokens = tag_tokens_from_tags(item_tags)\n        chunk_hits = len(keyword_set & chunk_tokens)\n        item_hits = len(keyword_set & item_tokens)\n        score = (chunk_hits * 2) + item_hits\n        max_score = max(max_score, score)\n        scored.append((score, idx, chunk))\n\n    if max_score <= 0:\n        return results\n    scored.sort(key=lambda item: (-item[0], item[1]))\n    return [item[2] for item in scored]\n\n\ndef search_redis_knn(\n    client: redis.Redis,\n    index: str,\n    vec: bytes,\n    k: int,\n) -> List[Dict[str, Any]]:\n    raw = client.execute_command(\n        \"FT.SEARCH\",\n        index,\n        f\"*=>[KNN {k} @embedding $vec AS score]\",\n        \"PARAMS\",\n        \"2\",\n        \"vec\",\n        vec,\n        \"SORTBY\",\n        \"score\",\n        \"RETURN\",\n        \"11\",\n        \"doc_id\",\n        \"chunk_id\",\n        \"attachment_key\",\n        \"source_pdf\",\n        \"page_start\",\n        \"page_end\",\n        \"section\",\n        \"text\",\n        \"tags\",\n        \"chunk_tags\",\n        \"score\",\n        \"DIALECT\",\n        \"2\",\n    )\n    return parse_results(raw)\n\n\ndef chunk_key(item: Dict[str, Any]) -> str:\n    value = item.get(\"chunk_id\")\n    if value is None:\n        return \"\"\n    return str(value)\n\n\ndef dedupe_by_chunk_id(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    seen: Set[str] = set()\n    deduped: List[Dict[str, Any]] = []\n    for item in items:\n        key = chunk_key(item)\n        if not key or key in seen:\n            continue\n        seen.add(key)\n        deduped.append(item)\n    return deduped\n\n\n_MIN_CONTEXT_CHUNKS = 3\n_MIN_CONTEXT_CHARS = 1500\n_MAX_ACCEPTABLE_SCORE = 0.4\n_MIN_NARRATIVE_RATIO = 0.5\n_MIN_CONTENT_FOR_RATIO = 4\n_RERANK_MAX_CHARS_DEFAULT = 2000\n_RRF_K = 60\n\n\ndef retrieve_chunks(\n    client: redis.Redis,\n    index: str,\n    vec: bytes,\n    k: int,\n    keywords: List[str],\n    strict: bool = True,\n    rrf_k: int = _RRF_K,\n    rrf_log_top: int = 0,\n    max_per_doc: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    vector_results = search_redis_knn(client, index, vec, k)\n    retrieved = vector_results\n\n    lexical_limit = max(k, 5)\n    lexical_results = run_lexical_search(client, index, keywords, lexical_limit)\n    lexical_ids: Set[str] = set()\n    if lexical_results:\n        for item in lexical_results:\n            key = chunk_key(item)\n            if key:\n                lexical_ids.add(key)\n\n        max_total = k + lexical_limit\n        combined = lexical_results + retrieved\n        if len(combined) > max_total:\n            combined = combined[:max_total]\n        retrieved = dedupe_by_chunk_id(combined)\n    else:\n        retrieved = dedupe_by_chunk_id(retrieved)\n\n    if strict:\n        filtered = [\n            c for c in retrieved\n            if is_content_chunk(c) and looks_narrative(c.get(\"text\", \"\"))\n        ]\n        if not filtered:\n            filtered = [c for c in retrieved if is_content_chunk(c)]\n    else:\n        filtered = [c for c in retrieved if is_content_chunk(c)]\n        if not filtered:\n            filtered = retrieved\n\n    if lexical_ids:\n        seen_ids = {chunk_key(item) for item in filtered if chunk_key(item)}\n        for item in lexical_results:\n            key = chunk_key(item)\n            if not key:\n                continue\n            if key in seen_ids:\n                continue\n            text = str(item.get(\"text\", \"\") or \"\").strip()\n            if not text:\n                continue\n            filtered.append(item)\n            seen_ids.add(key)\n\n    metrics = compute_retrieval_metrics(retrieved, filtered)\n    rrf_scores = build_rrf_scores(vector_results, lexical_results, rrf_k=rrf_k)\n    ordered = order_by_rrf(filtered, rrf_scores)\n    if rrf_log_top > 0:\n        log_rrf_top(ordered, rrf_scores, rrf_log_top)\n    ordered = apply_tag_boosting(ordered, keywords)\n    ordered = apply_doc_cap(ordered, max_per_doc)\n    return ordered, metrics\n\n\ndef run_lexical_search(\n    client: redis.Redis,\n    index: str,\n    keywords: List[str],\n    limit: int,\n) -> List[Dict[str, Any]]:\n    if not keywords or limit <= 0:\n        return []\n    tokens = [\"\".join(ch for ch in token if ch.isalnum()) for token in keywords]\n    tokens = [token for token in tokens if token]\n    if not tokens:\n        return []\n    text_terms = \"|\".join(tokens)\n    tag_terms = \"|\".join(tokens)\n    field_types = get_field_types(client, index)\n\n    def should_include(name: str, required: bool = False) -> bool:\n        if field_types:\n            return required or name in field_types\n        return required\n\n    def field_is_tag(name: str) -> bool:\n        return field_types.get(name, \"\").upper() == \"TAG\"\n\n    def format_term(name: str) -> str:\n        field = f\"@{name}\"\n        if field_is_tag(name):\n            return f\"{field}:{{{tag_terms}}}\"\n        return f\"{field}:({text_terms})\"\n\n    parts: List[Tuple[str, str]] = []\n    if should_include(\"text\", required=True):\n        parts.append((\"text\", format_term(\"text\")))\n    if should_include(\"title\"):\n        parts.append((\"title\", format_term(\"title\")))\n    if should_include(\"authors\"):\n        parts.append((\"authors\", format_term(\"authors\")))\n    if should_include(\"tags\"):\n        parts.append((\"tags\", format_term(\"tags\")))\n    if should_include(\"chunk_tags\"):\n        parts.append((\"chunk_tags\", format_term(\"chunk_tags\")))\n    if should_include(\"doc_id\"):\n        parts.append((\"doc_id\", format_term(\"doc_id\")))\n    if not parts:\n        return []\n    query = \"(\" + \" OR \".join(clause for _name, clause in parts) + \")\"\n\n    def run_search(query_text: str) -> Tuple[List[Dict[str, Any]], int]:\n        raw = client.execute_command(\n            \"FT.SEARCH\",\n            index,\n            query_text,\n            \"LIMIT\",\n            \"0\",\n            str(limit),\n            \"RETURN\",\n            \"11\",\n            \"doc_id\",\n            \"chunk_id\",\n            \"attachment_key\",\n            \"source_pdf\",\n            \"page_start\",\n            \"page_end\",\n            \"section\",\n            \"text\",\n            \"tags\",\n            \"chunk_tags\",\n            \"score\",\n            \"DIALECT\",\n            \"2\",\n        )\n        total = 0\n        if isinstance(raw, list) and raw:\n            try:\n                total = int(raw[0])\n            except Exception:\n                total = 0\n        return parse_results(raw), total\n\n    def dedupe_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        seen: Set[str] = set()\n        merged: List[Dict[str, Any]] = []\n        for item in results:\n            chunk_id = item.get(\"chunk_id\")\n            if not chunk_id:\n                continue\n            cid = str(chunk_id)\n            if cid in seen:\n                continue\n            seen.add(cid)\n            merged.append(item)\n            if limit > 0 and len(merged) >= limit:\n                break\n        return merged\n\n    try:\n        results, total = run_search(query)\n        if total == 0:\n            fallback_results: List[Dict[str, Any]] = []\n            for _name, clause in parts:\n                try:\n                    field_results, _ = run_search(clause)\n                    fallback_results.extend(field_results)\n                except Exception:\n                    continue\n            merged = dedupe_results(fallback_results)\n            if merged:\n                return merged\n        return results\n    except Exception:\n        fallback_results = []\n        for _name, clause in parts:\n            try:\n                field_results, _ = run_search(clause)\n                fallback_results.extend(field_results)\n            except Exception:\n                continue\n        return dedupe_results(fallback_results)\n\ndef is_content_chunk(chunk: Dict[str, Any]) -> bool:\n    text = chunk.get(\"text\", \"\")\n    if not text:\n        return False\n\n    # 1. Minimum length (filters title pages, citations)\n    if len(text) < 500:\n        return False\n\n    # 2. Must contain narrative sentences\n    # (bibliographies rarely have multiple full sentences)\n    if text.count(\". \") < 3:\n        return False\n\n    return True\n\ndef looks_narrative(text: str) -> bool:\n    if not text:\n        return False\n\n    # Must contain several complete sentences\n    if text.count(\". \") < 4:\n        return False\n\n    # Optional: avoid list-like text\n    if text.count(\"\\n\") > len(text) / 80:\n        return False\n\n    return True\n\ndef parse_score(value: Any) -> Optional[float]:\n    if value is None:\n        return None\n    try:\n        return float(value)\n    except Exception:\n        return None\n\n\ndef compute_retrieval_metrics(\n    raw: List[Dict[str, Any]],\n    filtered: List[Dict[str, Any]],\n) -> Dict[str, Any]:\n    content_chunks = [chunk for chunk in raw if is_content_chunk(chunk)]\n    narrative_chunks = [\n        chunk for chunk in content_chunks if looks_narrative(chunk.get(\"text\", \"\"))\n    ]\n    scores = [parse_score(chunk.get(\"score\")) for chunk in raw]\n    scores = [score for score in scores if score is not None]\n    return {\n        \"raw_total\": len(raw),\n        \"content_total\": len(content_chunks),\n        \"narrative_total\": len(narrative_chunks),\n        \"filtered_total\": len(filtered),\n        \"filtered_chars\": sum(len(str(chunk.get(\"text\", \"\"))) for chunk in filtered),\n        \"best_score\": min(scores) if scores else None,\n    }\n\ndef is_short_query(query: str) -> bool:\n    tokens = re.findall(r\"[\\\\w]+\", query, flags=re.UNICODE)\n    tokens = [token for token in tokens if token]\n    return len(tokens) <= 3\n\n\ndef should_broaden_retrieval(metrics: Dict[str, Any], k: int) -> Tuple[bool, List[str]]:\n    reasons: List[str] = []\n    min_chunks = min(_MIN_CONTEXT_CHUNKS, max(1, k))\n    if metrics.get(\"filtered_total\", 0) < min_chunks:\n        reasons.append(\"few_chunks\")\n    if metrics.get(\"filtered_chars\", 0) < _MIN_CONTEXT_CHARS:\n        reasons.append(\"short_context\")\n    best_score = metrics.get(\"best_score\")\n    if best_score is not None and best_score > _MAX_ACCEPTABLE_SCORE:\n        reasons.append(\"weak_scores\")\n    content_total = metrics.get(\"content_total\", 0)\n    filtered_total = metrics.get(\"filtered_total\", 0)\n    if content_total >= _MIN_CONTENT_FOR_RATIO:\n        ratio = filtered_total / max(1, content_total)\n        if ratio < _MIN_NARRATIVE_RATIO:\n            reasons.append(\"narrative_filtered\")\n    return bool(reasons), reasons\n\n\ndef build_rrf_scores(\n    vector_results: Sequence[Dict[str, Any]],\n    lexical_results: Sequence[Dict[str, Any]],\n    rrf_k: int = _RRF_K,\n) -> Dict[str, float]:\n    rrf_k = max(1, int(rrf_k))\n    scores: Dict[str, float] = {}\n    for rank, item in enumerate(vector_results, start=1):\n        key = chunk_key(item)\n        if not key:\n            continue\n        scores[key] = scores.get(key, 0.0) + 1.0 / (rrf_k + rank)\n    for rank, item in enumerate(lexical_results, start=1):\n        key = chunk_key(item)\n        if not key:\n            continue\n        scores[key] = scores.get(key, 0.0) + 1.0 / (rrf_k + rank)\n    return scores\n\n\ndef order_by_rrf(\n    candidates: List[Dict[str, Any]],\n    rrf_scores: Dict[str, float],\n) -> List[Dict[str, Any]]:\n    if not candidates or not rrf_scores:\n        return candidates\n    scored: List[Tuple[float, int, Dict[str, Any]]] = []\n    for idx, item in enumerate(candidates):\n        key = chunk_key(item)\n        score = rrf_scores.get(key, 0.0) if key else 0.0\n        scored.append((score, idx, item))\n    scored.sort(key=lambda row: (-row[0], row[1]))\n    return [row[2] for row in scored]\n\n\ndef apply_doc_cap(\n    results: List[Dict[str, Any]],\n    max_per_doc: int,\n) -> List[Dict[str, Any]]:\n    if max_per_doc <= 0 or not results:\n        return results\n    capped: List[Dict[str, Any]] = []\n    counts: Dict[str, int] = {}\n    for item in results:\n        doc_id = str(item.get(\"doc_id\", \"\") or \"\")\n        if not doc_id:\n            capped.append(item)\n            continue\n        count = counts.get(doc_id, 0)\n        if count >= max_per_doc:\n            continue\n        counts[doc_id] = count + 1\n        capped.append(item)\n    return capped\n\n\ndef log_rrf_top(\n    ordered: Sequence[Dict[str, Any]],\n    rrf_scores: Dict[str, float],\n    top_n: int,\n) -> None:\n    if top_n <= 0 or not ordered:\n        return\n    limit = min(top_n, len(ordered))\n    eprint(f\"RRF top {limit}:\")\n    for idx, item in enumerate(ordered[:limit], start=1):\n        key = chunk_key(item)\n        score = rrf_scores.get(key, 0.0) if key else 0.0\n        doc_id = item.get(\"doc_id\", \"\")\n        chunk_id = item.get(\"chunk_id\", \"\")\n        vector_score = item.get(\"score\", \"\")\n        eprint(\n            f\"  {idx}. rrf={score:.6f} doc_id={doc_id} chunk_id={chunk_id} score={vector_score}\"\n        )\n\n\ndef retrieve_with_broadening(\n    client: redis.Redis,\n    index: str,\n    vec: bytes,\n    k: int,\n    keywords: List[str],\n    rrf_k: int = _RRF_K,\n    rrf_log_top: int = 0,\n    max_per_doc: int = 0,\n) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    retrieved, metrics = retrieve_chunks(\n        client,\n        index,\n        vec,\n        k,\n        keywords,\n        strict=True,\n        rrf_k=rrf_k,\n        rrf_log_top=rrf_log_top,\n        max_per_doc=max_per_doc,\n    )\n    broaden, _ = should_broaden_retrieval(metrics, k)\n    if broaden:\n        fallback_k = max(k * 2, 12)\n        try:\n            retrieved, _ = retrieve_chunks(\n                client,\n                index,\n                vec,\n                fallback_k,\n                keywords,\n                strict=False,\n                rrf_k=rrf_k,\n                rrf_log_top=rrf_log_top,\n                max_per_doc=max_per_doc,\n            )\n        except Exception as exc:\n            eprint(f\"Fallback retrieval failed: {exc}\")\n    return retrieved, metrics\n\ndef build_context(retrieved: List[Dict[str, Any]]) -> str:\n    blocks = []\n    for chunk in retrieved:\n        doc_id = chunk.get(\"doc_id\", \"\")\n        chunk_id = chunk.get(\"chunk_id\", \"\")\n        source_pdf = chunk.get(\"source_pdf\", \"\")\n        page_start = chunk.get(\"page_start\", \"\")\n        page_end = chunk.get(\"page_end\", \"\")\n        score = chunk.get(\"score\", \"\")\n        text = chunk.get(\"text\", \"\")\n        pages = f\"{page_start}-{page_end}\"\n        block = (\n            f\"<Document source='{source_pdf}' pages='{pages}' doc_id='{doc_id}' \"\n            f\"chunk_id='{chunk_id}' score='{score}'>\\n{text}\\n</Document>\"\n        )\n        blocks.append(block)\n    return \"\\n\\n\".join(blocks)\n\n\ndef load_history_messages(path: str) -> List[Dict[str, Any]]:\n    if not path:\n        return []\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as handle:\n            payload = json.load(handle)\n    except Exception:\n        return []\n    if isinstance(payload, list):\n        return [item for item in payload if isinstance(item, dict)]\n    messages = payload.get(\"messages\") if isinstance(payload, dict) else None\n    if isinstance(messages, list):\n        return [item for item in messages if isinstance(item, dict)]\n    return []\n\n\ndef format_history_block(messages: List[Dict[str, Any]]) -> str:\n    lines: List[str] = []\n    for message in messages:\n        role = str(message.get(\"role\", \"\")).strip().lower()\n        content = str(message.get(\"content\", \"\")).strip()\n        if not content:\n            continue\n        if role not in (\"user\", \"assistant\"):\n            role = \"user\"\n        label = \"User\" if role == \"user\" else \"Assistant\"\n        lines.append(f\"{label}: {content}\")\n    return \"\\n\".join(lines)\n\n\ndef extract_annotation_key(chunk_id: str) -> str:\n    if not chunk_id:\n        return \"\"\n    if \":\" in chunk_id:\n        chunk_id = chunk_id.split(\":\", 1)[1]\n    candidate = chunk_id.strip().upper()\n    if re.fullmatch(r\"[A-Z0-9]{8}\", candidate):\n        return candidate\n    return \"\"\n\n\ndef build_citations(retrieved: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    seen = set()\n    citations: List[Dict[str, Any]] = []\n    for chunk in retrieved:\n        doc_id = chunk.get(\"doc_id\", \"\")\n        chunk_id = chunk.get(\"chunk_id\", \"\")\n        attachment_key = chunk.get(\"attachment_key\", \"\")\n        page_start = chunk.get(\"page_start\", \"\")\n        page_end = chunk.get(\"page_end\", \"\")\n        source_pdf = chunk.get(\"source_pdf\", \"\")\n        key = (doc_id, attachment_key, page_start, page_end, source_pdf)\n        if key in seen:\n            continue\n        seen.add(key)\n        annotation_key = extract_annotation_key(str(chunk_id))\n        citations.append({\n            \"doc_id\": doc_id,\n            \"chunk_id\": chunk_id,\n            \"attachment_key\": attachment_key,\n            \"annotation_key\": annotation_key or None,\n            \"page_start\": page_start,\n            \"page_end\": page_end,\n            \"pages\": f\"{page_start}-{page_end}\",\n            \"source_pdf\": source_pdf,\n        })\n    return citations\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Query RedisSearch and answer with RAG.\")\n    parser.add_argument(\"--query\", required=True)\n    parser.add_argument(\"--k\", type=int, default=10)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--chat-base-url\", required=True)\n    parser.add_argument(\"--chat-api-key\", default=\"\")\n    parser.add_argument(\"--chat-model\", required=True)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--stream\", action=\"store_true\")\n    parser.add_argument(\"--history-file\", help=\"Optional JSON file with recent chat history\")\n    parser.add_argument(\"--expand-query\", action=\"store_true\")\n    parser.add_argument(\"--expand-count\", type=int, default=3)\n    parser.add_argument(\"--rerank\", action=\"store_true\")\n    parser.add_argument(\"--rerank-model\", default=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n    parser.add_argument(\"--rerank-candidates\", type=int, default=4)\n    parser.add_argument(\"--rerank-max-chars\", type=int, default=_RERANK_MAX_CHARS_DEFAULT)\n    parser.add_argument(\"--rrf-k\", type=int, default=_RRF_K)\n    parser.add_argument(\"--rrf-log-top\", type=int, default=0)\n    parser.add_argument(\"--max-per-doc\", type=int, default=0)\n    args = parser.parse_args()\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n    use_combo = bool(args.expand_query or args.rerank)\n    expanded_queries: List[str] = []\n    raw_query = args.query\n    query_for_display = raw_query\n    index_dim_cache: Optional[int] = None\n    rrf_k = max(1, int(args.rrf_k or _RRF_K))\n    rrf_log_top = max(0, int(args.rrf_log_top or 0))\n    max_per_doc = max(0, int(args.max_per_doc or 0))\n\n    def embed_query(query_text: str) -> bytes:\n        nonlocal client, index_dim_cache\n        embedding = request_embedding(args.embed_base_url, args.embed_api_key, args.embed_model, query_text)\n        embedding_dim = len(embedding)\n        if index_dim_cache is None:\n            index_dim_cache = get_index_vector_dim(client, args.index)\n        if index_dim_cache and index_dim_cache != embedding_dim:\n            raise RuntimeError(f\"Embedding dim mismatch: index={index_dim_cache} model={embedding_dim}\")\n        embedding = normalize_vector(embedding)\n        return vector_to_bytes(embedding)\n\n    if use_combo:\n        if args.expand_query:\n            expanded_queries = expand_query(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                raw_query,\n                max(1, int(args.expand_count or 0)),\n            )\n        if expanded_queries:\n            query_for_display = expanded_queries[0]\n        candidate_multiplier = max(1, int(args.rerank_candidates or 1))\n        base_k = max(1, int(args.k))\n        if is_short_query(raw_query):\n            base_k = max(base_k, 12)\n        candidate_k = max(base_k * candidate_multiplier, base_k)\n        query_variants = [raw_query] + expanded_queries\n        candidates_map: Dict[str, Dict[str, Any]] = {}\n        try:\n            for variant in query_variants:\n                vec = embed_query(variant)\n                keywords = extract_keywords(variant)\n                retrieved_variant, _ = retrieve_with_broadening(\n                    client,\n                    args.index,\n                    vec,\n                    candidate_k,\n                    keywords,\n                    rrf_k=rrf_k,\n                    rrf_log_top=rrf_log_top,\n                    max_per_doc=0,\n                )\n                for item in retrieved_variant:\n                    key = chunk_key(item)\n                    if not key:\n                        continue\n                    existing = candidates_map.get(key)\n                    if not existing:\n                        candidates_map[key] = item\n                        continue\n                    score_new = parse_score(item.get(\"score\"))\n                    score_old = parse_score(existing.get(\"score\"))\n                    if score_new is not None and (score_old is None or score_new < score_old):\n                        candidates_map[key] = item\n        except Exception as exc:\n            eprint(f\"RedisSearch query failed: {exc}\")\n            return 2\n\n        candidates = list(candidates_map.values())\n        if args.rerank:\n            rerank_query = query_for_display or raw_query\n            reranker = load_reranker(args.rerank_model)\n            reranked = rerank_candidates(\n                reranker,\n                rerank_query,\n                candidates,\n                max(200, int(args.rerank_max_chars or _RERANK_MAX_CHARS_DEFAULT)),\n            )\n            retrieved = apply_doc_cap(reranked, max_per_doc)[:base_k]\n        else:\n            ordered = apply_tag_boosting(candidates, extract_keywords(raw_query))\n            retrieved = apply_doc_cap(ordered, max_per_doc)[:base_k]\n    else:\n        try:\n            vec = embed_query(raw_query)\n        except Exception as exc:\n            eprint(f\"Failed to embed query: {exc}\")\n            return 2\n        keywords = extract_keywords(raw_query)\n        base_k = args.k\n        if is_short_query(raw_query):\n            base_k = max(base_k, 12)\n        try:\n            retrieved, _ = retrieve_with_broadening(\n                client,\n                args.index,\n                vec,\n                base_k,\n                keywords,\n                rrf_k=rrf_k,\n                rrf_log_top=rrf_log_top,\n                max_per_doc=max_per_doc,\n            )\n        except Exception as exc:\n            eprint(f\"RedisSearch query failed: {exc}\")\n            return 2\n\n    context = build_context(retrieved)\n\n    system_prompt = (\n        \"Use ONLY the provided context for factual claims. If insufficient, say you do not know. \"\n        \"Chat history is only for conversational continuity or for providing concepts to be retrieved. \"\n        \"Add inline citations using this exact format: [[cite:DOC_ID:PAGE_START-PAGE_END]]. \"\n        \"Example: ... [[cite:ABC123:12-13]].\"\n    )\n    history_messages = load_history_messages(args.history_file) if args.history_file else []\n    history_block = format_history_block(history_messages)\n    if history_block:\n        history_block = f\"Chat history (for reference only):\\n{history_block}\\n\\n\"\n    def build_user_prompt(context_block: str) -> str:\n        return f\"{history_block}Question: {args.query}\\n\\nContext:\\n{context_block}\"\n\n    user_prompt = build_user_prompt(context)\n\n    citations = build_citations(retrieved)\n\n    answer = \"\"\n    streamed = False\n    if args.stream:\n        def emit(obj: Dict[str, Any]) -> None:\n            print(json.dumps(obj, ensure_ascii=False), flush=True)\n\n        try:\n            answer = request_chat_stream(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                args.temperature,\n                system_prompt,\n                user_prompt,\n                lambda chunk: emit({\"type\": \"delta\", \"content\": chunk}),\n            )\n            streamed = True\n        except Exception as exc:\n            if is_stream_unsupported(str(exc)):\n                streamed = False\n            else:\n                eprint(f\"Chat request failed: {exc}\")\n                return 2\n\n    if not streamed:\n        try:\n            answer = request_chat(\n                args.chat_base_url,\n                args.chat_api_key,\n                args.chat_model,\n                args.temperature,\n                system_prompt,\n                user_prompt,\n            )\n        except Exception as exc:\n            eprint(f\"Chat request failed: {exc}\")\n            return 2\n\n    output = {\n        \"query\": query_for_display,\n        \"raw_query\": raw_query if expanded_queries else \"\",\n        \"expanded_queries\": expanded_queries,\n        \"rerank_used\": bool(args.rerank),\n        \"rerank_model\": args.rerank_model if args.rerank else \"\",\n        \"answer\": answer,\n        \"citations\": citations,\n        \"retrieved\": retrieved,\n    }\n\n    if args.stream and streamed:\n        print(json.dumps({\"type\": \"final\", **output}, ensure_ascii=False), flush=True)\n    else:\n        print(json.dumps(output, ensure_ascii=False))\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "search_redis.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\n\nimport argparse\nimport json\nimport re\nimport sys\nfrom typing import Any, Dict, List, Tuple\n\nimport redis\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef decode_value(value: Any) -> Any:\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\", errors=\"ignore\")\n    return value\n\n\ndef parse_results(raw: List[Any]) -> List[Dict[str, Any]]:\n    results: List[Dict[str, Any]] = []\n    if not raw or len(raw) < 2:\n        return results\n    for idx in range(1, len(raw), 2):\n        if idx + 1 >= len(raw):\n            break\n        fields_raw = raw[idx + 1]\n        if not isinstance(fields_raw, list):\n            continue\n        field_map: Dict[str, Any] = {}\n        for i in range(0, len(fields_raw), 2):\n            key = decode_value(fields_raw[i])\n            value = decode_value(fields_raw[i + 1]) if i + 1 < len(fields_raw) else \"\"\n            field_map[str(key)] = value\n        results.append(field_map)\n    return results\n\n\nFIELD_TYPE_CACHE: Dict[str, Dict[str, str]] = {}\n\n\ndef parse_info_map(info: Any) -> Dict[str, Any]:\n    if not isinstance(info, (list, tuple)):\n        return {}\n    it = iter(info)\n    result: Dict[str, Any] = {}\n    for key in it:\n        value = next(it, None)\n        result[str(decode_value(key))] = value\n    return result\n\n\ndef get_field_types(client: redis.Redis, index: str) -> Dict[str, str]:\n    if index in FIELD_TYPE_CACHE:\n        return FIELD_TYPE_CACHE[index]\n    try:\n        info = client.execute_command(\"FT.INFO\", index)\n    except Exception:\n        return {}\n    info_map = parse_info_map(info)\n    attributes = info_map.get(\"attributes\") or info_map.get(\"fields\") or []\n    field_types: Dict[str, str] = {}\n    if isinstance(attributes, (list, tuple)):\n        for attr in attributes:\n            if not isinstance(attr, (list, tuple)):\n                continue\n            attr_map: Dict[str, Any] = {}\n            for i in range(0, len(attr) - 1, 2):\n                attr_map[str(decode_value(attr[i]))] = decode_value(attr[i + 1])\n            name = attr_map.get(\"identifier\") or attr_map.get(\"attribute\") or attr_map.get(\"name\")\n            ftype = attr_map.get(\"type\")\n            if name and ftype:\n                field_types[str(name)] = str(ftype).upper()\n    FIELD_TYPE_CACHE[index] = field_types\n    return field_types\n\n\ndef format_field_types(field_types: Dict[str, str]) -> str:\n    if not field_types:\n        return \"{}\"\n    ordered = \", \".join(f\"{key}:{field_types[key]}\" for key in sorted(field_types.keys()))\n    return \"{\" + ordered + \"}\"\n\n\ndef build_query_parts(tokens: List[str], field_types: Dict[str, str]) -> List[Tuple[str, str]]:\n    text_terms = \"|\".join(tokens)\n    tag_terms = \"|\".join(tokens)\n\n    def field_is_tag(name: str) -> bool:\n        return field_types.get(name, \"\").upper() == \"TAG\"\n\n    def should_include(name: str, required: bool = False) -> bool:\n        if field_types:\n            return required or name in field_types\n        return required\n\n    def format_term(name: str) -> str:\n        field = f\"@{name}\"\n        if field_is_tag(name):\n            return f\"{field}:{{{tag_terms}}}\"\n        return f\"{field}:({text_terms})\"\n\n    parts: List[Tuple[str, str]] = []\n    if should_include(\"text\", required=True):\n        parts.append((\"text\", format_term(\"text\")))\n    if should_include(\"title\"):\n        parts.append((\"title\", format_term(\"title\")))\n    if should_include(\"authors\"):\n        parts.append((\"authors\", format_term(\"authors\")))\n    if should_include(\"tags\"):\n        parts.append((\"tags\", format_term(\"tags\")))\n    if should_include(\"chunk_tags\"):\n        parts.append((\"chunk_tags\", format_term(\"chunk_tags\")))\n    if should_include(\"doc_id\"):\n        parts.append((\"doc_id\", format_term(\"doc_id\")))\n    return parts\n\n\ndef build_query(term: str, raw: bool, field_types: Dict[str, str]) -> Tuple[str, List[Tuple[str, str]]]:\n    term = term.strip()\n    if not term:\n        return \"\", []\n    if raw:\n        return term, []\n    raw_tokens = re.findall(r\"[\\w'\\-]{2,}\", term, flags=re.UNICODE)\n    tokens: List[str] = []\n    for token in raw_tokens:\n        cleaned = \"\".join(ch for ch in token if ch.isalnum())\n        if not cleaned:\n            continue\n        tokens.append(cleaned)\n        if token.lower().endswith((\"'s\", \"\\u2019s\")) and len(cleaned) > 3:\n            stem = cleaned[:-1]\n            if stem:\n                tokens.append(stem)\n    tokens = [token for token in tokens if token]\n    if not tokens:\n        return \"\", []\n    parts = build_query_parts(tokens, field_types)\n    if not parts:\n        return \"\", []\n    return \"(\" + \" OR \".join(term for _name, term in parts) + \")\", parts\n\n\ndef run_search(\n    client: redis.Redis,\n    index: str,\n    query: str,\n    offset: int,\n    limit: int,\n) -> List[Any]:\n    return client.execute_command(\n        \"FT.SEARCH\",\n        index,\n        query,\n        \"LIMIT\",\n        str(max(0, offset)),\n        str(max(1, limit)),\n        \"RETURN\",\n        \"15\",\n        \"doc_id\",\n        \"chunk_id\",\n        \"attachment_key\",\n        \"title\",\n        \"authors\",\n        \"tags\",\n        \"chunk_tags\",\n        \"item_type\",\n        \"year\",\n        \"page_start\",\n        \"page_end\",\n        \"section\",\n        \"source_pdf\",\n        \"text\",\n        \"score\",\n        \"DIALECT\",\n        \"2\",\n    )\n\n\ndef dedupe_results(results: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:\n    seen: set = set()\n    merged: List[Dict[str, Any]] = []\n    for item in results:\n        key = item.get(\"chunk_id\") or item.get(\"doc_id\")\n        if key is None:\n            continue\n        key = str(key)\n        if key in seen:\n            continue\n        seen.add(key)\n        merged.append(item)\n        if limit > 0 and len(merged) >= limit:\n            break\n    return merged\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Search Redis index for a term.\")\n    parser.add_argument(\"--query\", required=True, help=\"Search term or raw RedisSearch query\")\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--limit\", type=int, default=10)\n    parser.add_argument(\"--offset\", type=int, default=0)\n    parser.add_argument(\"--raw\", action=\"store_true\")\n    args = parser.parse_args()\n\n    client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n    field_types = get_field_types(client, args.index)\n    query, parts = build_query(args.query, args.raw, field_types)\n    if not query:\n        eprint(\"Query produced no tokens.\")\n        return 2\n    raw = None\n    try:\n        raw = run_search(client, args.index, query, args.offset, args.limit)\n        results = parse_results(raw)\n        total = 0\n        if isinstance(raw, list) and raw:\n            try:\n                total = int(raw[0])\n            except Exception:\n                total = 0\n        if total == 0 and parts:\n            fallback_results: List[Dict[str, Any]] = []\n            for _name, clause in parts:\n                try:\n                    fallback_raw = run_search(client, args.index, clause, args.offset, args.limit)\n                    fallback_results.extend(parse_results(fallback_raw))\n                except Exception:\n                    continue\n            merged = dedupe_results(fallback_results, args.limit)\n            if merged:\n                payload = {\n                    \"query\": query,\n                    \"raw_query\": args.query,\n                    \"total\": len(merged),\n                    \"results\": merged,\n                    \"fallback_used\": True,\n                    \"fallback_reason\": \"empty_combined_query\",\n                    \"fallback_queries\": [clause for _name, clause in parts],\n                }\n            else:\n                payload = {\n                    \"query\": query,\n                    \"raw_query\": args.query,\n                    \"total\": total,\n                    \"results\": results,\n                }\n        else:\n            payload = {\n                \"query\": query,\n                \"raw_query\": args.query,\n                \"total\": total,\n                \"results\": results,\n            }\n    except Exception as exc:\n        eprint(f\"RedisSearch query failed: {exc}\")\n        eprint(f\"Search diagnostics: index={args.index} raw={args.raw} raw_query={args.query!r}\")\n        eprint(f\"Search diagnostics: parsed_query={query!r}\")\n        eprint(f\"Search diagnostics: field_types={format_field_types(field_types)}\")\n        fallback_results: List[Dict[str, Any]] = []\n        failed_fields: List[str] = []\n        for name, clause in parts:\n            try:\n                fallback_raw = run_search(client, args.index, clause, args.offset, args.limit)\n                fallback_results.extend(parse_results(fallback_raw))\n            except Exception as field_exc:\n                failed_fields.append(name)\n                eprint(f\"Search diagnostics: field_query_failed field={name} query={clause!r} error={field_exc}\")\n        merged = dedupe_results(fallback_results, args.limit)\n        payload = {\n            \"query\": query,\n            \"raw_query\": args.query,\n            \"total\": len(merged),\n            \"results\": merged,\n            \"fallback_queries\": [clause for _name, clause in parts],\n            \"fallback_failed_fields\": failed_fields,\n        }\n    payload.setdefault(\"field_types\", field_types)\n    payload.setdefault(\"fallback_used\", False)\n    payload.setdefault(\"fallback_reason\", \"\")\n    payload.setdefault(\"fallback_queries\", [])\n    payload.setdefault(\"fallback_failed_fields\", [])\n    print(json.dumps(payload, ensure_ascii=False))\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "redis_diagnostics.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\n\nimport argparse\nimport json\nimport sys\nfrom typing import Any, Dict, Tuple\n\nimport redis\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef decode_value(value: Any) -> Any:\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\", errors=\"ignore\")\n    return value\n\n\ndef parse_info_map(info: Any) -> Dict[str, Any]:\n    if not isinstance(info, (list, tuple)):\n        return {}\n    it = iter(info)\n    result: Dict[str, Any] = {}\n    for key in it:\n        value = next(it, None)\n        result[str(decode_value(key))] = value\n    return result\n\n\ndef extract_summary(info_map: Dict[str, Any]) -> Dict[str, Any]:\n    summary: Dict[str, Any] = {}\n    for key in (\n        \"index_name\",\n        \"num_docs\",\n        \"num_terms\",\n        \"max_doc_id\",\n        \"hash_indexing_failures\",\n        \"percent_indexed\",\n        \"gc_stats\",\n    ):\n        if key in info_map:\n            summary[key] = decode_value(info_map[key])\n    return summary\n\n\ndef make_json_safe(value: Any) -> Any:\n    if isinstance(value, bytes):\n        return value.decode(\"utf-8\", errors=\"ignore\")\n    if isinstance(value, dict):\n        return {str(k): make_json_safe(v) for k, v in value.items()}\n    if isinstance(value, (list, tuple)):\n        return [make_json_safe(item) for item in value]\n    return value\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Collect Redis/RediSearch diagnostics.\")\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    args = parser.parse_args()\n\n    payload: Dict[str, Any] = {\n        \"redis_url\": args.redis_url,\n        \"index\": args.index,\n    }\n\n    try:\n        client = redis.Redis.from_url(args.redis_url, decode_responses=False)\n        pong = client.ping()\n        payload[\"ping\"] = bool(pong)\n        try:\n            info = client.execute_command(\"FT.INFO\", args.index)\n            info_map = parse_info_map(info)\n            payload[\"ft_info\"] = extract_summary(info_map)\n            payload[\"ft_info_raw\"] = {\n                key: decode_value(value) for key, value in info_map.items()\n            }\n        except Exception as exc:\n            payload[\"ft_info_error\"] = str(exc)\n    except Exception as exc:\n        payload[\"error\"] = str(exc)\n\n    print(json.dumps(make_json_safe(payload), ensure_ascii=False))\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "purge_redis_orphans.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom typing import Dict, Optional, Set\n\nimport redis\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef extract_doc_id(key: str, prefix: str) -> Optional[str]:\n    if not key.startswith(prefix):\n        return None\n    remainder = key[len(prefix) :]\n    if not remainder:\n        return None\n    return remainder.split(\":\", 1)[0] or None\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=\"Delete Redis chunk keys that have no matching cached item/chunk JSON.\"\n    )\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--key-prefix\", required=True)\n    parser.add_argument(\"--chunk-dir\", required=True)\n    parser.add_argument(\"--item-dir\", required=True)\n    parser.add_argument(\"--dry-run\", action=\"store_true\")\n    parser.add_argument(\"--sample\", type=int, default=10)\n    args = parser.parse_args()\n\n    payload = {\n        \"redis_url\": args.redis_url,\n        \"key_prefix\": args.key_prefix,\n        \"chunk_dir\": args.chunk_dir,\n        \"item_dir\": args.item_dir,\n        \"dry_run\": bool(args.dry_run),\n    }\n\n    try:\n        client = redis.Redis.from_url(args.redis_url, decode_responses=True)\n    except Exception as exc:\n        eprint(f\"Failed to connect to Redis: {exc}\")\n        return 2\n\n    pattern = f\"{args.key_prefix}*\"\n    doc_cache: Dict[str, bool] = {}\n    orphan_doc_ids: Set[str] = set()\n    keys_scanned = 0\n    keys_deleted = 0\n    docs_checked = 0\n\n    pipeline = None\n    if not args.dry_run:\n        pipeline = client.pipeline(transaction=False)\n\n    def doc_missing_cache(doc_id: str) -> bool:\n        nonlocal docs_checked\n        if doc_id in doc_cache:\n            return doc_cache[doc_id]\n        docs_checked += 1\n        chunk_path = os.path.join(args.chunk_dir, f\"{doc_id}.json\")\n        item_path = os.path.join(args.item_dir, f\"{doc_id}.json\")\n        missing = not os.path.isfile(chunk_path) and not os.path.isfile(item_path)\n        doc_cache[doc_id] = missing\n        return missing\n\n    try:\n        for key in client.scan_iter(match=pattern, count=500):\n            keys_scanned += 1\n            doc_id = client.hget(key, \"doc_id\")\n            if not doc_id:\n                doc_id = extract_doc_id(key, args.key_prefix)\n            if not doc_id:\n                continue\n            if doc_missing_cache(doc_id):\n                orphan_doc_ids.add(doc_id)\n                if pipeline is not None:\n                    pipeline.delete(key)\n                    keys_deleted += 1\n                    if keys_deleted % 500 == 0:\n                        pipeline.execute()\n        if pipeline is not None:\n            pipeline.execute()\n    except Exception as exc:\n        eprint(f\"Failed to purge orphaned keys: {exc}\")\n        return 2\n\n    payload.update(\n        {\n            \"keys_scanned\": keys_scanned,\n            \"keys_deleted\": keys_deleted,\n            \"docs_checked\": docs_checked,\n            \"orphan_doc_count\": len(orphan_doc_ids),\n            \"sample_orphan_doc_ids\": sorted(orphan_doc_ids)[: max(0, args.sample)],\n        }\n    )\n    print(json.dumps(payload, ensure_ascii=False))\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "batch_index_pyzotero.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Set\n\nfrom pyzotero import zotero\nfrom tqdm import tqdm\n\n\ndef eprint(message: str) -> None:\n    sys.stderr.write(message + \"\\n\")\n\n\ndef ensure_dir(path: Path) -> None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef load_checkpoint(path: Path) -> Set[str]:\n    if not path.exists():\n        return set()\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        items = data.get(\"processed\", [])\n        return set(str(x) for x in items)\n    except Exception:\n        return set()\n\n\ndef save_checkpoint(path: Path, processed: Set[str]) -> None:\n    payload = {\"processed\": sorted(processed)}\n    path.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n\n\ndef run_script(script: Path, args: List[str]) -> None:\n    command = [sys.executable, str(script)] + args\n    result = subprocess.run(command, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise RuntimeError(result.stderr.strip() or f\"Command failed: {' '.join(command)}\")\n\n\ndef fetch_parent_item(client: zotero.Zotero, parent_key: str) -> Dict[str, Any]:\n    try:\n        item = client.item(parent_key)\n        if isinstance(item, list):\n            return item[0] if item else {}\n        return item\n    except Exception:\n        return {}\n\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(description=\"Batch index a Zotero library with Docling and RedisSearch.\")\n    parser.add_argument(\"--library-id\", required=True)\n    parser.add_argument(\"--library-type\", choices=[\"user\", \"group\"], required=True)\n    parser.add_argument(\"--api-key\", required=True)\n    parser.add_argument(\"--redis-url\", required=True)\n    parser.add_argument(\"--index\", required=True)\n    parser.add_argument(\"--prefix\", required=True)\n    parser.add_argument(\"--embed-base-url\", required=True)\n    parser.add_argument(\"--embed-api-key\", default=\"\")\n    parser.add_argument(\"--embed-model\", required=True)\n    parser.add_argument(\"--embed-include-metadata\", action=\"store_true\")\n    parser.add_argument(\"--out-dir\", default=\"./data\")\n    parser.add_argument(\"--ocr\", choices=[\"auto\", \"force\", \"off\"], default=\"auto\")\n    parser.add_argument(\"--chunking\", choices=[\"page\", \"section\"], default=\"page\")\n    parser.add_argument(\"--limit\", type=int)\n    parser.add_argument(\"--since\", type=int)\n    parser.add_argument(\"--reindex\", action=\"store_true\")\n    args = parser.parse_args()\n\n    out_dir = Path(args.out_dir).resolve()\n    pdf_dir = out_dir / \"pdfs\"\n    item_dir = out_dir / \"items\"\n    doc_dir = out_dir / \"docs\"\n    chunk_dir = out_dir / \"chunks\"\n    checkpoint_path = out_dir / \"checkpoint.json\"\n\n    for folder in (pdf_dir, item_dir, doc_dir, chunk_dir):\n        ensure_dir(folder)\n\n    processed = set() if args.reindex else load_checkpoint(checkpoint_path)\n\n    client = zotero.Zotero(args.library_id, args.library_type, args.api_key)\n\n    params: Dict[str, Any] = {\"itemType\": \"attachment\"}\n    if args.limit:\n        params[\"limit\"] = args.limit\n    if args.since:\n        params[\"since\"] = args.since\n\n    try:\n        attachments = client.everything(client.items(**params))\n    except Exception as exc:\n        eprint(f\"Failed to fetch Zotero items: {exc}\")\n        return 2\n\n    pdf_items = []\n    for item in attachments:\n        data = item.get(\"data\", {})\n        content_type = data.get(\"contentType\", \"\") or \"\"\n        if content_type.startswith(\"application/pdf\"):\n            pdf_items.append(item)\n\n    script_dir = Path(__file__).resolve().parent\n    docling_script = script_dir / \"docling_extract.py\"\n    index_script = script_dir / \"index_redisearch.py\"\n\n    errors: List[str] = []\n\n    for item in tqdm(pdf_items, desc=\"Indexing PDFs\"):\n        attachment_key = item.get(\"key\")\n        if not attachment_key:\n            continue\n        parent_key = item.get(\"data\", {}).get(\"parentItem\")\n        doc_id = parent_key or attachment_key\n\n        if doc_id in processed:\n            continue\n\n        pdf_path = pdf_dir / f\"{attachment_key}.pdf\"\n        item_path = item_dir / f\"{doc_id}.json\"\n        doc_path = doc_dir / f\"{doc_id}.md\"\n        chunk_path = chunk_dir / f\"{doc_id}.json\"\n\n        try:\n            content = client.file(attachment_key)\n            if not content:\n                raise RuntimeError(\"Empty PDF content\")\n            pdf_path.write_bytes(content)\n        except Exception as exc:\n            errors.append(f\"{doc_id}: download failed ({exc})\")\n            continue\n\n        try:\n            metadata = fetch_parent_item(client, parent_key) if parent_key else item\n            item_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n        except Exception as exc:\n            errors.append(f\"{doc_id}: metadata write failed ({exc})\")\n            continue\n\n        try:\n            run_script(\n                docling_script,\n                [\n                    \"--pdf\",\n                    str(pdf_path),\n                    \"--doc-id\",\n                    doc_id,\n                    \"--out-json\",\n                    str(chunk_path),\n                    \"--out-md\",\n                    str(doc_path),\n                    \"--chunking\",\n                    args.chunking,\n                    \"--ocr\",\n                    args.ocr,\n                ],\n            )\n        except Exception as exc:\n            errors.append(f\"{doc_id}: docling failed ({exc})\")\n            continue\n\n        try:\n            index_args = [\n                \"--chunks-json\",\n                str(chunk_path),\n                \"--redis-url\",\n                args.redis_url,\n                \"--index\",\n                args.index,\n                \"--prefix\",\n                args.prefix,\n                \"--embed-base-url\",\n                args.embed_base_url,\n                \"--embed-api-key\",\n                args.embed_api_key,\n                \"--embed-model\",\n                args.embed_model,\n            ]\n            if args.embed_include_metadata:\n                index_args.append(\"--embed-include-metadata\")\n            run_script(index_script, index_args)\n        except Exception as exc:\n            errors.append(f\"{doc_id}: redis index failed ({exc})\")\n            continue\n\n        processed.add(doc_id)\n        save_checkpoint(checkpoint_path, processed)\n\n    if errors:\n        eprint(\"Failures:\")\n        for entry in errors:\n            eprint(f\"- {entry}\")\n\n    eprint(f\"Processed {len(processed)} items. Errors: {len(errors)}\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
  "utils_embedding.py": "#!/usr/bin/env python3\n# zotero-redisearch-rag tool version: 0.4.5\nimport math\nimport struct\nimport requests\nfrom typing import List\n\ndef normalize_vector(values: List[float]) -> List[float]:\n    norm = math.sqrt(sum(v * v for v in values))\n    if norm == 0:\n        return values\n    return [v / norm for v in values]\n\ndef vector_to_bytes(values: List[float]) -> bytes:\n    return struct.pack(\"<\" + \"f\" * len(values), *values)\n\ndef request_embedding(base_url: str, api_key: str, model: str, text: str) -> List[float]:\n    url = base_url.rstrip(\"/\") + \"/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n    response = requests.post(url, json={\"input\": text, \"model\": model}, headers=headers, timeout=120)\n    if response.status_code >= 400:\n        raise RuntimeError(f\"Embedding request failed: {response.status_code} {response.text}\")\n    payload = response.json()\n    data = payload.get(\"data\")\n    if not data:\n        raise RuntimeError(\"Embedding response missing data field\")\n    embedding = data[0].get(\"embedding\")\n    if not embedding:\n        raise RuntimeError(\"Embedding response missing embedding\")\n    return [float(x) for x in embedding]\n",
  "ocr_wordlist.txt": "# zotero-redisearch-rag tool version: 0.4.5\naai\naam\nabb\nabge\nabr\nabsol\nabteilungs\nacad\nacc\nacco\naccu\nackley\nacn\nacp\nactu\naddie\nadolphus\nadress\naf\nafew\naff\nafton\nagarwal\nagonized\nagonizing\nagrar\nagt\nagu\nahh\nahram\naicha\naigner\naip\naire\naj\nake\naken\nala\nalanus\nalbeck\nalbers\nalbornoz\nald\nalda\naleksandra\nalfaro\nalibaba\nalittle\nalla\nallard\nalli\nallin\nallman\nallright\nalongwith\nalonso\name\namelie\nameri\namg\namityville\namr\nams\nanalyze\nanalyzed\nanalyzing\nanan\nanc\nanca\nance\nande\nander\nandersonville\nandr\nandra\nandrae\nandrus\nands\nandthe\nane\nanent\nange\nangelis\nani\nania\nanish\nanneke\nanni\nannis\nantone\nantti\nanz\nao\napac\napel\naph\napl\napologize\nappl\napplegate\naq\narbei\narbeits\narcheological\narchi\nari\naris\narmando\narmor\narri\narrowsmith\nartem\nartes\narti\narvid\narxiv\nasan\naschauer\nase\nasi\naskin\naspx\nassis\nasso\nassoc\nastro\naswell\nater\natk\natla\natlan\natleast\natmos\nats\natt\natta\natten\natthe\naubry\naufge\naufl\naul\naun\naurore\nausge\nauskunfts\nauss\nauthorization\nauthorized\nauthorizing\navas\navg\navo\naw\naways\nawi\nax\nay\nayres\naz\nazhar\nbab\nbadgley\nbagwell\nbaily\nbains\nbal\nbalaji\nballston\nbama\nbanerjee\nbanz\nbarger\nbarnaby\nbaro\nbarret\nbascom\nbatchelor\nbayless\nbayne\nbaynes\nbayo\nbbe\nbeall\nbeaty\nbeca\nbeckedahl\nbeetz\nbefor\nbegleit\nbehavior\nbehavioral\nbehaviors\nbeit\nbek\nbelcher\nbellum\nbelter\nbeltran\nbemis\nbene\nbengt\nbenj\nbepreisung\nbera\nberatungs\nbergemann\nbernal\nbero\nberr\nberryman\nberthier\nbertin\nbertuch\nbesse\nbestof\nbeteiligungs\nbetz\nbeuth\nbewertungs\nbge\nbiblio\nbibliotheks\nbice\nbie\nbil\nbildungs\nbina\nbir\nbirney\nbirte\nbisbee\nbischoff\nbissell\nbiswas\nbitkom\nbitt\nbjorn\nbjörk\nblacklight\nblackwall\nblaisdell\nblakeley\nblakely\nble\nbles\nblinn\nblogspot\nblowed\nblu\nbmi\nbmwi\nboe\nboehm\nboggs\nbogue\nboj\nbol\nboland\nboldt\nboller\nboney\nbonino\nborchers\nboren\nborrego\nborrmann\nbos\nbosman\nboulanger\nboult\nbourdieu\nboysen\nbpa\nbrabeck\nbracher\nbrammer\nbrashear\nbreck\nbreuning\nbreyer\nbridgeman\nbridgette\nbrien\nbrin\nbrinker\nbriony\nbris\nbriscoe\nbrockmann\nbrok\nbrost\nbrubaker\nbrunner\nbruns\nbsi\nbu\nbuchner\nbudrich\nbui\nbuildin\nbuildup\nbuisson\nbul\nbungen\nbunn\nburchardt\nburdett\nburks\nburling\nbusi\nböhmer\nböker\nbühren\nbür\nbüttner\ncabeza\ncade\ncady\ncai\ncali\ncalif\ncallender\ncallie\ncally\ncampania\ncanan\ncandor\ncaney\ncantrell\ncao\ncapi\ncaplan\ncapt\ncaput\ncarell\ncaribe\ncarmack\ncaron\ncarondelet\ncarpathia\ncarrasco\ncarrillo\ncassel\ncassell\ncastell\ncastellanos\ncastelli\ncastleman\ncatalog\ncataloging\ncatalogs\ncate\ncategorization\ncategorize\ncategorized\ncates\ncau\ncaus\ncavalli\ncavanaugh\ncci\ncco\ncdi\ncec\ncecilie\nced\ncedrik\ncele\ncelo\ncentered\ncentering\ncenterline\ncentern\ncenterpiece\ncentimeters\ncentralized\ncept\ncera\ncerf\nchai\nchampollion\nchancellorsville\nchantel\nchao\nchapelle\nchappell\ncharacterize\ncharacterized\ncharacterizes\ncharacterizing\nchas\nchawla\nched\nchien\nchil\nchilds\nchim\nchiseled\nchristof\nchristophe\nchua\nchul\nchun\ncil\ncin\ncio\ncios\ncip\ncit\nciv\ncivilized\ncked\ncken\nclamor\nclarins\nclaussen\ncle\nclearinghouse\nclemons\ncler\nclu\ncof\ncoinbase\ncolla\ncolo\ncolonized\ncolor\ncolored\ncolorful\ncolors\ncolvin\ncolwell\ncom\ncommis\ncommu\ncommun\ncommuni\ncompl\nconant\nconceptualization\ncondi\nconf\nconfed\nconn\nconsilium\nconst\nconstans\ncontro\ncookson\ncoons\ncoord\ncordis\ncormack\ncorp\ncorrado\ncorre\ncorte\ncortez\ncostas\ncouldn\ncoun\ncour\ncourant\ncov\ncowles\ncrain\ncre\ncrea\ncremer\ncrippen\ncris\ncriticize\ncriticized\ncroom\ncros\ncrue\ncta\ncullum\nculp\ncunard\ncuno\ncupp\ncurr\ncurtin\ncus\ncutoff\ndae\ndage\ndai\ndailey\ndak\ndalit\ndalla\ndani\ndarko\ndarlin\ndarrow\ndau\ndavey\ndayal\nddi\ndecentralized\ndeepwater\ndefense\ndefenses\ndefi\ndelisle\ndelt\ndemeanor\ndemobilization\ndemocratization\ndemocratizing\ndende\ndene\ndenney\ndennison\ndeppe\ndept\ndesy\ndeve\ndevel\ndhar\ndiarrhea\ndickel\ndidi\ndidn\ndier\ndierkes\ndietze\ndif\ndigi\ndigitalization\ndigitization\ndigitize\ndigitized\ndil\ndiller\ndinan\ndinh\ndini\ndipl\ndirec\ndiw\ndle\ndoa\ndoan\ndocent\ndocu\ndoesn\ndois\ndol\ndominika\ndonatella\ndonelson\ndor\ndorman\ndors\ndorsett\ndoty\ndou\ndowell\ndowntown\ndoz\ndra\ndragan\ndrago\ndred\ndren\ndrescher\ndressler\ndreyer\ndri\ndrumm\ndrydock\ndsgvo\ndte\ndto\nduce\nduquesne\ndurin\ndurkin\neac\nead\neadie\neam\nearle\nearnshaw\neastport\nebd\nebe\nebel\neberl\nebi\nebru\necl\neco\necon\neda\nedc\nedi\nedu\neep\neer\neero\neet\nef\neffi\nefl\neggenstein\negovernment\nehem\nehr\neickhoff\neinge\neini\neir\nek\neldridge\nele\nelearning\nelec\nelectrolytic\nelek\nelevator\neley\nelihu\nelina\nelkin\neln\nels\neman\nemelia\nemer\nemerick\nemilie\nemmons\nemp\nemphasize\nemphasized\nemphasizes\nemphasizing\nena\nenb\nence\nendeavor\nendeavored\nendeavoring\nendeavors\nene\nenes\nengelhardt\nengi\nengl\nengle\nengr\nenke\nenos\nenrollment\nent\nents\nentstehungs\nentwicklungs\nenwg\neos\nepi\nepicenter\nepub\nequaled\nerc\nerd\nerfahrungs\nerick\nern\nert\nertl\nerw\nery\neso\nesq\ness\nesta\nestab\netc\nete\netl\netta\nette\neurop\neuropaea\neuropeana\nevi\nevtl\new\newr\nexc\nexpe\nexper\nexperi\ney\nez\neze\nfabienne\nfabio\nfabricius\nfabrizio\nfaelle\nfairbank\nfal\nfam\nfami\nfannie\nfarb\nfarida\nfarrar\nfarris\nfaruk\nfau\nfavor\nfavorable\nfavorably\nfavored\nfavorite\nfavorites\nfavors\nfechner\nfect\nfel\nfellner\nfennell\nfiberglass\nfid\nfidler\nfied\nfif\nfilippo\nfilson\nfinalized\nfinke\nfinkle\nfiz\nfla\nflaherty\nflam\nflavor\nfo\nfoltz\nfom\nfon\nforde\nformalized\nfors\nforschungs\nforthe\nfos\nfournier\nfrac\nfrantz\nfranzen\nfrasier\nfre\nfrede\nfsu\nfte\nfue\nfueled\nfueling\nfuer\nful\nfulfill\nfulfillment\nfung\nfurth\nfyfe\nfä\nför\nfüh\ngabbert\ngah\ngaller\ngalvanized\ngan\ngangen\ngannon\ngant\ngaray\ngarber\ngart\ngass\ngassmann\ngaynor\ngebauer\ngebhart\ngeddy\ngeert\ngehostet\ngend\ngener\ngeneralize\ngeneralized\ngennady\ngeoportal\ngeorgen\ngeorgy\ngerdes\ngerstner\ngetz\ngfa\nghosh\ngia\ngie\ngien\ngiga\ngillam\ngillen\ngini\nginn\ngivens\nglaeser\nglamor\nglaucus\ngle\nglei\ngleim\nglenwood\ngoble\ngoll\ngonzalo\ngoodell\ngoodspeed\ngorman\ngov\ngove\ngover\ngovt\ngow\ngoyal\ngra\ngradl\ngrandy\ngraßhoff\ngree\ngreenlee\ngress\ngrethe\ngriebel\ngris\ngro\ngroessten\ngroth\ngrubbs\ngrueling\ngsi\nguage\nguid\ngundlach\ngung\nguo\ngustin\ngutach\ngutknecht\ngvo\ngötting\ngünzel\nhaa\nhadad\nhadn\nhaight\nhalleck\nhalliday\nhamblin\nhammonds\nhandlungs\nhanlon\nhanni\nhanser\nhao\nhar\nharari\nharbors\nharen\nharland\nharmonia\nharpercollins\nharrassowitz\nhartig\nhartung\nhaslinger\nhasn\nhatteras\nhausers\nhav\nhavard\nhavemann\nhawken\nhayashi\nhayman\nhazzard\nhedlund\nhedrick\nhee\nheesen\nheidrich\nheinke\nheinzel\nheise\nheit\nhel\nhelbig\nhelbing\nhennessy\nhenrich\nhenrike\nherchen\nhermione\nherron\nhewes\nheyde\nhickox\nhig\nhight\nhildreth\nhillmann\nhinde\nhinman\nhinze\nhippel\nhippler\nhir\nhirt\nhisto\nhistor\nhite\nhoffmeister\nhoge\nhogrefe\nhollenbeck\nholliday\nholston\nholzer\nhom\nhoman\nhomeoffice\nhon\nhonor\nhonorably\nhonored\nhonoring\nhonors\nhoppe\nhoppin\nhor\nhoran\nhori\nhornbostel\nhorstmann\nhoskins\nhospitalization\nhospitalized\nhouten\nhowards\nhre\nhu\nhua\nhubbell\nhulbert\nhuma\nhumm\nhungen\nhup\nhur\nhusted\nhvac\nhöck\nhübner\nhülsmann\niai\niam\niana\niat\nib\nibero\nibi\nibs\nica\nican\nico\nicr\nics\nict\nident\nidf\nidi\nidl\nidlewild\niel\nife\nifyou\nigd\night\nigi\nign\nih\nihave\nihe\nij\nik\nikt\nil\nilene\nilie\nille\nilli\nillus\nils\nime\nimma\nimmortalized\nimpor\nimpro\nimt\ninan\nincase\nincl\ninclud\nindi\nindustrialization\ninfact\ninfor\ninforma\ningen\ningraham\ninhouse\ninit\ninjun\ninkl\ninl\ninnis\ninno\ninnova\ninsbes\ninslee\ninso\ninsp\ninstill\ninte\ninteragency\ninteres\ninterhyp\ninthe\nione\nior\nious\nipcc\nipp\niro\nirt\nisadore\nisc\nisin\nisla\nismay\nisn\nison\nisu\nita\nite\nithink\nitis\nity\niven\niwas\niwill\nized\njaap\njabez\njahnke\njama\njamison\njanna\njanney\njano\njantzen\njarrett\njas\njayne\njenn\njeopardize\njeopardized\njesper\njessika\njewell\njewelry\njewett\nji\njian\njie\njif\njillian\njin\njobe\njochum\njohne\njol\njolley\njoost\njopp\njordon\njos\njour\njourdan\njugg\njusti\njuventa\njyoti\njörn\nkad\nkaden\nkag\nkalman\nkaminsky\nkan\nkannt\nkaran\nkarina\nkarolin\nkarsch\nkas\nkatarzyna\nkaupp\nkawa\nkeeble\nkees\nkei\nkeiser\nkeit\nkeiten\nkelli\nkeo\nket\nketcham\nkhalsa\nkhanna\nki\nkii\nkiley\nkilometers\nkimber\nkirstie\nkis\nkiva\nklei\nkli\nkmu\nkno\nknopp\nknowl\nkoeln\nkok\nkom\nkommer\nkommis\nkommunikations\nkon\nkonstantinos\nkonstanze\nkontroll\nkonzentrations\nkoo\nkoon\nkoontz\nkoordinations\nkor\nkosel\nkpi\nkrahn\nkramm\nkrems\nkretz\nkreutzer\nkrogh\nkröger\nkröner\nkuehn\nkug\nkuk\nkul\nkun\nkura\nkwon\nkyiv\nkämper\nkön\nkönigshausen\nkönn\nköster\nlaban\nlabeled\nlabeling\nlabored\nlaborers\nlaboring\nlada\nlaf\nlafferty\nlai\nlaidlaw\nlal\nlamartine\nlames\nlamy\nlandi\nlandin\nlapointe\nlar\nlastig\nlatif\nlauber\nlaughlin\nlaun\nlda\nleaderboard\nlechler\nleclair\nleed\nleggett\nlegrand\nlehnert\nleit\nleitch\nleitungs\nlem\nlemaire\nlemay\nlemuel\nlenka\nleopoldina\nler\nlern\nletty\nleuze\nleveled\nleveln\nlevent\nlewandowski\nlhe\nlibri\nlibris\nlic\nlich\nliche\nlier\nligue\nlile\nlim\nlindell\nlinne\nlis\nlite\nlitera\nliv\nlle\nlmu\nloa\nloc\nlocalized\nlod\nloewe\nlofton\nloh\nloi\nlon\nlond\nlongtime\nlor\nloran\nlorena\nloring\nloui\nlous\nlovis\nlowden\nlowenthal\nlowrey\nlsa\nlse\nlta\nluc\nlucke\nlue\nluella\nluiz\nlum\nlus\nlusk\nluttrell\nlytle\nlän\nlö\nlöser\nmaass\nmachin\nmadita\nmaes\nmagni\nmaher\nmahmood\nmaitland\nmaj\nmak\nmakin\nmalin\nmals\nmam\nmanas\nmand\nmander\nmaneuver\nmaneuverability\nmaneuverable\nmaneuvered\nmans\nmarah\nmarginalized\nmari\nmarjan\nmarleen\nmartialed\nmartius\nmartyn\nmarveled\nmarvelous\nmaryann\nmasi\nmassie\nmasur\nmatchen\nmateus\nmathers\nmatias\nmatth\nmattison\nmaximize\nmayr\nmaysville\nmbi\nmbo\nmcadoo\nmcclanahan\nmcclelland\nmccown\nmccurdy\nmccutcheon\nmcfall\nmcginnis\nmcginty\nmcgrady\nmckeen\nmckelvey\nmckenney\nmclaurin\nmclellan\nmcloughlin\nmcmillen\nmcnabb\nmcneal\nmcnulty\nmcphail\nmcvey\nmeager\nmeas\nmechanicsburg\nmedi\nmei\nmeinel\nmeisel\nmell\nmemorialize\nmemorialized\nmende\nmense\nment\nments\nmerc\nmerce\nmerrifield\nmerriman\nmetcalf\nmeuser\nmex\nmga\nmichener\nmichi\nmie\nmil\nmili\nmilitar\nmilitarization\nmillar\nmillersville\nmilliken\nmindest\nminimize\nminimized\nminimizing\nminn\nmio\nmip\nmis\nmitscherlich\nmittermaier\nmobilitäts\nmodeler\nmodelers\nmodeling\nmodernization\nmoglich\nmohican\nmohler\nmolded\nmolloy\nmom\nmoma\nmonopolize\nmontauk\nmony\nmooc\nmoocs\nmor\nmowry\nmoxley\nmpi\nmsa\nmuenchen\nmunday\nmunsey\nmusser\nmög\nmünch\nnace\nnachdr\nnad\nnade\nnadeau\nnahme\nnal\nnang\nnapo\nnapp\nnath\nnati\nnatio\nnatu\nnaujoks\nnauvoo\nnaveen\nncbi\nnce\nneb\nnederlandse\nneer\nneff\nneher\nneighbor\nneighborhood\nneighboring\nnel\nnelles\nneto\nnevins\nnewhouse\nnewyork\nnex\nney\nnger\nnickerson\nnida\nnien\nnijmegen\nnikolay\nnikos\nnin\nnir\nnis\nnisha\nnisse\nnoe\nnom\nnomos\nnoncompliance\nnonexistent\nnormalized\nnorthrup\nnos\nnott\nnotz\nnoy\nnse\nnum\nnung\nnutt\nnuttig\nnutz\nnutzungs\nnuys\nnwo\nobj\nobs\noc\nocc\noccurence\noclock\nocto\nodebrecht\nodo\nodr\nodum\noellers\nofa\nofcourse\noffe\noffense\noffi\noffnen\noffs\nofhis\nofi\noftentimes\nofthe\nofthis\nogc\nohi\nohn\nois\nokey\nol\noli\nolmstead\nome\nona\nond\nonetime\nonl\nons\nonthe\noo\nood\noor\nopac\nopensource\nopenstack\nopi\nopr\noptimized\noram\norde\norga\norgani\norganization\norganizational\norganizations\norganize\norganized\norganizer\norganizers\norganizing\nori\norigi\nork\norl\norn\nors\nosf\nosm\nosswald\nothe\nou\nould\noup\nous\nouse\nov\nowers\nowncloud\nows\noxley\noya\noßwald\npaal\npagano\nparkhurst\nparkman\nparlors\nparrish\nparte\npasquale\npatronized\npatsey\npatta\npau\npauer\npauley\npaulina\npauly\npavillion\npawlik\npekka\npembina\npenalize\npendergast\npeo\npeop\npepe\npepin\npernambuco\nperrys\nperso\npersson\npersönlichkeits\npetsch\npez\nphe\nphila\nphilippa\nphilo\nphineas\nphong\npietsch\npii\npil\npinus\npis\npitts\npizarro\npla\nplagiarized\nplaine\nplanungs\nple\npleasants\nples\npling\nplos\nplow\nplowed\nplowing\nplows\nplugins\npnas\npoc\npoindexter\npoli\npolit\npoliti\npom\npon\npopularizing\npor\nposi\nposix\npotomac\npotosi\npotthoff\npowe\npra\nprabhakar\nprac\npractica\npracticed\npracticing\npraeger\nprather\npresi\npressurized\nprewar\npri\nprin\nprioritize\nprized\nprob\nproblema\nproc\nprofesional\nproj\npron\nproquest\nprot\nproto\nprov\npruitt\npryor\npubl\npublicized\npubmed\npurdue\npuschmann\nputtin\npöschl\nqian\nqu\nquali\nqualitäts\nquan\nque\nquel\nques\nquitman\nraddatz\nrade\nradhakrishnan\nradke\nradtke\nragan\nraghavan\nragsdale\nraju\nral\nrall\nrapha\nrapp\nrass\nrauber\nravenswood\nrawlins\nrda\nrealization\nrealize\nrealized\nrealizing\nrebekah\nreco\nrecognize\nrecognized\nrecognizing\nredaktionsteam\nredman\nredstone\nrefueled\nrefueling\nregener\nregi\nregner\nreichmann\nreimer\nreits\nrekt\nrela\nreli\nrell\nremodeled\nrenz\nrepl\nresi\nreso\nresourcen\nressources\nreto\nretz\nrevista\nrevo\nrevolutionized\nria\nric\nridgely\nrieck\nrien\nrigh\nrigor\nrijksmuseum\nrin\nris\nrisi\nriv\nriviere\nro\nrocca\nroddy\nrodolphe\nrohit\nrohrer\nrol\nroo\nroommate\nroon\nror\nrosanne\nrosenblum\nrowboat\nrse\nrubenstein\nrud\nrumors\nrumsey\nrungen\nruppert\nrylan\nryman\nrösch\nröttgen\nrück\nrülke\nrümelin\nsaas\nsach\nsachverständigenrates\nsacri\nsafford\nsager\nsahr\nsall\nsaml\nsammen\nsamu\nsandiego\nsandro\nsandt\nsani\nsanju\nsanna\nsanz\nsaro\nsaur\nsavin\nsavior\nsaylor\nsbe\nschachtner\nschaffer\nsche\nschefer\nschen\nschenck\nschland\nschlitzer\nschnepf\nscholze\nschoolcraft\nschrade\nschu\nschul\nschultes\nschulungs\nschwandt\nschäffler\nschönberger\nsci\nscientifique\nscopus\nscrutinized\nseco\nseits\nseize\nseized\nseizing\nsel\nsella\nseng\nsenger\nsengupta\nsens\nseq\nseria\nsert\nserv\nservi\nsess\nsev\nseve\nsevera\nsey\nshal\nshalini\nshan\nshar\nshaul\nsheed\nshel\nshen\nsheng\nsher\nsherrod\nshing\nsho\nshoaib\nshotwell\nshoup\nshreve\nshu\nshukla\nshuler\nshultz\nsibel\nsiche\nsicherheits\nsidewalk\nsiebeck\nsiebold\nsightlines\nsigna\nsignaled\nsignaling\nsil\nsiler\nsimonds\nsinha\nsiri\nsizable\nskaggs\nskepticism\nskeptics\nskillful\nslaven\nslaw\nsle\nsma\nsmashwords\nsme\nsmit\nsmits\nsmok\nsnelling\nsobre\nsoc\nsoep\nsoftwares\nsom\nsommerville\nsoren\nsota\nsoto\nsouthwesterly\nsowell\nsozio\nspaulding\nspeci\nspecialization\nspecialize\nspecialized\nspecialty\nspect\nspei\nspiekermann\nspiers\nsplendor\nsprech\nspurlock\nsru\nsta\nstaden\nstandardization\nstandardized\nstanek\nstansbury\nstarck\nstarnes\nstata\nstatista\nstaton\nstavros\nstegemann\nsteinke\nstel\nstellv\nstephane\nster\nsteyer\nstillman\nstimson\nsto\nstoll\nstoppin\nstor\nstra\nstraightaway\nstrate\nstreck\nstreeter\nstrother\nstruct\nstu\nstuckey\nsturges\nsturtevant\nsua\nsuc\nsuccor\nsuf\nsug\nsugimoto\nsuhr\nsui\nsul\nsuleman\nsummarization\nsummarize\nsummarized\nsummarizes\nsummarizing\nsupe\nsupp\nsupt\nsur\nsus\nsut\nsuzanna\nswantje\nsympathize\nsympathizers\nsystematized\nsöllner\nsönke\ntaggart\ntak\ntakano\ntakeda\ntaliaferro\ntalmadge\ntamir\ntamu\ntana\ntann\ntant\ntappan\ntarver\ntas\ntasso\ntaubert\ntbe\nteague\ntechn\ntei\nteichert\ntelekommunikations\ntenn\ntera\ntert\ntesti\ntha\nthacker\nthanos\nther\nthetis\nthi\nthia\nthibodeau\nthie\nthiel\nthiemann\ntho\nthoma\nthomaston\nthornburg\nthos\nthueringen\nthurber\ntice\ntidwell\ntiefergehende\ntien\ntig\ntige\ntigt\ntil\ntilson\ntion\ntions\ntis\ntite\ntitty\ntivo\ntiwari\ntke\ntla\ntle\ntna\ntober\ntoda\ntol\ntolbert\ntomasz\ntotaled\ntotaling\ntothe\ntotten\ntoussaint\ntowa\ntowson\ntra\ntradeoffs\ntral\ntraumatized\ntrav\ntraveled\ntraveler\ntravelers\ntraveling\ntre\ntremont\ntren\ntri\ntrib\ntrinh\ntro\ntru\ntrum\ntröger\ntsukuba\ntte\ntubbs\ntudo\ntung\nture\nturen\ntuscarora\ntwen\ntwente\ntwigg\ntylers\ntät\nua\nual\nub\nubc\nuber\nuc\nucd\nueber\nuel\nuhl\nuia\nuld\nuli\null\numb\nume\numg\nunderhill\nunderrepresented\nunfavorable\nuniv\nuniversidad\nuniversitaet\nuniversitäts\nunterneh\nunterstützungs\nusin\nusu\nusw\nutilization\nutilize\nutilized\nutilizing\nuu\nva\nvaca\nvadis\nvaldes\nvalor\nvania\nvann\nvapor\nvapors\nvar\nvari\nvas\nvauban\nveen\nvelden\nveritas\nverma\nvernet\nverschie\nverschiede\nverwaltungs\nvey\nviale\nvicks\nvide\nvidya\nvierkant\nvieweg\nvigor\nvin\nvinh\nvir\nvirg\nvisser\nvisualization\nvisualizations\nvisualizing\nvive\nviz\nvo\nvoight\nvorder\nvorge\nvorgehensmodell\nvos\nvossen\nvox\nvoy\nwaa\nwageningen\nwah\nwak\nwallin\nwarrenton\nwasa\nwashroom\nwasn\nwasson\nwat\nwatercolor\nwatkinson\nwaverly\nwayman\nwebinare\nwech\nwef\nwegener\nwei\nweichert\nweigel\nweils\nweingart\nwel\nwellcome\nwerf\nwescott\nwezel\nwga\nwhe\nwher\nwhi\nwhic\nwhitepaper\nwhitten\nwieviel\nwifi\nwik\nwillful\nwillson\nwindeck\nwis\nwisc\nwiss\nwissenschafts\nwiththe\nwittenburg\nwmo\nwofford\nwoll\nwom\nwooldridge\nwoolf\nwor\nwou\nwoul\nwouldn\nwulf\nwur\nwusst\nwuttke\nwäh\nwür\nwüthrich\nxia\nxiao\nyager\nyannis\nyare\nyasemin\nyi\nyoon\nyoun\nyu\nyumi\nyun\nyuval\nza\nzachariah\nzalando\nzeich\nzeidler\nzeng\nzent\nzi\nzon\nzung\nzusam\nzwi\nöffent\nöpnv\nüberarb\n",
  "requirements.txt": "# zotero-redisearch-rag tool version: 0.4.5\ndocling\nlangcodes[data]\nmarkdown\nmarkdown-it-py\nnumpy\npaddleocr[doc-parser]\npaddlepaddle==3.2.2\npdf2image\npillow\npypdf\npytesseract\npyzotero\nredis\nrequests\nsentence-transformers\nstopwordsiso\ntqdm\nwordfreq\n\n# Optional for language normalization and spellchecking\n# hunspell  # Disabled: fails to build on macOS/Python 3.13, use spylls fallback\nspylls\n",
  "docker-compose.yml": "# zotero-redisearch-rag tool version: 0.4.5\nservices:\n  redis-stack:\n    image: redis/redis-stack-server:latest\n    command: [\"redis-stack-server\", \"/redis-stack.conf\", \"--dir\", \"/data\"]\n    environment:\n      - REDIS_ARGS=\n    ports:\n      - \"${ZRR_PORT:-6379}:6379\"\n    volumes:\n      - \"${ZRR_DATA_DIR:-./.zotero-redisearch-rag/redis-data}:/data\"\n      - \"./redis-stack.conf:/redis-stack.conf:ro\"\n",
  "redis-stack.conf": "# zotero-redisearch-rag tool version: 0.4.5\n# Redis Stack persistence config for local RAG index\nappendonly yes\nappendfsync everysec\n\ndir /data\n",
};